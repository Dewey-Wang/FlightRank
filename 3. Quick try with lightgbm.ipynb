{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bcca8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from lightgbm import LGBMRegressor\n",
    "from scipy.stats import pearsonr\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.base import clone\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.chdir(\"/Users/deweywang/Desktop/GitHub/FlightRank/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8de6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from scripts.config import CFG\n",
    "class Trainer:\n",
    "    def __init__(self, model, models_folder):\n",
    "        \"\"\"\n",
    "        model:         sklearn/LightGBM é£æ ¼çš„å›å½’æ¨¡å‹å®ä¾‹\n",
    "        models_folder: (å¯é€‰) ä¿å­˜æœ€ä½³æŠ˜æ¨¡å‹çš„ç›®å½•ï¼›å¦‚æœä¸º Noneï¼Œåˆ™ä¸ä¿å­˜\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.models_folder = models_folder\n",
    "        if self.models_folder:\n",
    "            os.makedirs(self.models_folder, exist_ok=True)\n",
    "            \n",
    "    def fit_predict(self, X, y, X_test):\n",
    "        print(f\"Training {self.model.__class__.__name__}\\n\")\n",
    "        train_fold_scores = []\n",
    "        fold_scores = []\n",
    "        oof_preds = np.zeros(X.shape[0])\n",
    "        test_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "        split = KFold(n_splits=CFG.n_folds, shuffle=False).split(X, y)\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(split):\n",
    "\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "            model = clone(self.model)\n",
    "            print(\"Start trainning...\")\n",
    "            model.fit(X_train, y_train)\n",
    "            # ä¿å­˜æœ¬æŠ˜æ¨¡å‹\n",
    "            fold_folder = os.path.join(self.models_folder, f\"fold{fold_idx}\")\n",
    "            os.makedirs(fold_folder, exist_ok=True)\n",
    "            fold_path = os.path.join(fold_folder, f\"{self.model.__class__.__name__}.pkl\")\n",
    "            joblib.dump(model, fold_path)\n",
    "            print(f\"ğŸ“¦ Saved fold {fold_idx} model to {fold_path}\")\n",
    "\n",
    "            y_preds = model.predict(X_val)\n",
    "            oof_preds[val_idx] = y_preds\n",
    "            \n",
    "            y_train_preds = model.predict(X_train)\n",
    "\n",
    "\n",
    "            temp_test_preds = model.predict(X_test)\n",
    "            test_preds += temp_test_preds / CFG.n_folds\n",
    "\n",
    "            fold_score = pearsonr(y_val, y_preds)[0]\n",
    "            fold_scores.append(fold_score)\n",
    "            \n",
    "            train_fold_score = pearsonr(y_train, y_train_preds)[0]\n",
    "            train_fold_scores.append(train_fold_score)\n",
    "\n",
    "            print(f\"--- Fold {fold_idx} - val_pearson: {fold_score:.6f} - train_pearson: {train_fold_score:.6f}\")\n",
    "\n",
    "            del X_train, y_train, X_val, y_val, y_preds, model, temp_test_preds\n",
    "            gc.collect()\n",
    "\n",
    "        overall_score = pearsonr(y, oof_preds)[0]\n",
    "        mean_score = np.mean(fold_scores)\n",
    "        std_score = np.std(fold_scores)\n",
    "        \n",
    "        print(f\"\\n------ Overall Score: {overall_score:.6f} - Mean val_pearson: {mean_score:.6f} Â± {std_score:.6f} - Mean train_pearson: {np.mean(train_fold_scores):.6f} Â± {np.std(train_fold_scores):.6f} ------\\n\")\n",
    "        \n",
    "        return oof_preds, test_preds, fold_scores, train_fold_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba28b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from scripts.config import CFG\n",
    "\n",
    "\n",
    "def create_time_decay_weights(n: int, decay: float = 0.9) -> np.ndarray:\n",
    "    pos = np.arange(n)\n",
    "    normed = pos / (n - 1)\n",
    "    w = decay ** (1.0 - normed)\n",
    "    return w * n / w.sum()\n",
    "\n",
    "\n",
    "def get_time_slices(n_samples: int):\n",
    "    return [\n",
    "        {\"name\": \"full_data\", \"cutoff\": 0},\n",
    "        {\"name\": \"last_75pct\", \"cutoff\": int(0.25 * n_samples)},\n",
    "        {\"name\": \"last_50pct\", \"cutoff\": int(0.50 * n_samples)}\n",
    "    ]\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, models_folder):\n",
    "        self.model = model\n",
    "        self.models_folder = models_folder\n",
    "        if self.models_folder:\n",
    "            os.makedirs(self.models_folder, exist_ok=True)\n",
    "\n",
    "    def fit_predict(self, X, y, X_test):\n",
    "        print(f\"Training {self.model.__class__.__name__}\\n\")\n",
    "        train_fold_scores = []\n",
    "        fold_scores = []\n",
    "        oof_preds = np.zeros(X.shape[0])\n",
    "        test_preds_slices = {s[\"name\"]: np.zeros(X_test.shape[0]) for s in get_time_slices(X.shape[0])}\n",
    "        oof_preds_slices = {s[\"name\"]: np.zeros(X.shape[0]) for s in get_time_slices(X.shape[0])}\n",
    "\n",
    "        kf = KFold(n_splits=3, shuffle=False)\n",
    "        n_total = X.shape[0]\n",
    "        slices = get_time_slices(n_total)\n",
    "\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "            print(f\"\\nğŸ§© Fold {fold_idx}\")\n",
    "            X_val, y_val = X.iloc[val_idx], y[val_idx]\n",
    "\n",
    "            for s in slices:\n",
    "                slice_name, cutoff = s[\"name\"], s[\"cutoff\"]\n",
    "                effective_train_idx = train_idx[train_idx >= cutoff]\n",
    "                if len(effective_train_idx) == 0:\n",
    "                    continue\n",
    "                subset_X = X.iloc[cutoff:].reset_index(drop=True)\n",
    "                subset_y = y[cutoff:].reset_index(drop=True)\n",
    "                rel_idx = train_idx[train_idx >= cutoff] - cutoff\n",
    "\n",
    "                X_train = subset_X.iloc[rel_idx]\n",
    "                y_train = subset_y.iloc[rel_idx]\n",
    "                sw = create_time_decay_weights(len(subset_X))[rel_idx]\n",
    "\n",
    "                model = clone(self.model)\n",
    "                print(f\"  â³ Training slice: {slice_name}, samples: {len(X_train)}\")\n",
    "                model.fit(X_train, y_train, sample_weight=sw)\n",
    "\n",
    "                fold_folder = os.path.join(self.models_folder, f\"fold{fold_idx}_{slice_name}\")\n",
    "                os.makedirs(fold_folder, exist_ok=True)\n",
    "                fold_path = os.path.join(fold_folder, f\"{self.model.__class__.__name__}.pkl\")\n",
    "                joblib.dump(model, fold_path)\n",
    "                print(f\"  ğŸ“¦ Saved {slice_name} model to {fold_path}\")\n",
    "\n",
    "                y_preds = model.predict(X_val)\n",
    "                valid_mask = val_idx >= cutoff\n",
    "                if valid_mask.any():\n",
    "                    masked_idx = val_idx[valid_mask]\n",
    "                    oof_preds_slices[slice_name][masked_idx] = y_preds[valid_mask]\n",
    "                if cutoff > 0 and (~valid_mask).any():\n",
    "                    oof_preds_slices[slice_name][val_idx[~valid_mask]] = oof_preds_slices[\"full_data\"][val_idx[~valid_mask]]\n",
    "                y_train_preds = model.predict(X_train)\n",
    "                temp_test_preds = model.predict(X_test)\n",
    "                test_preds_slices[slice_name] += temp_test_preds / 3\n",
    "\n",
    "                fold_score = pearsonr(y_val, y_preds)[0]\n",
    "                train_score = pearsonr(y_train, y_train_preds)[0]\n",
    "                fold_scores.append(fold_score)\n",
    "                train_fold_scores.append(train_score)\n",
    "\n",
    "                print(f\"  âœ… {slice_name} - val_pearson: {fold_score:.6f} - train_pearson: {train_score:.6f}\")\n",
    "\n",
    "                del X_train, y_train, y_preds, model, temp_test_preds\n",
    "                gc.collect()\n",
    "\n",
    "        # Simple ensemble (mean)\n",
    "        oof_simple = np.mean(list(oof_preds_slices.values()), axis=0)\n",
    "        test_simple = np.mean(list(test_preds_slices.values()), axis=0)\n",
    "        score_simple = pearsonr(y, oof_simple)[0]\n",
    "\n",
    "        # Weighted ensemble (based on pearson score)\n",
    "        slice_scores = {s: pearsonr(y, oof_preds_slices[s])[0] for s in oof_preds_slices}\n",
    "        total_score = sum(slice_scores.values())\n",
    "        oof_weighted = sum(slice_scores[s] / total_score * oof_preds_slices[s] for s in slice_scores)\n",
    "        test_weighted = sum(slice_scores[s] / total_score * test_preds_slices[s] for s in slice_scores)\n",
    "        score_weighted = pearsonr(y, oof_weighted)[0]\n",
    "\n",
    "        print(f\"\\nSimple Ensemble Pearson:   {score_simple:.6f}\")\n",
    "        print(f\"Weighted Ensemble Pearson: {score_weighted:.6f}\")\n",
    "\n",
    "        overall_score = pearsonr(y, oof_simple)[0]\n",
    "        mean_score = np.mean(fold_scores)\n",
    "        std_score = np.std(fold_scores)\n",
    "\n",
    "        print(f\"\\n------ Overall Score: {overall_score:.6f} - Mean val_pearson: {mean_score:.6f} Â± {std_score:.6f} - Mean train_pearson: {np.mean(train_fold_scores):.6f} Â± {np.std(train_fold_scores):.6f} ------\\n\")\n",
    "\n",
    "        return oof_weighted, test_weighted, fold_scores, train_fold_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "213109c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨˜æ†¶é«”è®ŠåŒ–: 615.30 MB\n",
      "Polarsä¼°ç®— DataFrame å¤§å°: 11598.53 MB\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# è®€å–\n",
    "train_filled = pl.read_parquet(\"train_filled.parquet\")\n",
    "\n",
    "# è¦æ’é™¤çš„æ¬„ä½\n",
    "exclude_cols = [\n",
    "    'Id', 'ranker_id', 'selected', 'profileId', 'requestDate',\n",
    "    'miniRules0_percentage', 'miniRules1_percentage',\n",
    "    'pricingInfo_passengerCount'\n",
    "]\n",
    "\n",
    "feature_cols = [c for c in train_filled.columns if c not in exclude_cols]\n",
    "\n",
    "# è‡ªå‹•æ‰¾ float64 æ¬„ä½\n",
    "float64_cols = [\n",
    "    c for c in feature_cols\n",
    "    if train_filled[c].dtype == pl.Float64\n",
    "]\n",
    "\n",
    "# è½‰æˆ float32\n",
    "import psutil\n",
    "\n",
    "process = psutil.Process()\n",
    "before_mem = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "# åšæŸäº›æ“ä½œ\n",
    "train_filled = train_filled.with_columns(\n",
    "    [pl.col(c).cast(pl.Float32) for c in float64_cols]\n",
    ")\n",
    "\n",
    "after_mem = process.memory_info().rss / (1024 * 1024)\n",
    "print(f\"è¨˜æ†¶é«”è®ŠåŒ–: {after_mem - before_mem:.2f} MB\")\n",
    "print(f\"Polarsä¼°ç®— DataFrame å¤§å°: {train_filled.estimated_size() / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a83b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆªæ‰å¾Œé‚„æœ‰ 6363568 è¡Œè³‡æ–™\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import polars as pl\n",
    "\n",
    "# ç¬¬ä¸€æ­¥ï¼šè¨ˆç®—æ¯å€‹ranker_idçš„å‡ºç¾æ¬¡æ•¸\n",
    "group_counts = (\n",
    "    train_filled\n",
    "    .group_by(\"ranker_id\")\n",
    "    .agg(pl.count().alias(\"n_rows\"))\n",
    ")\n",
    "\n",
    "# ç¬¬äºŒæ­¥ï¼šæŒ‘é¸ >=10 çš„ ranker_id\n",
    "valid_ranker_ids = (\n",
    "    group_counts\n",
    "    .filter(pl.col(\"n_rows\") >= 1000)\n",
    "    .select(\"ranker_id\")\n",
    ")\n",
    "\n",
    "# ç¬¬ä¸‰æ­¥ï¼šéæ¿¾æ‰ <10 çš„\n",
    "train_filled = (\n",
    "    train_filled\n",
    "    .join(valid_ranker_ids, on=\"ranker_id\", how=\"inner\")\n",
    ")\n",
    "\n",
    "print(f\"åˆªæ‰å¾Œé‚„æœ‰ {train_filled.height} è¡Œè³‡æ–™\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d9a00d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarsä¼°ç®— DataFrame å¤§å°: 4067.59 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "after_mem = process.memory_info().rss / (1024 * 1024)\n",
    "print(f\"Polarsä¼°ç®— DataFrame å¤§å°: {train_filled.estimated_size() / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e661223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 160 features\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_cols = [col for col in train_filled.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Using {len(feature_cols)} features\")\n",
    "\n",
    "X = train_filled.select(feature_cols)\n",
    "y = train_filled.select('selected')\n",
    "groups = train_filled.select('ranker_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ea68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# ç¢ºèªé€™äº›ç‰©ä»¶éƒ½æ˜¯Polars DataFrame\n",
    "# X, y, groups\n",
    "# éƒ½æ˜¯ shape [n_rows, n_cols]\n",
    "\n",
    "# å…ˆæŠŠ ranker_idè½‰list\n",
    "unique_rankers = groups.select(\"ranker_id\").unique().to_series().to_list()\n",
    "\n",
    "# æ‰“äº‚\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(unique_rankers)\n",
    "\n",
    "# åˆ‡8:2\n",
    "n_train = int(0.8 * len(unique_rankers))\n",
    "train_rankers = set(unique_rankers[:n_train])\n",
    "val_rankers = set(unique_rankers[n_train:])\n",
    "\n",
    "# ç”¨ Polars éæ¿¾ train/val\n",
    "is_train = groups.select(pl.col(\"ranker_id\").is_in(list(train_rankers)).alias(\"is_train\"))\n",
    "\n",
    "# å…ˆ concat mask\n",
    "X_with_mask = X.with_columns(is_train)\n",
    "y_with_mask = y.with_columns(is_train)\n",
    "groups_with_mask = groups.with_columns(is_train)\n",
    "\n",
    "# åˆ†å‰² DataFrame\n",
    "X_train_df = X_with_mask.filter(pl.col(\"is_train\"))\n",
    "X_val_df = X_with_mask.filter(~pl.col(\"is_train\"))\n",
    "y_train_df = y_with_mask.filter(pl.col(\"is_train\"))\n",
    "y_val_df = y_with_mask.filter(~pl.col(\"is_train\"))\n",
    "groups_train_df = groups_with_mask.filter(pl.col(\"is_train\"))\n",
    "groups_val_df = groups_with_mask.filter(~pl.col(\"is_train\"))\n",
    "\n",
    "# å†è½‰ numpy (åˆ†æ‰¹)\n",
    "X_train_np = X_train_df.drop(\"is_train\").to_numpy()\n",
    "X_val_np = X_val_df.drop(\"is_train\").to_numpy()\n",
    "y_train_np = y_train_df.drop(\"is_train\").to_numpy().flatten()\n",
    "y_val_np = y_val_df.drop(\"is_train\").to_numpy().flatten()\n",
    "groups_train_np = groups_train_df.drop(\"is_train\").to_numpy().flatten()\n",
    "groups_val_np = groups_val_df.drop(\"is_train\").to_numpy().flatten()\n",
    "\n",
    "# æœ€å¾Œè¨ˆç®— group sizes\n",
    "group_sizes_train = (\n",
    "    groups_train_df.drop(\"is_train\")\n",
    "    .group_by(\"ranker_id\")\n",
    "    .agg(pl.len().alias(\"size\"))\n",
    "    .sort(\"ranker_id\")[\"size\"]\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "group_sizes_val = (\n",
    "    groups_val_df.drop(\"is_train\")\n",
    "    .group_by(\"ranker_id\")\n",
    "    .agg(pl.len().alias(\"size\"))\n",
    "    .sort(\"ranker_id\")[\"size\"]\n",
    "    .to_numpy()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedc82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# DMatrix å»ºç«‹ (ä¸å†ç”¨ X.columns)\n",
    "dtrain = xgb.DMatrix(\n",
    "    X_train_np,\n",
    "    label=y_train_np,\n",
    ")\n",
    "dtrain.set_group(group_sizes_train)\n",
    "\n",
    "dval = xgb.DMatrix(\n",
    "    X_val_np,\n",
    "    label=y_val_np,\n",
    ")\n",
    "dval.set_group(group_sizes_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28b43402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-ndcg@3:0.40046\tval-ndcg@3:0.36380\n",
      "[20]\ttrain-ndcg@3:0.53183\tval-ndcg@3:0.40077\n",
      "[40]\ttrain-ndcg@3:0.56886\tval-ndcg@3:0.40368\n",
      "[60]\ttrain-ndcg@3:0.60412\tval-ndcg@3:0.40785\n",
      "[80]\ttrain-ndcg@3:0.63796\tval-ndcg@3:0.41526\n",
      "[100]\ttrain-ndcg@3:0.66802\tval-ndcg@3:0.41978\n",
      "[120]\ttrain-ndcg@3:0.70568\tval-ndcg@3:0.42278\n",
      "[140]\ttrain-ndcg@3:0.73823\tval-ndcg@3:0.42878\n",
      "[160]\ttrain-ndcg@3:0.76533\tval-ndcg@3:0.43217\n",
      "[180]\ttrain-ndcg@3:0.78308\tval-ndcg@3:0.43459\n",
      "[200]\ttrain-ndcg@3:0.80294\tval-ndcg@3:0.43787\n",
      "[220]\ttrain-ndcg@3:0.81615\tval-ndcg@3:0.44211\n",
      "[240]\ttrain-ndcg@3:0.83163\tval-ndcg@3:0.44897\n",
      "[260]\ttrain-ndcg@3:0.84075\tval-ndcg@3:0.44923\n",
      "[280]\ttrain-ndcg@3:0.85239\tval-ndcg@3:0.45233\n",
      "[300]\ttrain-ndcg@3:0.86052\tval-ndcg@3:0.45739\n",
      "[320]\ttrain-ndcg@3:0.86722\tval-ndcg@3:0.45941\n",
      "[340]\ttrain-ndcg@3:0.87219\tval-ndcg@3:0.45965\n",
      "[360]\ttrain-ndcg@3:0.87792\tval-ndcg@3:0.45902\n",
      "[380]\ttrain-ndcg@3:0.88167\tval-ndcg@3:0.45837\n",
      "[400]\ttrain-ndcg@3:0.88669\tval-ndcg@3:0.46120\n",
      "[420]\ttrain-ndcg@3:0.89189\tval-ndcg@3:0.46131\n",
      "[440]\ttrain-ndcg@3:0.89563\tval-ndcg@3:0.46267\n",
      "[460]\ttrain-ndcg@3:0.89891\tval-ndcg@3:0.46356\n",
      "[480]\ttrain-ndcg@3:0.90250\tval-ndcg@3:0.46277\n",
      "[499]\ttrain-ndcg@3:0.90573\tval-ndcg@3:0.46252\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# åƒæ•¸\n",
    "xgb_params = {\n",
    "    'objective': 'rank:pairwise',\n",
    "    'eval_metric': 'ndcg@3',\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 10,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'lambda': 10.0,\n",
    "    'learning_rate': 0.05,\n",
    "    'seed': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "# è¨“ç·´\n",
    "xgb_model = xgb.train(\n",
    "    xgb_params,\n",
    "    dtrain,\n",
    "    num_boost_round=500,\n",
    "    evals=[(dtrain, \"train\"), (dval, \"val\")],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09b4b212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å„²å­˜æ¨¡å‹åˆ° 'xgb_ranker_model.bin'\n"
     ]
    }
   ],
   "source": [
    "# å­˜æˆäºŒé€²ä½æ¨¡å‹\n",
    "xgb_model.save_model(\"xgb_ranker_model.bin\")\n",
    "print(\"âœ… å·²å„²å­˜æ¨¡å‹åˆ° 'xgb_ranker_model.bin'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65f784f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â­ Top 20 Feature Importances (by weight):\n",
      "totalPrice: 8578.0\n",
      "companyID: 6165.0\n",
      "legs0_segments0_flightNumber: 6092.0\n",
      "legs1_segments0_flightNumber: 5554.0\n",
      "days_before_departure: 5454.0\n",
      "legs1_segments0_seatsAvailable: 3737.0\n",
      "legs0_segments0_seatsAvailable: 3721.0\n",
      "legs0_arrivalAt_hour: 3577.0\n",
      "legs1_arrivalAt_hour: 3280.0\n",
      "legs0_departureAt_hour: 3152.0\n",
      "legs1_departureAt_hour: 2996.0\n",
      "corporateTariffCode: 2835.0\n",
      "taxes: 2525.0\n",
      "legs1_departureAt_weekday: 1990.0\n",
      "legs0_departureAt_weekday: 1817.0\n",
      "legs0_arrivalAt_weekday: 1588.0\n",
      "legs1_arrivalAt_weekday: 1501.0\n",
      "frequentFlyer: 1446.0\n",
      "legs1_duration: 1376.0\n",
      "legs0_duration: 1354.0\n"
     ]
    }
   ],
   "source": [
    "# å–å¾—é‡è¦æ€§ (index key: f0, f1, ...)\n",
    "importance_dict = xgb_model.get_score(importance_type=\"weight\")\n",
    "\n",
    "# æŠŠç‰¹å¾µåç¨±å°æ‡‰å› X.columns\n",
    "# å¦‚æœXæ˜¯ Polars DataFrame\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "# é‡æ–°å‘½å\n",
    "importance_named = {}\n",
    "for k, v in importance_dict.items():\n",
    "    # k = 'f0', 'f1', ...\n",
    "    idx = int(k[1:])  # å–æ•¸å­—\n",
    "    real_name = feature_names[idx]\n",
    "    importance_named[real_name] = v\n",
    "\n",
    "# æ’åº\n",
    "sorted_importance = sorted(importance_named.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# åˆ—å°\n",
    "print(\"â­ Top 20 Feature Importances (by weight):\")\n",
    "for feat, score in sorted_importance[:20]:\n",
    "    print(f\"{feat}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21c37925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        feature  weight_rank  gain_rank  \\\n",
      "76                                   totalPrice       8578.0   4.235002   \n",
      "22                   legs0_segments0_cabinClass        109.0  42.103062   \n",
      "1                                     companyID       6165.0   3.913192   \n",
      "34   legs0_segments1_departureFrom_airport_iata          1.0  41.605595   \n",
      "26        legs0_segments0_marketingCarrier_code        153.0   3.633880   \n",
      "62   legs1_segments1_departureFrom_airport_iata          2.0   7.080189   \n",
      "25                 legs0_segments0_flightNumber       6092.0   4.140559   \n",
      "0                              both_legs_direct         35.0  21.166515   \n",
      "60  legs1_segments1_arrivalTo_airport_city_iata          1.0  18.557640   \n",
      "55                 legs1_segments0_flightNumber       5554.0   4.186655   \n",
      "16                           legs0_main_carrier         22.0   3.867804   \n",
      "3                         days_before_departure       5454.0   3.778232   \n",
      "20    legs0_segments0_baggageAllowance_quantity        367.0  18.194492   \n",
      "52                   legs1_segments0_cabinClass         82.0   6.713810   \n",
      "30  legs0_segments1_arrivalTo_airport_city_iata         85.0  14.179173   \n",
      "58               legs1_segments0_seatsAvailable       3737.0   3.428240   \n",
      "32    legs0_segments1_baggageAllowance_quantity         13.0   9.738872   \n",
      "28               legs0_segments0_seatsAvailable       3721.0   3.500748   \n",
      "66                    miniRules0_monetaryAmount        387.0  13.252401   \n",
      "9                          legs0_arrivalAt_hour       3577.0   4.667577   \n",
      "51    legs1_segments0_baggageAllowance_quantity        337.0   9.030958   \n",
      "40                         legs1_arrivalAt_hour       3280.0   4.884501   \n",
      "13                       legs0_departureAt_hour       3152.0   4.933882   \n",
      "36                 legs0_segments1_flightNumber        113.0   6.896737   \n",
      "71                                  nationality         56.0   4.966136   \n",
      "44                       legs1_departureAt_hour       2996.0   4.557615   \n",
      "64                 legs1_segments1_flightNumber         96.0   4.868381   \n",
      "2                           corporateTariffCode       2835.0   4.091713   \n",
      "47                           legs1_main_carrier         33.0   4.998295   \n",
      "53   legs1_segments0_departureFrom_airport_iata        760.0   6.379269   \n",
      "75                                        taxes       2525.0   4.494143   \n",
      "45                    legs1_departureAt_weekday       1990.0   4.234576   \n",
      "31       legs0_segments1_arrivalTo_airport_iata         20.0   6.194692   \n",
      "68                    miniRules1_monetaryAmount        627.0   6.089478   \n",
      "72                       pricingInfo_isAccessTP        666.0   5.068095   \n",
      "14                    legs0_departureAt_weekday       1817.0   3.998265   \n",
      "10                      legs0_arrivalAt_weekday       1588.0   4.056602   \n",
      "15                               legs0_duration       1354.0   5.694603   \n",
      "54                     legs1_segments0_duration       1025.0   5.470428   \n",
      "41                      legs1_arrivalAt_weekday       1501.0   4.133005   \n",
      "8                    legs0_arrivalAt_day_period        108.0   4.562662   \n",
      "4                                 frequentFlyer       1446.0   3.581264   \n",
      "69                       miniRules1_statusInfos        203.0   5.436374   \n",
      "19       legs0_segments0_arrivalTo_airport_iata        640.0   5.419785   \n",
      "46                               legs1_duration       1376.0   5.175110   \n",
      "56        legs1_segments0_marketingCarrier_code         96.0   3.987250   \n",
      "17                legs0_segments0_aircraft_code       1332.0   4.612083   \n",
      "73                                  searchRoute        478.0   5.087680   \n",
      "48                legs1_segments0_aircraft_code       1112.0   3.876847   \n",
      "6                                         isVip        195.0   5.082795   \n",
      "\n",
      "     cover_rank  \n",
      "76  1051.144165  \n",
      "22  8636.604492  \n",
      "1    557.099731  \n",
      "34  1809.537231  \n",
      "26  4330.587402  \n",
      "62  3745.372803  \n",
      "25   611.121948  \n",
      "0   2513.184814  \n",
      "60  1159.031616  \n",
      "55   713.426208  \n",
      "16  3589.774902  \n",
      "3    406.564484  \n",
      "20  3101.126709  \n",
      "52  3487.523926  \n",
      "30  2815.055420  \n",
      "58   373.021179  \n",
      "32  3072.663086  \n",
      "28   348.634918  \n",
      "66  2770.843994  \n",
      "9    748.977051  \n",
      "51  1435.975708  \n",
      "40   731.923035  \n",
      "13   998.648132  \n",
      "36   751.836304  \n",
      "71  2507.800049  \n",
      "44   835.805725  \n",
      "64  2271.052979  \n",
      "2    586.894470  \n",
      "47  2204.699951  \n",
      "53  1588.734009  \n",
      "75   717.460022  \n",
      "45   435.478180  \n",
      "31  1545.135498  \n",
      "68   854.931519  \n",
      "72  1675.085205  \n",
      "14   630.044739  \n",
      "10   683.635254  \n",
      "15   729.967468  \n",
      "54  1176.345093  \n",
      "41   537.996094  \n",
      "8   1456.471558  \n",
      "4    387.877686  \n",
      "69   925.390869  \n",
      "19   750.591248  \n",
      "46   723.918091  \n",
      "56  1380.463501  \n",
      "17   873.642334  \n",
      "73  1161.782471  \n",
      "48   615.264282  \n",
      "6   1061.007690  \n",
      "âœ… å·²è¼¸å‡º feature_importance_top50.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# å…ˆæº–å‚™ç‰¹å¾µåç¨±\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "# å–ä¸‰ç¨®é‡è¦æ€§\n",
    "importance_types = [\"weight\", \"gain\", \"cover\"]\n",
    "importance_all = {}\n",
    "\n",
    "for imp_type in importance_types:\n",
    "    imp_raw = xgb_model.get_score(importance_type=imp_type)\n",
    "    imp_named = {}\n",
    "    for k, v in imp_raw.items():\n",
    "        idx = int(k[1:])\n",
    "        real_name = feature_names[idx]\n",
    "        imp_named[real_name] = v\n",
    "    # æ’åº\n",
    "    sorted_imp = sorted(imp_named.items(), key=lambda x: x[1], reverse=True)\n",
    "    importance_all[imp_type] = sorted_imp\n",
    "\n",
    "# æŠŠä¸‰å€‹æ¦œå–®æ”¾æˆDataFrameæ–¹ä¾¿æ¯”å°\n",
    "df_weight = pd.DataFrame(importance_all[\"weight\"], columns=[\"feature\", \"weight_rank\"])\n",
    "df_weight[\"weight_rank_pos\"] = df_weight.index\n",
    "\n",
    "df_gain = pd.DataFrame(importance_all[\"gain\"], columns=[\"feature\", \"gain_rank\"])\n",
    "df_gain[\"gain_rank_pos\"] = df_gain.index\n",
    "\n",
    "df_cover = pd.DataFrame(importance_all[\"cover\"], columns=[\"feature\", \"cover_rank\"])\n",
    "df_cover[\"cover_rank_pos\"] = df_cover.index\n",
    "\n",
    "# åˆä½µ\n",
    "df_merged = (\n",
    "    df_weight\n",
    "    .merge(df_gain, on=\"feature\", how=\"outer\")\n",
    "    .merge(df_cover, on=\"feature\", how=\"outer\")\n",
    ")\n",
    "\n",
    "# æŠŠä¸å­˜åœ¨çš„rankè£œå¤§æ•¸å­—\n",
    "df_merged[\"weight_rank_pos\"] = df_merged[\"weight_rank_pos\"].fillna(9999)\n",
    "df_merged[\"gain_rank_pos\"] = df_merged[\"gain_rank_pos\"].fillna(9999)\n",
    "df_merged[\"cover_rank_pos\"] = df_merged[\"cover_rank_pos\"].fillna(9999)\n",
    "\n",
    "# è¨ˆç®—ã€Œä¸‰å€‹æ¦œå–®ä¸­æœ€æ—©å‡ºç¾çš„ä½ç½®ã€\n",
    "df_merged[\"min_rank\"] = df_merged[[\"weight_rank_pos\", \"gain_rank_pos\", \"cover_rank_pos\"]].min(axis=1)\n",
    "\n",
    "# æ’åº\n",
    "df_merged_sorted = df_merged.sort_values(\"min_rank\")\n",
    "\n",
    "# å–å‰50\n",
    "top50 = df_merged_sorted.head(50)\n",
    "\n",
    "# é¡¯ç¤º\n",
    "print(top50[[\"feature\", \"weight_rank\", \"gain_rank\", \"cover_rank\"]])\n",
    "\n",
    "# å¦‚æœæƒ³è¼¸å‡ºCSV\n",
    "top50.to_csv(\"feature_importance_top50.csv\", index=False)\n",
    "print(\"âœ… å·²è¼¸å‡º feature_importance_top50.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5605a248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è³‡æ–™åˆ‡åˆ†å®Œæˆ\n",
      "Train: 6363568 rows, 3160 groups\n",
      "Val: 3580308 rows, 17930 groups\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# è®€å–\n",
    "train_filled = pl.read_parquet(\"train_filled.parquet\")\n",
    "\n",
    "exclude_cols = [\n",
    "    'Id', 'ranker_id', 'selected', 'profileId', 'requestDate',\n",
    "    'miniRules0_percentage', 'miniRules1_percentage',\n",
    "    'pricingInfo_passengerCount'\n",
    "]\n",
    "\n",
    "feature_cols = [c for c in train_filled.columns if c not in exclude_cols]\n",
    "\n",
    "# è½‰ float32\n",
    "float64_cols = [\n",
    "    c for c in feature_cols\n",
    "    if train_filled[c].dtype == pl.Float64\n",
    "]\n",
    "train_filled = train_filled.with_columns(\n",
    "    [pl.col(c).cast(pl.Float32) for c in float64_cols]\n",
    ")\n",
    "\n",
    "# é å…ˆè¨ˆç®— group size\n",
    "group_counts = (\n",
    "    train_filled\n",
    "    .group_by(\"ranker_id\")\n",
    "    .agg(pl.count().alias(\"n_rows\"))\n",
    ")\n",
    "\n",
    "# é¸ >=1000 ç‚º train\n",
    "train_ranker_ids = (\n",
    "    group_counts\n",
    "    .filter(pl.col(\"n_rows\") >= 1000)\n",
    "    .select(\"ranker_id\")\n",
    "    .to_series()\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "# é¸ >10 ç‚º valid pool\n",
    "valid_ranker_ids = (\n",
    "    group_counts\n",
    "    .filter(pl.col(\"n_rows\") > 10)\n",
    "    .select(\"ranker_id\")\n",
    "    .to_series()\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "# Shuffle valid_rankers\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(valid_ranker_ids)\n",
    "\n",
    "# åˆ‡åˆ† valid 80:20\n",
    "n_valid_train = int(len(valid_ranker_ids) * 0.8)\n",
    "valid_train_rankers = set(valid_ranker_ids[:n_valid_train])\n",
    "valid_val_rankers = set(valid_ranker_ids[n_valid_train:])\n",
    "\n",
    "# æ¨™è¨˜æ¯ä¸€è¡Œå±¬æ–¼ train/val\n",
    "train_mask = train_filled.select(\n",
    "    pl.col(\"ranker_id\").is_in(train_ranker_ids).alias(\"is_train\")\n",
    ")\n",
    "val_mask = train_filled.select(\n",
    "    pl.col(\"ranker_id\").is_in(list(valid_val_rankers)).alias(\"is_val\")\n",
    ")\n",
    "\n",
    "# X,y,groups\n",
    "X = train_filled.select(feature_cols)\n",
    "y = train_filled.select('selected')\n",
    "groups = train_filled.select('ranker_id')\n",
    "\n",
    "# concat mask\n",
    "# å»ºç«‹ mask\n",
    "train_mask_df = train_filled.select(\n",
    "    pl.col(\"ranker_id\").is_in(train_ranker_ids).alias(\"is_train\")\n",
    ")\n",
    "val_mask_df = train_filled.select(\n",
    "    pl.col(\"ranker_id\").is_in(list(valid_val_rankers)).alias(\"is_val\")\n",
    ")\n",
    "\n",
    "# å– Series\n",
    "train_mask = train_mask_df[\"is_train\"]\n",
    "val_mask = val_mask_df[\"is_val\"]\n",
    "\n",
    "# åŠ é€² DataFrame\n",
    "X_with_mask = X.with_columns([train_mask, val_mask])\n",
    "y_with_mask = y.with_columns([train_mask, val_mask])\n",
    "groups_with_mask = groups.with_columns([train_mask, val_mask])\n",
    "\n",
    "\n",
    "# åˆ†å‡ºtrain\n",
    "X_train_df = X_with_mask.filter(pl.col(\"is_train\"))\n",
    "y_train_df = y_with_mask.filter(pl.col(\"is_train\"))\n",
    "groups_train_df = groups_with_mask.filter(pl.col(\"is_train\"))\n",
    "\n",
    "# åˆ†å‡ºval\n",
    "X_val_df = X_with_mask.filter(pl.col(\"is_val\"))\n",
    "y_val_df = y_with_mask.filter(pl.col(\"is_val\"))\n",
    "groups_val_df = groups_with_mask.filter(pl.col(\"is_val\"))\n",
    "\n",
    "# è½‰numpy\n",
    "X_train_np = X_train_df.drop([\"is_train\", \"is_val\"]).to_numpy()\n",
    "y_train_np = y_train_df.drop([\"is_train\", \"is_val\"]).to_numpy().flatten()\n",
    "groups_train_np = groups_train_df.drop([\"is_train\", \"is_val\"]).to_numpy().flatten()\n",
    "\n",
    "X_val_np = X_val_df.drop([\"is_train\", \"is_val\"]).to_numpy()\n",
    "y_val_np = y_val_df.drop([\"is_train\", \"is_val\"]).to_numpy().flatten()\n",
    "groups_val_np = groups_val_df.drop([\"is_train\", \"is_val\"]).to_numpy().flatten()\n",
    "\n",
    "# è¨ˆç®— group sizes\n",
    "group_sizes_train = (\n",
    "    pl.DataFrame({\"ranker_id\": groups_train_np})\n",
    "    .group_by(\"ranker_id\")\n",
    "    .agg(pl.len().alias(\"size\"))\n",
    "    .sort(\"ranker_id\")[\"size\"]\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "group_sizes_val = (\n",
    "    pl.DataFrame({\"ranker_id\": groups_val_np})\n",
    "    .group_by(\"ranker_id\")\n",
    "    .agg(pl.len().alias(\"size\"))\n",
    "    .sort(\"ranker_id\")[\"size\"]\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "print(\"âœ… è³‡æ–™åˆ‡åˆ†å®Œæˆ\")\n",
    "print(f\"Train: {X_train_np.shape[0]} rows, {len(np.unique(groups_train_np))} groups\")\n",
    "print(f\"Val: {X_val_np.shape[0]} rows, {len(np.unique(groups_val_np))} groups\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636abe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31måœ¨ç›®å‰å„²å­˜æ ¼æˆ–ä¸Šä¸€å€‹å„²å­˜æ ¼ä¸­åŸ·è¡Œç¨‹å¼ç¢¼æ™‚ï¼ŒKernel å·²ææ¯€ã€‚\n",
      "\u001b[1;31mè«‹æª¢é–±å„²å­˜æ ¼ä¸­çš„ç¨‹å¼ç¢¼ï¼Œæ‰¾å‡ºå¤±æ•—çš„å¯èƒ½åŸå› ã€‚\n",
      "\u001b[1;31må¦‚éœ€è©³ç´°è³‡è¨Šï¼Œè«‹æŒ‰ä¸€ä¸‹<a href='https://aka.ms/vscodeJupyterKernelCrash'>é€™è£¡</a>ã€‚\n",
      "\u001b[1;31må¦‚éœ€è©³ç´°è³‡æ–™ï¼Œè«‹æª¢è¦– Jupyter <a href='command:jupyter.viewOutput'>è¨˜éŒ„</a>ã€‚"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# DMatrix å»ºç«‹ (ä¸å†ç”¨ X.columns)\n",
    "dtrain = xgb.DMatrix(\n",
    "    X_train_np,\n",
    "    label=y_train_np,\n",
    ")\n",
    "dtrain.set_group(group_sizes_train)\n",
    "\n",
    "dval = xgb.DMatrix(\n",
    "    X_val_np,\n",
    "    label=y_val_np,\n",
    ")\n",
    "dval.set_group(group_sizes_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd324ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-ndcg@3:0.38412\tval-ndcg@3:0.67881\n",
      "[20]\ttrain-ndcg@3:0.51979\tval-ndcg@3:0.69860\n",
      "[40]\ttrain-ndcg@3:0.54846\tval-ndcg@3:0.70078\n",
      "[60]\ttrain-ndcg@3:0.58042\tval-ndcg@3:0.70415\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# åƒæ•¸\n",
    "xgb_params = {\n",
    "    'objective': 'rank:pairwise',\n",
    "    'eval_metric': 'ndcg@3',\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 10,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'lambda': 10.0,\n",
    "    'learning_rate': 0.05,\n",
    "    'seed': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "# è¨“ç·´\n",
    "xgb_model = xgb.train(\n",
    "    xgb_params,\n",
    "    dtrain,\n",
    "    num_boost_round=500,\n",
    "    evals=[(dtrain, \"train\"), (dval, \"val\")],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1904373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "model = xgb.Booster()\n",
    "model.load_model(\"xgb_ranker_model.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48a5b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict(dval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4ba06d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'groups_val_np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhitrate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_hitrate_at_3\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# é æ¸¬\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# è¨ˆç®— HitRate\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m compute_hitrate_at_3(\u001b[43mgroups_val_np\u001b[49m, y_val_np, val_preds)\n",
      "\u001b[31mNameError\u001b[39m: name 'groups_val_np' is not defined"
     ]
    }
   ],
   "source": [
    "from scripts.hitrate import compute_hitrate_at_3\n",
    "# é æ¸¬\n",
    "# è¨ˆç®— HitRate\n",
    "compute_hitrate_at_3(groups_val_np, y_val_np, val_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa38f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# è®€å–\n",
    "train_filled = pl.read_parquet(\"train_filled.parquet\")\n",
    "\n",
    "# æ’é™¤æ¬„\n",
    "exclude_cols = [\n",
    "    'Id', 'ranker_id', 'selected', 'profileId', 'requestDate',\n",
    "    'miniRules0_percentage', 'miniRules1_percentage',\n",
    "    'pricingInfo_passengerCount'\n",
    "]\n",
    "feature_cols = [c for c in train_filled.columns if c not in exclude_cols]\n",
    "\n",
    "# è½‰ float32\n",
    "float64_cols = [c for c in feature_cols if train_filled[c].dtype == pl.Float64]\n",
    "train_filled = train_filled.with_columns(\n",
    "    [pl.col(c).cast(pl.Float32) for c in float64_cols]\n",
    ")\n",
    "\n",
    "# è¨ˆç®—group size\n",
    "group_counts = (\n",
    "    train_filled\n",
    "    .group_by(\"ranker_id\")\n",
    "    .agg(pl.count().alias(\"n_rows\"))\n",
    ")\n",
    "\n",
    "# æ¨™è¨»åˆ†ç¾¤\n",
    "group_counts = group_counts.with_columns([\n",
    "    pl.when(pl.col(\"n_rows\") <= 19).then(\"small\")\n",
    "     .when((pl.col(\"n_rows\") >=20) & (pl.col(\"n_rows\") <=154)).then(\"medium\")\n",
    "     .otherwise(\"large\")\n",
    "     .alias(\"group_category\")\n",
    "])\n",
    "\n",
    "# æŠŠ group_category join å›å»\n",
    "train_filled = train_filled.join(group_counts, on=\"ranker_id\", how=\"left\")\n",
    "\n",
    "# ç¢ºèªæœ‰æ¨™ç±¤\n",
    "print(train_filled.select(\"group_category\").unique())\n",
    "\n",
    "# ä¾åˆ†ç¾¤åˆ‡åˆ†\n",
    "data_small = train_filled.filter(pl.col(\"group_category\") == \"small\")\n",
    "data_medium = train_filled.filter(pl.col(\"group_category\") == \"medium\")\n",
    "data_large = train_filled.filter(pl.col(\"group_category\") == \"large\")\n",
    "\n",
    "print(f\"Small: {data_small.height} rows\")\n",
    "print(f\"Medium: {data_medium.height} rows\")\n",
    "print(f\"Large: {data_large.height} rows\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FlightRank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
