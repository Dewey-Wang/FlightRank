{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "213109c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DataFrame ç¾åœ¨åªæœ‰ 83 æ¬„ä½: ['price_percentile', 'total_is_min_transfers', 'legs0_is_min_transfers', 'legs0_num_transfers_rank', 'legs0_segments0_flightNumber', 'price_per_duration_rank', 'total_num_transfers_rank', 'price_from_median_zscore', 'price_per_duration', 'legs1_num_transfers_rank', 'legs1_is_min_transfers', 'free_exchange', 'legs0_departureAt_hour', 'price_minus_fee_rank', 'totalPrice_rank', 'legs0_arrivalAt_hour', 'price_per_fee', 'price_per_fee_rank', 'legs0_mean_cabin', 'legs0_weighted_mean_cabin', 'companyID_loo_mean_legs0_departureAt_hour', 'legs0_max_duration_cabin', 'days_before_departure', 'companyID_loo_mean_legs0_arrivalAt_hour', 'price_per_tax', 'legs0_segments0_baggageAllowance_quantity', 'companyID_total_occurrences', 'group_size', 'baggage_total_rank', 'leg0_view_diff_mean', 'legs0_segments0_cabinClass', 'companyID_loo_mean_totalPrice', 'companyID_loo_selected_count', 'miniRules0_monetaryAmount', 'legs1_mean_cabin', 'legs1_segments0_baggageAllowance_quantity', 'all_view_diff_mean', 'both_legs_carrier_all_same', 'tax_rate', 'price_minus_fee', 'is_major_carrier', 'companyID_loo_mean_taxes', 'log_price', 'pricingInfo_isAccessTP', 'total_num_transfers', 'baggage_total', 'legs1_departureAt_day_period', 'total_weighted_mean_cabin', 'legs1_departureAt_hour', 'duration_ratio', 'has_access_tp', 'companyID_loo_mean_legs0_duration', 'free_cancel', 'legs1_segments0_flightNumber', 'miniRules1_monetaryAmount', 'has_fees', 'group_size_log', 'legs1_segments0_marketingCarrier_code', 'legs1_arrivalAt_day_period', 'total_duration', 'has_baggage', 'legs0_segments0_seatsAvailable', 'legs1_arrivalAt_hour', 'companyID_loo_mean_legs1_departureAt_hour', 'isVip', 'unmatched_duration_rank', 'legs1_main_carrier', 'companyID_loo_mean_legs1_arrivalAt_hour', 'corporateTariffCode', 'legs0_segments1_key_view_count_rank', 'total_duration_rank', 'leg1_view_diff_mean', 'legs0_segments0_key_view_count', 'total_fees', 'leg0_flight_view_count', 'companyID_loo_mean_cabin_class', 'totalPrice', 'taxes', 'legs1_duration', 'legs1_segments0_cabinClass', 'selected', 'ranker_id', 'companyID']\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "# è®€å– parquet\n",
    "train_filled = pl.read_parquet(\"data/Try only companyID/train_filled.parquet\")\n",
    "\n",
    "# è®€å–top50 features\n",
    "n_top = 80\n",
    "save_dir = \"model_output/selected_features_xgb/one_model/features_v1_with_company_ID/k-fold/\"\n",
    "df = pd.read_csv(\"model_output/all_features_xgb/v1_base_features/with_companyID_engineer/v1_model/model_importance/common_features_with_ranks.csv\")\n",
    "top50_features = df[\"feature\"].head(n_top).tolist()\n",
    "top50_features = [f for f in top50_features if f != \"companyID\"]\n",
    "\n",
    "# ä½ è¦ä¿ç•™çš„æ¬„ä½ (Top50 + target + group id)\n",
    "cols_to_keep = top50_features + [\"selected\", \"ranker_id\", \"companyID\"]\n",
    "\n",
    "# åªä¿ç•™é€™äº›æ¬„ä½\n",
    "train_filled = train_filled.select(cols_to_keep)\n",
    "\n",
    "\n",
    "print(f\"âœ… DataFrame ç¾åœ¨åªæœ‰ {len(train_filled.columns)} æ¬„ä½: {train_filled.columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2274fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å…±æ‰¾åˆ° 12 å€‹å­˜åœ¨çš„æ¬„ä½è¦è½‰ int: ['pricingInfo_isAccessTP', 'legs0_segments0_baggageAllowance_quantity', 'legs1_segments0_baggageAllowance_quantity', 'miniRules1_statusInfos', 'baggage_total', 'legs0_segments0_seatsAvailable', 'miniRules1_monetaryAmount', 'total_fees', 'price_minus_fee', 'taxes', 'totalPrice', 'legs1_segments0_seatsAvailable']\n"
     ]
    }
   ],
   "source": [
    "# è¦è½‰æˆ int çš„æ¬„ä½\n",
    "cols_to_int = [\n",
    "    \"pricingInfo_isAccessTP\",\n",
    "    \"legs0_segments0_baggageAllowance_quantity\",\n",
    "    \"legs1_segments0_baggageAllowance_quantity\",\n",
    "    \"miniRules1_statusInfos\",\n",
    "    \"baggage_total\",\n",
    "    \"legs0_segments0_seatsAvailable\",\n",
    "    \"miniRules1_monetaryAmount\",\n",
    "    \"total_fees\",\n",
    "    \"price_minus_fee\",\n",
    "    \"taxes\",\n",
    "    \"totalPrice\",\n",
    "    \"legs1_segments0_seatsAvailable\"\n",
    "]\n",
    "\n",
    "# å…ˆæª¢æŸ¥å“ªäº›æ¬„ä½å­˜åœ¨\n",
    "existing_cols = [c for c in cols_to_int if c in train_filled.columns]\n",
    "\n",
    "print(f\"âœ… å…±æ‰¾åˆ° {len(existing_cols)} å€‹å­˜åœ¨çš„æ¬„ä½è¦è½‰ int: {existing_cols}\")\n",
    "\n",
    "# åšè½‰å‹\n",
    "train_filled = train_filled.with_columns([\n",
    "    pl.col(c).fill_null(0).cast(pl.Int32).alias(c)\n",
    "    for c in existing_cols\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e60b2977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 81 features\n"
     ]
    }
   ],
   "source": [
    "exclude_cols = [\n",
    "    'Id', 'ranker_id', 'selected',\n",
    "    'profileId', 'requestDate',\n",
    "    'legs0_departureAt', 'legs0_arrivalAt', 'legs1_departureAt', 'legs1_arrivalAt',\n",
    "    'miniRules0_percentage', 'miniRules1_percentage',  # >90% missing\n",
    "    'frequentFlyer',  # Already processed\n",
    "    # Exclude constant columns\n",
    "    'pricingInfo_passengerCount'\n",
    "]\n",
    "\n",
    "feature_cols = [col for col in train_filled.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Using {len(feature_cols)} features\")\n",
    "\n",
    "X = train_filled.select(feature_cols)\n",
    "y = train_filled.select('selected')\n",
    "groups = train_filled.select('ranker_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f39e14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 81 features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "\n",
    "# ===== ä½ åŸæœ¬çš„ exclude èˆ‡ feature_cols =====\n",
    "exclude_cols = [\n",
    "    'Id', 'ranker_id', 'selected',\n",
    "    'profileId', 'requestDate',\n",
    "    'legs0_departureAt', 'legs0_arrivalAt', 'legs1_departureAt', 'legs1_arrivalAt',\n",
    "    'miniRules0_percentage', 'miniRules1_percentage',\n",
    "    'frequentFlyer',\n",
    "    'pricingInfo_passengerCount'\n",
    "]\n",
    "feature_cols = [c for c in train_filled.columns if c not in exclude_cols]\n",
    "print(f\"Using {len(feature_cols)} features\")\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "\n",
    "# ===== ä½ çš„æ—¢æœ‰è¨­å®š =====\n",
    "categorical_cols = [\n",
    "    'legs0_segments0_flightNumber',\n",
    "    'legs1_segments0_flightNumber',\n",
    "    'legs0_segments0_cabinClass',\n",
    "    'legs1_segments0_cabinClass',\n",
    "    'legs1_segments0_marketingCarrier_code',\n",
    "    'corporateTariffCode',\n",
    "    'is_major_carrier',\n",
    "    'isVip',\n",
    "    'has_baggage',\n",
    "    'has_access_tp',\n",
    "    'free_exchange',\n",
    "    'free_cancel',\n",
    "    'companyID',\n",
    "]\n",
    "categorical_cols = [c for c in categorical_cols if c in feature_cols]\n",
    "numeric_cols = [c for c in feature_cols if c not in categorical_cols]\n",
    "\n",
    "X = train_filled.select(feature_cols)\n",
    "y = train_filled.select('selected')\n",
    "groups = train_filled.select('ranker_id')\n",
    "\n",
    "# ===== split by ranker_id (åŒä½ åŸæœ¬æµç¨‹) =====\n",
    "unique_rankers = groups.select(\"ranker_id\").unique().to_series().to_list()\n",
    "np.random.seed(42); np.random.shuffle(unique_rankers)\n",
    "n_train = int(0.8 * len(unique_rankers))\n",
    "train_rankers = set(unique_rankers[:n_train])\n",
    "\n",
    "is_train = groups.select(pl.col(\"ranker_id\").is_in(list(train_rankers)).alias(\"is_train\"))\n",
    "X_with_mask = X.with_columns(is_train)\n",
    "y_with_mask = y.with_columns(is_train)\n",
    "groups_with_mask = groups.with_columns(is_train)\n",
    "\n",
    "X_train_df = X_with_mask.filter(pl.col(\"is_train\"))\n",
    "X_val_df   = X_with_mask.filter(~pl.col(\"is_train\"))\n",
    "y_train_df = y_with_mask.filter(pl.col(\"is_train\"))\n",
    "y_val_df   = y_with_mask.filter(~pl.col(\"is_train\"))\n",
    "groups_train_df = groups_with_mask.filter(pl.col(\"is_train\"))\n",
    "groups_val_df   = groups_with_mask.filter(~pl.col(\"is_train\"))\n",
    "\n",
    "# ===== é€™è£¡æ˜¯é—œéµä¿®æ­£ï¼šç”¨ StringCache + å…ˆè½‰ Utf8 å†è½‰ Categoricalï¼Œå†å–æ•´æ•¸ç¢¼ =====\n",
    "with pl.StringCache():\n",
    "    def encode_cats(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        if not categorical_cols:\n",
    "            return df\n",
    "        cat_exprs = []\n",
    "        for c in categorical_cols:\n",
    "            # 1) å…ˆè½‰æˆå­—ä¸² (Utf8)ï¼›2) å†è½‰ Categoricalï¼›3) å–å¯¦é«”ç¢¼ (u32)ï¼›4) è½‰æˆ Int32ï¼›5) ç©ºå€¼è£œ -1\n",
    "            cat_exprs.append(\n",
    "                pl.col(c)\n",
    "                  .cast(pl.Utf8)\n",
    "                  .cast(pl.Categorical)\n",
    "                  .to_physical()           # UInt32\n",
    "                  .cast(pl.Int32)          # Int32 æ‰èƒ½æœ‰ missing=-1\n",
    "                  .fill_null(-1)\n",
    "                  .alias(c)\n",
    "            )\n",
    "        return df.with_columns(cat_exprs)\n",
    "\n",
    "    X_train_df = encode_cats(X_train_df)\n",
    "    X_val_df   = encode_cats(X_val_df)\n",
    "\n",
    "# å…¶ä»–æ•¸å€¼æ¬„ä½è½‰ float32ï¼ˆçœè¨˜æ†¶é«”ï¼‰\n",
    "num_exprs = [pl.col(c).cast(pl.Float32).alias(c) for c in numeric_cols]\n",
    "X_train_df = X_train_df.with_columns(num_exprs)\n",
    "X_val_df   = X_val_df.with_columns(num_exprs)\n",
    "\n",
    "# ===== è½‰ NumPy =====\n",
    "X_train_np = X_train_df.drop(\"is_train\").to_numpy()\n",
    "X_val_np   = X_val_df.drop(\"is_train\").to_numpy()\n",
    "y_train_np = y_train_df.drop(\"is_train\").to_numpy().astype(np.float32).ravel()\n",
    "y_val_np   = y_val_df.drop(\"is_train\").to_numpy().astype(np.float32).ravel()\n",
    "\n",
    "# ===== group sizes =====\n",
    "group_sizes_train = (\n",
    "    groups_train_df.drop(\"is_train\")\n",
    "    .group_by(\"ranker_id\", maintain_order=True)\n",
    "    .agg(pl.len())['len'].to_numpy()\n",
    ")\n",
    "group_sizes_val = (\n",
    "    groups_val_df.drop(\"is_train\")\n",
    "    .group_by(\"ranker_id\", maintain_order=True)\n",
    "    .agg(pl.len())['len'].to_numpy()\n",
    ")\n",
    "\n",
    "# ===== å‘Šè¨´ XGBoost å“ªäº›æ˜¯é¡åˆ¥ã€å“ªäº›æ˜¯é€£çºŒ =====\n",
    "feature_types = ['c' if c in categorical_cols else 'q' for c in feature_cols]\n",
    "\n",
    "# ===== å»º DMatrixï¼ˆæ³¨æ„ missing=-1 å°æ‡‰æˆ‘å€‘ä¸Šé¢å¡«çš„ -1ï¼‰=====\n",
    "dtrain = xgb.DMatrix(\n",
    "    X_train_np,\n",
    "    label=y_train_np,\n",
    "    feature_names=feature_cols,\n",
    "    feature_types=feature_types,\n",
    "    enable_categorical=True,\n",
    "    missing=-1\n",
    ")\n",
    "dtrain.set_group(group_sizes_train)\n",
    "\n",
    "dval = xgb.DMatrix(\n",
    "    X_val_np,\n",
    "    label=y_val_np,\n",
    "    feature_names=feature_cols,\n",
    "    feature_types=feature_types,\n",
    "    enable_categorical=True,\n",
    "    missing=-1\n",
    ")\n",
    "dval.set_group(group_sizes_val)\n",
    "\n",
    "del X_train_np, y_train_np, group_sizes_train\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5ea68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# ç¢ºèªé€™äº›ç‰©ä»¶éƒ½æ˜¯Polars DataFrame\n",
    "# X, y, groups\n",
    "# éƒ½æ˜¯ shape [n_rows, n_cols]\n",
    "\n",
    "# å…ˆæŠŠ ranker_idè½‰list\n",
    "unique_rankers = groups.select(\"ranker_id\").unique().to_series().to_list()\n",
    "\n",
    "# æ‰“äº‚\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(unique_rankers)\n",
    "\n",
    "# åˆ‡8:2\n",
    "n_train = int(0.8 * len(unique_rankers))\n",
    "train_rankers = set(unique_rankers[:n_train])\n",
    "val_rankers = set(unique_rankers[n_train:])\n",
    "\n",
    "# ç”¨ Polars éæ¿¾ train/val\n",
    "is_train = groups.select(pl.col(\"ranker_id\").is_in(list(train_rankers)).alias(\"is_train\"))\n",
    "\n",
    "# å…ˆ concat mask\n",
    "X_with_mask = X.with_columns(is_train)\n",
    "y_with_mask = y.with_columns(is_train)\n",
    "groups_with_mask = groups.with_columns(is_train)\n",
    "\n",
    "# åˆ†å‰² DataFrame\n",
    "X_train_df = X_with_mask.filter(pl.col(\"is_train\"))\n",
    "X_val_df = X_with_mask.filter(~pl.col(\"is_train\"))\n",
    "y_train_df = y_with_mask.filter(pl.col(\"is_train\"))\n",
    "y_val_df = y_with_mask.filter(~pl.col(\"is_train\"))\n",
    "groups_train_df = groups_with_mask.filter(pl.col(\"is_train\"))\n",
    "groups_val_df = groups_with_mask.filter(~pl.col(\"is_train\"))\n",
    "\n",
    "# å†è½‰ numpy (åˆ†æ‰¹)\n",
    "X_train_np = X_train_df.drop(\"is_train\").to_numpy()\n",
    "X_val_np = X_val_df.drop(\"is_train\").to_numpy()\n",
    "y_train_np = y_train_df.drop(\"is_train\").to_numpy().flatten()\n",
    "y_val_np = y_val_df.drop(\"is_train\").to_numpy().flatten()\n",
    "groups_train_np = groups_train_df.drop(\"is_train\").to_numpy().flatten()\n",
    "groups_val_np = groups_val_df.drop(\"is_train\").to_numpy().flatten()\n",
    "# æœ€å¾Œè¨ˆç®— group sizes\n",
    "group_sizes_train = (\n",
    "    groups_train_df.drop(\"is_train\")\n",
    "    .group_by(\"ranker_id\", maintain_order=True)\n",
    "    .agg(pl.len())['len']\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "group_sizes_val = (\n",
    "    groups_val_df.drop(\"is_train\")\n",
    "    .group_by(\"ranker_id\", maintain_order=True)\n",
    "    .agg(pl.len())['len']\n",
    "    .to_numpy()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c124aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import os\n",
    "\n",
    "# ğŸ“Œ åƒæ•¸\n",
    "lgb_params = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\": \"ndcg\",\n",
    "    \"ndcg_eval_at\": [3],\n",
    "    \"max_depth\": 10,\n",
    "    \"min_data_in_leaf\": 10,      # â‰ˆ min_child_weight\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"lambda_l2\": 10.0,           # XGB çš„ lambda\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"seed\": 42,\n",
    "    \"num_threads\": -1,\n",
    "    \"verbosity\": 20\n",
    "}\n",
    "\n",
    "# ğŸ“Œ å»ºç«‹ Dataset\n",
    "dtrain = lgb.Dataset(\n",
    "    X_train_np,\n",
    "    label=y_train_np,\n",
    "    feature_name = feature_cols,\n",
    "    group=group_sizes_train\n",
    ")\n",
    "dval = lgb.Dataset(\n",
    "    X_val_np,\n",
    "    label=y_val_np,\n",
    "    feature_name = feature_cols,\n",
    "    group=group_sizes_val,\n",
    "    reference=dtrain\n",
    ")\n",
    "\n",
    "# ğŸ“Œ è¨“ç·´\n",
    "lgb_model = lgb.train(\n",
    "    lgb_params,\n",
    "    dtrain,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=[dtrain, dval],\n",
    "    valid_names=[\"train\", \"val\"],\n",
    ")\n",
    "\n",
    "# ğŸ“Œ å„²å­˜æ¨¡å‹\n",
    "model_path = os.path.join(save_dir, f\"top{n_top}\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "model_file = os.path.join(model_path, \"lgb_ranker_model.txt\")\n",
    "\n",
    "lgb_model.save_model(model_file)\n",
    "print(f\"âœ… å·²å„²å­˜æ¨¡å‹åˆ° 'lgb_ranker_model.txt' in {model_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from scripts.hitrate import compute_hitrate_at_3\n",
    "# é æ¸¬\n",
    "val_preds = lgb_model.predict(X_val_np)\n",
    "\n",
    "# è¨ˆç®— HitRate\n",
    "compute_hitrate_at_3(groups_val_np, y_val_np, val_preds)\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ä¸‰ç¨®é‡è¦æ€§å°æ‡‰ LightGBM\n",
    "importance_types = {\n",
    "    \"weight\": \"split\",    # ç­‰æ–¼ XGBoost çš„ \"weight\"\n",
    "    \"gain\": \"gain\",       # ç­‰æ–¼ XGBoost çš„ \"gain\"\n",
    "    # LightGBM æ²’æœ‰ \"cover\"ï¼Œåªæœ‰ split/gain\n",
    "}\n",
    "\n",
    "importance_all = {}\n",
    "\n",
    "for k, lgb_type in importance_types.items():\n",
    "    imp_raw = lgb_model.feature_importance(importance_type=lgb_type)\n",
    "    imp_named = dict(zip(lgb_model.feature_name(), imp_raw))\n",
    "    sorted_imp = sorted(imp_named.items(), key=lambda x: x[1], reverse=True)\n",
    "    importance_all[k] = sorted_imp\n",
    "\n",
    "# weightæ¦œ\n",
    "df_weight = pd.DataFrame(importance_all[\"weight\"], columns=[\"feature\", \"weight_rank\"])\n",
    "df_weight[\"weight_rank_pos\"] = df_weight.index\n",
    "\n",
    "# gainæ¦œ\n",
    "df_gain = pd.DataFrame(importance_all[\"gain\"], columns=[\"feature\", \"gain_rank\"])\n",
    "df_gain[\"gain_rank_pos\"] = df_gain.index\n",
    "\n",
    "# åˆä½µ\n",
    "df_merged = (\n",
    "    df_weight\n",
    "    .merge(df_gain, on=\"feature\", how=\"outer\")\n",
    ")\n",
    "\n",
    "# è£œç©ºå€¼\n",
    "df_merged[\"weight_rank_pos\"] = df_merged[\"weight_rank_pos\"].fillna(9999)\n",
    "df_merged[\"gain_rank_pos\"] = df_merged[\"gain_rank_pos\"].fillna(9999)\n",
    "\n",
    "# æœ€å°rank\n",
    "df_merged[\"min_rank\"] = df_merged[[\"weight_rank_pos\", \"gain_rank_pos\"]].min(axis=1)\n",
    "\n",
    "# æ’åº\n",
    "df_merged_sorted = df_merged.sort_values(\"min_rank\")\n",
    "\n",
    "# Top50\n",
    "top50 = df_merged_sorted.head(50)\n",
    "\n",
    "# é¡¯ç¤º\n",
    "print(top50[[\"feature\", \"weight_rank\", \"gain_rank\"]])\n",
    "\n",
    "# è¼¸å‡ºCSV\n",
    "csv_path = os.path.join(save_dir, f\"top{n_top}/lgb_feature_importance.csv\")\n",
    "df_merged_sorted.to_csv(csv_path, index=False)\n",
    "print(f\"âœ… å·²è¼¸å‡º feature_importance.csv åˆ° {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del train_filled, X_train_df,X_val_df,X_with_mask,  y_train_df,y_val_df,y_with_mask, groups, groups_train_df, groups_val_df, is_train, groups_with_mask\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedc82e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import gc\n",
    "\n",
    "# DMatrix å»ºç«‹ (ä¸å†ç”¨ X.columns)\n",
    "dtrain = xgb.DMatrix(\n",
    "    X_train_np,\n",
    "    label=y_train_np,\n",
    "    feature_names = feature_cols,\n",
    ")\n",
    "dtrain.set_group(group_sizes_train)\n",
    "del X_train_np, y_train_np, group_sizes_train\n",
    "gc.collect()\n",
    "dval = xgb.DMatrix(\n",
    "    X_val_np,\n",
    "    label=y_val_np,\n",
    "    feature_names = feature_cols,\n",
    ")\n",
    "dval.set_group(group_sizes_val)\n",
    "del X_val_np\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d45c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def analyze_monotonic_trends(X: pd.DataFrame, y: pd.Series, bins=20, show_plot=True):\n",
    "    mono_result = {}\n",
    "\n",
    "    for col in X.columns:\n",
    "        x = pd.Series(X[col])\n",
    "\n",
    "        # Skip if too few non-null values\n",
    "        if x.dropna().shape[0] < 1000:\n",
    "            continue\n",
    "\n",
    "        if x.nunique() <= 2:\n",
    "            # Binary feature\n",
    "            grp = pd.DataFrame({col: x, \"target\": y}).groupby(col).mean()\n",
    "            if grp.shape[0] < 2:\n",
    "                direction = 0\n",
    "            else:\n",
    "                diff = grp[\"target\"].diff().iloc[-1]\n",
    "                direction = int(diff > 0) - int(diff < 0)\n",
    "            mono_result[col] = direction\n",
    "\n",
    "            if show_plot:\n",
    "                grp[\"target\"].plot(kind=\"bar\", title=f\"{col} (binary) => direction={direction}\")\n",
    "                plt.ylabel(\"Mean Target\")\n",
    "                plt.grid()\n",
    "                plt.show()\n",
    "        else:\n",
    "            # Continuous feature\n",
    "            df = pd.DataFrame({col: x, \"target\": y}).dropna()\n",
    "            try:\n",
    "                df[\"bin\"] = pd.qcut(df[col], bins, duplicates=\"drop\")\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            bin_stats = df.groupby(\"bin\")[\"target\"].agg([\"count\", \"sum\", \"mean\"])\n",
    "            if bin_stats[\"count\"].min() < 200:\n",
    "                continue\n",
    "\n",
    "            bin_avg = bin_stats[\"mean\"]\n",
    "            bin_mid = df.groupby(\"bin\")[col].mean()\n",
    "            coef, _ = spearmanr(bin_mid, bin_avg)\n",
    "            coef = coef if not pd.isna(coef) else 0\n",
    "\n",
    "            # Set stricter threshold for monotonicity\n",
    "            direction = int(coef > 0.85) - int(coef < -0.85)\n",
    "            mono_result[col] = direction\n",
    "\n",
    "            if show_plot:\n",
    "                plt.plot(bin_mid, bin_avg, marker=\"o\")\n",
    "                plt.title(f\"{col} (continuous) => Spearman={coef:.3f}, direction={direction}\")\n",
    "                plt.xlabel(col)\n",
    "                plt.ylabel(\"Mean Target\")\n",
    "                plt.grid()\n",
    "                plt.show()\n",
    "\n",
    "    return mono_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef464430",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = y[y.columns[0]]  # æˆ– y.squeeze() ä½†é€™è£¡ç”¨æ¬„ä½åæœ€\n",
    "\n",
    "monotone_constraints = analyze_monotonic_trends(X, Y, bins=20, show_plot=True)\n",
    "\n",
    "# çµæœæ ¼å¼ï¼š{\"feature_name1\": +1, \"feature_name2\": -1, \"feature_name3\": 0, ...}\n",
    "# å¯ä¾ç…§é †åºè½‰æ›æˆ list å‚³çµ¦ XGBoost / LightGBM:\n",
    "constraint_list = [monotone_constraints[col] for col in X.columns]\n",
    "print(\"âœ… å»ºè­° monotone_constraints:\", constraint_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b43402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-ndcg@3:0.36567\tval-ndcg@3:0.35084\n",
      "[20]\ttrain-ndcg@3:0.61783\tval-ndcg@3:0.47784\n",
      "[40]\ttrain-ndcg@3:0.67960\tval-ndcg@3:0.49686\n",
      "[60]\ttrain-ndcg@3:0.72167\tval-ndcg@3:0.50800\n",
      "[80]\ttrain-ndcg@3:0.75901\tval-ndcg@3:0.52213\n",
      "[100]\ttrain-ndcg@3:0.78568\tval-ndcg@3:0.52928\n",
      "[120]\ttrain-ndcg@3:0.80895\tval-ndcg@3:0.53582\n",
      "[140]\ttrain-ndcg@3:0.83246\tval-ndcg@3:0.54403\n",
      "[160]\ttrain-ndcg@3:0.84893\tval-ndcg@3:0.54763\n",
      "[180]\ttrain-ndcg@3:0.86703\tval-ndcg@3:0.55235\n",
      "[200]\ttrain-ndcg@3:0.88109\tval-ndcg@3:0.55554\n",
      "[220]\ttrain-ndcg@3:0.89160\tval-ndcg@3:0.55879\n",
      "[240]\ttrain-ndcg@3:0.89875\tval-ndcg@3:0.56175\n",
      "[260]\ttrain-ndcg@3:0.90370\tval-ndcg@3:0.56307\n",
      "[280]\ttrain-ndcg@3:0.90853\tval-ndcg@3:0.56461\n",
      "[300]\ttrain-ndcg@3:0.91093\tval-ndcg@3:0.56473\n",
      "[320]\ttrain-ndcg@3:0.91346\tval-ndcg@3:0.56564\n",
      "[340]\ttrain-ndcg@3:0.91452\tval-ndcg@3:0.56556\n",
      "[360]\ttrain-ndcg@3:0.91680\tval-ndcg@3:0.56545\n",
      "[380]\ttrain-ndcg@3:0.91870\tval-ndcg@3:0.56594\n",
      "[400]\ttrain-ndcg@3:0.91918\tval-ndcg@3:0.56641\n",
      "[420]\ttrain-ndcg@3:0.92014\tval-ndcg@3:0.56665\n",
      "[440]\ttrain-ndcg@3:0.92050\tval-ndcg@3:0.56712\n",
      "[460]\ttrain-ndcg@3:0.92074\tval-ndcg@3:0.56709\n",
      "[480]\ttrain-ndcg@3:0.92100\tval-ndcg@3:0.56706\n",
      "[490]\ttrain-ndcg@3:0.92170\tval-ndcg@3:0.56709\n",
      "âœ… å·²å„²å­˜æœ€ä½³æ¨¡å‹ï¼šmodel_output/selected_features_xgb/one_model/features_v1_with_company_ID/k-fold/top80_v2/xgb_ranker_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import joblib, os, json, numpy as np\n",
    "from scripts.hitrate import compute_hitrate_at_3\n",
    "n_top = \"80_v2\"\n",
    "model_dir = os.path.join(save_dir, f\"top{n_top}\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "params = {\n",
    "    'objective': 'rank:pairwise',\n",
    "    'eval_metric': 'ndcg@3',\n",
    "    \"learning_rate\": 0.022641389657079056,\n",
    "    \"max_depth\": 14,\n",
    "    \"min_child_weight\": 2,\n",
    "    \"subsample\": 0.8842234913702768,\n",
    "    \"colsample_bytree\": 0.45840689146263086,\n",
    "    \"gamma\": 3.3084297630544888,\n",
    "    \"lambda\": 6.952586917313028,\n",
    "    \"alpha\": 0.6395254133055179,\n",
    "    'seed': 42,\n",
    "    'n_jobs': -1,\n",
    "    # 'device': 'cuda'\n",
    "}\n",
    "with open(os.path.join(model_dir, \"xgb_params.json\"), \"w\") as f:\n",
    "    json.dump(params, f, indent=2)\n",
    "\n",
    "# # æ¬Šé‡\n",
    "# def make_sample_weights(y, pos_weight=10.0):\n",
    "#     return np.where(y == 1, pos_weight, 1.0)\n",
    "\n",
    "# w_train = make_sample_weights(y_train_np)\n",
    "# w_val = make_sample_weights(y_val_np)\n",
    "\n",
    "# dtrain = xgb.DMatrix(X_train_np, label=y_train_np, weight=w_train)\n",
    "# dtrain.set_group(group_sizes_train)\n",
    "\n",
    "# dval = xgb.DMatrix(X_val_np, label=y_val_np, weight=w_val)\n",
    "# dval.set_group(group_sizes_val)\n",
    "\n",
    "evals = [(dtrain, \"train\"), (dval, \"val\")]\n",
    "\n",
    "# è¨“ç·´ + callback æ¨¡æ“¬ early stopping + hitrate\n",
    "xgb_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=20,\n",
    ")\n",
    "\n",
    "# å„²å­˜æœ€ä½³æ¨¡å‹ï¼ˆå¯è½‰æˆ SHAPï¼‰\n",
    "model_path = os.path.join(model_dir, \"xgb_ranker_model.pkl\")\n",
    "joblib.dump(xgb_model, model_path)\n",
    "print(f\"âœ… å·²å„²å­˜æœ€ä½³æ¨¡å‹ï¼š{model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d85b3d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… HitRate@3 (groups size in [10, inf]): 0.6017\n",
      "\n",
      "âœ… å·²å„²å­˜æ‰€æœ‰ Hitrate çµæœè‡³ model_output/selected_features_xgb/one_model/features_v1_with_company_ID/k-fold/top80_v2/hitrate_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from scripts.hitrate import compute_hitrate_at_3\n",
    "\n",
    "# é æ¸¬\n",
    "val_preds = xgb_model.predict(dval)\n",
    "\n",
    "\n",
    "# è¨ˆç®— HitRate\n",
    "hitrate = compute_hitrate_at_3(groups_val_np, y_val_np, val_preds)\n",
    "hitrate_records = []\n",
    "hitrate_records.append({\"split_label\": \"overall\", \"hitrate\": hitrate})\n",
    "\n",
    "\n",
    "hitrate_df = pl.DataFrame(hitrate_records)\n",
    "csv_path = os.path.join(model_dir, \"hitrate_summary.csv\")\n",
    "hitrate_df.write_csv(csv_path)\n",
    "print(f\"\\nâœ… å·²å„²å­˜æ‰€æœ‰ Hitrate çµæœè‡³ {csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21c37925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      feature  weight_rank    gain_rank  \\\n",
      "67                           price_percentile      12908.0    14.226832   \n",
      "76                     total_is_min_transfers         93.0  4835.294922   \n",
      "32                     legs0_is_min_transfers         72.0   903.296143   \n",
      "78                   total_num_transfers_rank         97.0   851.695374   \n",
      "62                         price_per_duration      12224.0    10.403891   \n",
      "68                     pricingInfo_isAccessTP        465.0    74.151596   \n",
      "38               legs0_segments0_flightNumber      11440.0     8.484340   \n",
      "35                   legs0_num_transfers_rank         57.0   564.806030   \n",
      "59                   price_from_median_zscore      11418.0    14.836967   \n",
      "31                     legs0_departureAt_hour      10206.0    10.336510   \n",
      "22                              has_access_tp        215.0    75.881935   \n",
      "48                     legs1_is_min_transfers         53.0   328.760284   \n",
      "19                              free_exchange        248.0   141.783539   \n",
      "64                              price_per_fee      10201.0    13.130466   \n",
      "37                 legs0_segments0_cabinClass        108.0    93.031212   \n",
      "63                    price_per_duration_rank      10131.0    10.120821   \n",
      "51                   legs1_num_transfers_rank         47.0    90.116127   \n",
      "16                      days_before_departure       9947.0     8.514164   \n",
      "66                              price_per_tax       9770.0     8.341358   \n",
      "33                   legs0_max_duration_cabin        124.0    27.859236   \n",
      "72                            totalPrice_rank       9718.0    10.658759   \n",
      "20                                 group_size       9626.0    10.040174   \n",
      "42                  legs0_weighted_mean_cabin        179.0    48.818985   \n",
      "41        legs0_segments1_key_view_count_rank        631.0    36.001957   \n",
      "24                                   has_fees        193.0    19.703661   \n",
      "14                companyID_total_occurrences       9605.0     9.376928   \n",
      "26                           is_major_carrier        410.0    31.020334   \n",
      "34                           legs0_mean_cabin        234.0    31.670216   \n",
      "30                       legs0_arrivalAt_hour       9596.0     9.783179   \n",
      "36  legs0_segments0_baggageAllowance_quantity       1199.0    29.702063   \n",
      "61                       price_minus_fee_rank       9391.0     9.642180   \n",
      "79                  total_weighted_mean_cabin        358.0    20.708042   \n",
      "8           companyID_loo_mean_legs0_duration       9305.0    11.341264   \n",
      "65                         price_per_fee_rank       9280.0     8.568635   \n",
      "80                    unmatched_duration_rank       4740.0    16.500721   \n",
      "11                   companyID_loo_mean_taxes       9265.0    10.045272   \n",
      "3                  both_legs_carrier_all_same        329.0    26.304342   \n",
      "77                        total_num_transfers        346.0    21.619938   \n",
      "28                        leg0_view_diff_mean       9122.0    10.434266   \n",
      "25                                      isVip        578.0    18.336134   \n",
      "23                                has_baggage        478.0    21.044935   \n",
      "7   companyID_loo_mean_legs0_departureAt_hour       9115.0    10.778905   \n",
      "58                  miniRules1_monetaryAmount       1601.0    16.895660   \n",
      "4                                   companyID       8905.0     9.250352   \n",
      "15                        corporateTariffCode       4557.0    12.322786   \n",
      "12              companyID_loo_mean_totalPrice       8367.0    12.182522   \n",
      "18                                free_cancel        277.0    19.168634   \n",
      "6     companyID_loo_mean_legs0_arrivalAt_hour       8222.0    10.010968   \n",
      "74                        total_duration_rank       4872.0    18.403643   \n",
      "56                                  log_price       7742.0     7.938188   \n",
      "\n",
      "       cover_rank  \n",
      "67    5373.247559  \n",
      "76  353277.187500  \n",
      "32  112336.031250  \n",
      "78  163401.593750  \n",
      "62    5227.322266  \n",
      "68  126913.765625  \n",
      "38    4920.297363  \n",
      "35  122451.132812  \n",
      "59    4416.595703  \n",
      "31    5673.269531  \n",
      "22  117730.875000  \n",
      "48   80431.578125  \n",
      "19   31338.769531  \n",
      "64    5013.381348  \n",
      "37   30142.058594  \n",
      "63    4080.575684  \n",
      "51   66650.429688  \n",
      "16    2568.514648  \n",
      "66    2155.565674  \n",
      "33   35354.414062  \n",
      "72    4551.776855  \n",
      "20    2692.961182  \n",
      "42   16356.190430  \n",
      "41    6223.741211  \n",
      "24   18540.998047  \n",
      "14    4001.377930  \n",
      "26   18087.361328  \n",
      "34   13412.395508  \n",
      "30    4743.681152  \n",
      "36   17396.169922  \n",
      "61    2186.390869  \n",
      "79   16411.154297  \n",
      "8     2809.789307  \n",
      "65    2387.122314  \n",
      "80   15091.375977  \n",
      "11    2801.170410  \n",
      "3    14101.159180  \n",
      "77    7740.241211  \n",
      "28    3677.022461  \n",
      "25   14244.938477  \n",
      "23   12152.751953  \n",
      "7     2729.003418  \n",
      "58   14209.253906  \n",
      "4     2886.639893  \n",
      "15   13451.534180  \n",
      "12    3554.067627  \n",
      "18    6842.412109  \n",
      "6     2885.258545  \n",
      "74    6710.425781  \n",
      "56    2432.491455  \n",
      "âœ… å·²è¼¸å‡ºmodel_output/selected_features_xgb/one_model/features_v1_with_company_ID/k-fold/top80_v2/feature_importance.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# å–ä¸‰ç¨®é‡è¦æ€§\n",
    "importance_types = [\"weight\", \"gain\", \"cover\"]\n",
    "importance_all = {}\n",
    "\n",
    "for imp_type in importance_types:\n",
    "    imp_raw = xgb_model.get_score(importance_type=imp_type)\n",
    "    imp_named = {}\n",
    "    for k, v in imp_raw.items():\n",
    "        imp_named[k] = v\n",
    "    # æ’åº\n",
    "    sorted_imp = sorted(imp_named.items(), key=lambda x: x[1], reverse=True)\n",
    "    importance_all[imp_type] = sorted_imp\n",
    "\n",
    "# æŠŠä¸‰å€‹æ¦œå–®æ”¾æˆDataFrameæ–¹ä¾¿æ¯”å°\n",
    "df_weight = pd.DataFrame(importance_all[\"weight\"], columns=[\"feature\", \"weight_rank\"])\n",
    "df_weight[\"weight_rank_pos\"] = df_weight.index\n",
    "\n",
    "df_gain = pd.DataFrame(importance_all[\"gain\"], columns=[\"feature\", \"gain_rank\"])\n",
    "df_gain[\"gain_rank_pos\"] = df_gain.index\n",
    "\n",
    "df_cover = pd.DataFrame(importance_all[\"cover\"], columns=[\"feature\", \"cover_rank\"])\n",
    "df_cover[\"cover_rank_pos\"] = df_cover.index\n",
    "\n",
    "# åˆä½µ\n",
    "df_merged = (\n",
    "    df_weight\n",
    "    .merge(df_gain, on=\"feature\", how=\"outer\")\n",
    "    .merge(df_cover, on=\"feature\", how=\"outer\")\n",
    ")\n",
    "\n",
    "# æŠŠä¸å­˜åœ¨çš„rankè£œå¤§æ•¸å­—\n",
    "df_merged[\"weight_rank_pos\"] = df_merged[\"weight_rank_pos\"].fillna(9999)\n",
    "df_merged[\"gain_rank_pos\"] = df_merged[\"gain_rank_pos\"].fillna(9999)\n",
    "df_merged[\"cover_rank_pos\"] = df_merged[\"cover_rank_pos\"].fillna(9999)\n",
    "\n",
    "# è¨ˆç®—ã€Œä¸‰å€‹æ¦œå–®ä¸­æœ€æ—©å‡ºç¾çš„ä½ç½®ã€\n",
    "df_merged[\"min_rank\"] = df_merged[[\"weight_rank_pos\", \"gain_rank_pos\", \"cover_rank_pos\"]].min(axis=1)\n",
    "\n",
    "# æ’åº\n",
    "df_merged_sorted = df_merged.sort_values(\"min_rank\")\n",
    "\n",
    "# å–å‰50\n",
    "top50 = df_merged_sorted.head(50)\n",
    "\n",
    "# é¡¯ç¤º\n",
    "print(top50[[\"feature\", \"weight_rank\", \"gain_rank\", \"cover_rank\"]])\n",
    "# å¦‚æœæƒ³è¼¸å‡ºCSV\n",
    "csv_path = os.path.join(model_dir, \"feature_importance.csv\")\n",
    "\n",
    "df_merged_sorted.to_csv(csv_path, index=False)\n",
    "print(f\"âœ… å·²è¼¸å‡º{csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28467953",
   "metadata": {},
   "source": [
    "# Shap åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4bb5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "# åƒæ•¸\n",
    "\n",
    "import xgboost as xgb\n",
    "model_path = \"model_output/selected_features_xgb/one_model/features_v1_with_company_ID/model_par_4/top100/xgb_ranker_model.bin\"\n",
    "\n",
    "# è®€å–æ¨¡å‹\n",
    "xgb_model = xgb.Booster(model_file=model_path)\n",
    "# éš¨æ©ŸæŠ½æ¨£ indexï¼ˆä½¿ç”¨ polars çš„ row samplingï¼‰\n",
    "sample_idx = np.random.default_rng(42).choice(len(X), size=50000, replace=False)\n",
    "X_sample_pl = X[sample_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8053a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import shap\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # ==== Config ====\n",
    "# model_bin_path = \"model_output/selected_features_xgb/one_model/features_v1_with_company_ID/model_par_4/top100/xgb_ranker_model.bin\"\n",
    "# model_pkl_path = model_bin_path.replace(\".bin\", \".pkl\")\n",
    "# shap_dir = os.path.dirname(model_pkl_path)\n",
    "\n",
    "# # ==== Load Booster & Convert to Regressor ====\n",
    "# booster = xgb.Booster()\n",
    "# booster.load_model(model_bin_path)\n",
    "\n",
    "# xgb_reg = xgb.XGBRegressor()\n",
    "# xgb_reg._Booster = booster\n",
    "# xgb_reg.n_features_in_ = booster.num_features()\n",
    "\n",
    "# # å„²å­˜ç‚º .pkl\n",
    "# joblib.dump(xgb_reg, model_pkl_path)\n",
    "# print(f\"âœ… Booster å·²å„²å­˜ç‚º: {model_pkl_path}\")\n",
    "\n",
    "# ==== Load X_sample and Compute SHAP ====\n",
    "# å‡è¨­ä½ å·²ç¶“æœ‰ X å¯ä»¥ç”¨ä¾†å–æ¨£\n",
    "X_sample = X.sample(n=25000, random_state=42)\n",
    "\n",
    "explainer = shap.Explainer(xgb_model, X_sample)\n",
    "shap_values = explainer(X_sample)\n",
    "\n",
    "# å„²å­˜ SHAP å€¼\n",
    "np.save(os.path.join(xgb_model, \"shap_values.npy\"), shap_values.values)\n",
    "X_sample.to_parquet(os.path.join(xgb_model, \"shap_input.parquet\"))\n",
    "\n",
    "# SHAP summary plot\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_sample)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(os.path.join(shap_dir, \"shap_summary.png\"))\n",
    "plt.close()\n",
    "\n",
    "# SHAP bar plot\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_sample, plot_type=\"bar\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(os.path.join(shap_dir, \"shap_bar.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Top 20 features importance to CSV\n",
    "mean_abs_shap = np.abs(shap_values.values).mean(axis=0)\n",
    "top_features = pd.Series(mean_abs_shap, index=X_sample.columns).sort_values(ascending=False)\n",
    "top_features[:20].to_csv(os.path.join(shap_dir, \"shap_top20.csv\"))\n",
    "\n",
    "print(\"âœ… SHAP å€¼èˆ‡åœ–è¡¨å·²å„²å­˜å®Œç•¢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # === åƒæ•¸ ===\n",
    "# bin_model_path = \"model_output/selected_features_xgb/one_model/features_v1_with_company_ID/model_par_4/top100/xgb_ranker_model.bin\"\n",
    "# model_pkl_path = \"model_output/selected_features_xgb/one_model/features_v1_with_company_ID/model_par_4/top100/xgb_ranker_model.pkl\"\n",
    "# model_dir = os.path.dirname(model_pkl_path)\n",
    "# os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# # âœ… è¼‰å…¥ Booster\n",
    "# booster = xgb.Booster()\n",
    "# booster.load_model(bin_model_path)\n",
    "\n",
    "# # âœ… å»ºç«‹ XGBRegressor wrapper\n",
    "# xgb_reg = xgb.XGBRegressor()\n",
    "# xgb_reg._Booster = booster\n",
    "\n",
    "# # âœ… æ‰‹å‹•è¨­å®šå¿…è¦å±¬æ€§\n",
    "# xgb_reg._features_count = booster.num_features()\n",
    "\n",
    "# # å‡è¨­æ˜¯äºŒåˆ†é¡ï¼ˆæ­¤æ­¥é©Ÿå¯èƒ½è¦– SHAP æˆ– sklearn éœ€æ±‚ï¼‰\n",
    "# class DummyLabelEncoder:\n",
    "#     def transform(self, x): return x\n",
    "#     def inverse_transform(self, x): return x\n",
    "# xgb_reg._le = DummyLabelEncoder()\n",
    "\n",
    "# # âœ… å„²å­˜ .pkl\n",
    "# joblib.dump(xgb_reg, model_pkl_path)\n",
    "# print(f\"âœ… Booster å·²è½‰æ›ä¸¦å„²å­˜ç‚º: {model_pkl_path}\")\n",
    "\n",
    "# âœ… æº–å‚™ SHAP è¼¸å…¥\n",
    "sample_idx = np.random.default_rng(42).choice(len(X), size=5000, replace=False)\n",
    "X_sample_pl = X[sample_idx]\n",
    "X_sample = X_sample_pl.to_pandas()\n",
    "explainer = shap.Explainer(xgb_model, X_sample)\n",
    "shap_values = explainer(X_sample)\n",
    "\n",
    "# âœ… å„²å­˜ SHAP çµæœ\n",
    "np.save(os.path.join(model_dir, \"shap_values.npy\"), shap_values.values)\n",
    "X_sample.to_parquet(os.path.join(model_dir, \"shap_input.parquet\"))\n",
    "\n",
    "# âœ… å‰ 20 ç‰¹å¾µé‡è¦æ€§ CSV\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": X_sample.columns,\n",
    "    \"mean_abs_shap\": np.abs(shap_values.values).mean(axis=0)\n",
    "}).sort_values(\"mean_abs_shap\", ascending=False)\n",
    "\n",
    "importance_df.head(20).to_csv(os.path.join(model_dir, \"shap_top20.csv\"), index=False)\n",
    "\n",
    "# âœ… summary plot\n",
    "shap.summary_plot(shap_values, X_sample, show=False)\n",
    "plt.savefig(os.path.join(model_dir, \"shap_summary.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# âœ… bar plot\n",
    "shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
    "plt.savefig(os.path.join(model_dir, \"shap_bar.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"âœ… SHAP åˆ†æèˆ‡åœ–å½¢å„²å­˜å®Œç•¢\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f0389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# åªè½‰ sample çš„ subset æˆ pandasï¼Œé€Ÿåº¦å¿«ã€è¨˜æ†¶é«”å°\n",
    "X_sample = X_sample_pl.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93af329",
   "metadata": {},
   "outputs": [],
   "source": [
    "dX_sample = xgb.DMatrix(X_sample, feature_names=X_sample.columns.tolist())\n",
    "explainer = shap.TreeExplainer(xgb_model)  # æ˜ç¢ºæŒ‡å®š TreeExplainer\n",
    "shap_vals = explainer.shap_values(dX_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f90e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(xgb_model, X_sample)\n",
    "\n",
    "# ä¸€æ¬¡æ€§è™•ç†æ•´å€‹ sample\n",
    "shap_vals = explainer.shap_values(X_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f1d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SHAP è§£é‡‹å™¨èˆ‡ SHAP å€¼è¨ˆç®—ï¼ˆæ”¯æ´é€²åº¦æ¢ï¼‰\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer(X_sample, check_additivity=False)  # é€™æ”¯æ´é€²åº¦æ¢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 3. å„²å­˜ SHAP æ’åè‡³ CSV\n",
    "shap_importance = np.abs(shap_vals).mean(axis=0)\n",
    "shap_importance_df = pd.DataFrame({\n",
    "    \"feature\": X_sample.columns,\n",
    "    \"mean_abs_shap\": shap_importance\n",
    "}).sort_values(\"mean_abs_shap\", ascending=False)\n",
    "shap_importance_path = os.path.join(\"model_output/selected_features_xgb/one_model/features_v1_with_company_ID/model_par_4/top100\", \"shap_feature_importance.csv\")\n",
    "shap_importance_df.to_csv(shap_importance_path, index=False)\n",
    "print(f\"âœ… å·²å„²å­˜ shap æ’åè‡³ {shap_importance_path}\")\n",
    "\n",
    "# 4. summary plot\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_vals, X_sample)\n",
    "# plt.savefig(os.path.join(model_dir, \"shap_summary_plot.png\"), bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 5. bar plot\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_vals, X_sample, plot_type=\"bar\")\n",
    "# plt.savefig(os.path.join(model_dir, \"shap_bar_plot.png\"), bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"âœ… SHAP summary èˆ‡ bar åœ–å·²å„²å­˜\")\n",
    "\n",
    "# # 6. dependence plots for Top-20 features\n",
    "# top_features = shap_importance_df[\"feature\"].values[:20]\n",
    "# for feat in top_features:\n",
    "#     plt.figure()\n",
    "#     shap.dependence_plot(feat, shap_vals, X_sample, show=False)\n",
    "#     # plt.savefig(os.path.join(model_dir, f\"shap_dependence_{feat}.png\"), bbox_inches='tight')\n",
    "#     plt.close()\n",
    "print(\"âœ… å·²å„²å­˜å‰ 20 å€‹ SHAP dependence plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13124309",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Identifier = 'SeriesA-R4'\n",
    "\n",
    "NUM_BOOST_ROUND = 1_000\n",
    "EARLY_STOPPING_ROUNDS = 150\n",
    "VERBOSE_EVAL = 50\n",
    "\n",
    "\n",
    "\n",
    "XGB_PARAMS = {\n",
    "    'objective': 'rank:pairwise',\n",
    "    'eval_metric': 'ndcg@3',\n",
    "    \"learning_rate\": 0.075,\n",
    "    \"max_depth\": 14,\n",
    "    \"min_child_weight\": 7,\n",
    "    \"subsample\": 0.95,\n",
    "    \"colsample_bytree\": 0.40,\n",
    "    \"gamma\": 3.3084297630544888,\n",
    "    \"lambda\": 5.952586917313028,\n",
    "    \"alpha\": 0.6395254133055179,\n",
    "    \"seed\": RANDOM_STATE,\n",
    "    \"n_jobs\": -1,\n",
    "    # \"tree_method\": \"gpu_hist\",  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963387ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "\n",
    "# åƒæ•¸\n",
    "model_path = \"model_output/selected_features_xgb/one_model/top100/lgb_ranker_model.txt\"\n",
    "parquet_path = \"data/Try only companyID/test_filled.parquet\"\n",
    "\n",
    "# è®€å– LightGBM æ¨¡å‹\n",
    "lgb_model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "# å–å¾— feature_names\n",
    "model_features = lgb_model.feature_name()\n",
    "if model_features is None or len(model_features) == 0:\n",
    "    raise ValueError(\"âŒ æ¨¡å‹æ²’æœ‰ feature_namesï¼Œè«‹ç¢ºèªè¨“ç·´æ™‚æœ‰æŒ‡å®š feature_names\")\n",
    "print(f\"âœ… æ¨¡å‹å…± {len(model_features)} å€‹features\")\n",
    "\n",
    "# è®€å– test_filled\n",
    "df = pl.read_parquet(parquet_path)\n",
    "print(f\"âœ… è®€å– test_filledï¼Œå…± {df.height} rows\")\n",
    "\n",
    "# æª¢æŸ¥ç¼ºå¤±\n",
    "missing_in_data = [f for f in model_features if f not in df.columns]\n",
    "if missing_in_data:\n",
    "    raise ValueError(f\"âŒ ä¸‹åˆ—ç‰¹å¾µåœ¨ test_filled ä¸å­˜åœ¨: {missing_in_data}\")\n",
    "\n",
    "# ç¯©é¸&æ’åº\n",
    "df_for_predict = df.select(model_features)\n",
    "X_np = df_for_predict.to_numpy()\n",
    "\n",
    "# é æ¸¬\n",
    "preds = lgb_model.predict(X_np)\n",
    "print(f\"âœ… é æ¸¬å®Œæˆï¼Œå…± {len(preds)} ç­†\")\n",
    "\n",
    "# å›å­˜çµæœ\n",
    "df_result = (\n",
    "    df\n",
    "    .with_columns([\n",
    "        pl.Series(\"selected\", preds)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# æŸ¥çœ‹å‰å¹¾ç­†\n",
    "print(df_result.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f531e218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹å…± 81 å€‹features\n",
      "âœ… è®€å– test_filledï¼Œå…± 6897776 rows\n",
      "âœ… é æ¸¬å®Œæˆï¼Œå…± 6897776 ç­†\n",
      "shape: (5, 288)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Id       â”† bySelf â”† companyID â”† nationalit â”† â€¦ â”† companyID_ â”† companyID_ â”† companyID â”† selected  â”‚\n",
      "â”‚ ---      â”† ---    â”† ---       â”† y          â”†   â”† loo_mean_l â”† loo_select â”† _total_oc â”† ---       â”‚\n",
      "â”‚ i64      â”† i8     â”† i64       â”† ---        â”†   â”† egs1_arriv â”† ed_count   â”† currences â”† f32       â”‚\n",
      "â”‚          â”†        â”†           â”† i64        â”†   â”† â€¦          â”† ---        â”† ---       â”†           â”‚\n",
      "â”‚          â”†        â”†           â”†            â”†   â”† ---        â”† i64        â”† i64       â”†           â”‚\n",
      "â”‚          â”†        â”†           â”†            â”†   â”† f64        â”†            â”†           â”†           â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 18144679 â”† 1      â”† 62840     â”† 36         â”† â€¦ â”† 22.777778  â”† 18         â”† 150       â”† -0.585417 â”‚\n",
      "â”‚ 18144680 â”† 1      â”† 62840     â”† 36         â”† â€¦ â”† 22.777778  â”† 18         â”† 150       â”† 0.031562  â”‚\n",
      "â”‚ 18144681 â”† 1      â”† 62840     â”† 36         â”† â€¦ â”† 22.777778  â”† 18         â”† 150       â”† -2.518357 â”‚\n",
      "â”‚ 18144682 â”† 1      â”† 62840     â”† 36         â”† â€¦ â”† 22.777778  â”† 18         â”† 150       â”† -0.986068 â”‚\n",
      "â”‚ 18144683 â”† 1      â”† 62840     â”† 36         â”† â€¦ â”† 22.777778  â”† 18         â”† 150       â”† -0.944413 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# # åƒæ•¸model_output/selected_features_xgb/one_model/v1_base_features/with_companyID_engineer/v1_model/top120/xgb_params.json\n",
    "model_path = \"model_output/selected_features_xgb/one_model/features_v1_with_company_ID/k-fold/top80_v2/xgb_ranker_model.pkl\"\n",
    "parquet_path = \"data/Try only companyID/test_filled.parquet\"\n",
    "\n",
    "# # è®€å–æ¨¡å‹\n",
    "# xgb_model = xgb.Booster(model_file=model_path)\n",
    "# ä½¿ç”¨ joblib è¼‰å…¥ .pkl æ¨¡å‹\n",
    "xgb_model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "# ç¢ºèª feature_names\n",
    "model_features = xgb_model.feature_names\n",
    "if model_features is None:\n",
    "    raise ValueError(\"âŒ æ¨¡å‹æ²’æœ‰ feature_namesï¼Œè«‹ç¢ºèªè¨“ç·´æ™‚æœ‰æŒ‡å®š feature_names\")\n",
    "print(f\"âœ… æ¨¡å‹å…± {len(model_features)} å€‹features\")\n",
    "\n",
    "# è®€å– test_filled\n",
    "# df = pl.read_parquet(parquet_path)\n",
    "print(f\"âœ… è®€å– test_filledï¼Œå…± {df.height} rows\")\n",
    "\n",
    "# æª¢æŸ¥ç¼ºå¤±\n",
    "missing_in_data = [f for f in model_features if f not in df.columns]\n",
    "if missing_in_data:\n",
    "    raise ValueError(f\"âŒ ä¸‹åˆ—ç‰¹å¾µåœ¨ test_filled ä¸å­˜åœ¨: {missing_in_data}\")\n",
    "\n",
    "# ç¯©é¸&æ’åº\n",
    "df_for_predict = df.select(model_features)\n",
    "X_np = df_for_predict.to_numpy()\n",
    "\n",
    "# é æ¸¬\n",
    "dtest = xgb.DMatrix(X_np, feature_names=model_features)\n",
    "preds = xgb_model.predict(dtest)\n",
    "print(f\"âœ… é æ¸¬å®Œæˆï¼Œå…± {len(preds)} ç­†\")\n",
    "\n",
    "# å›å­˜çµæœ\n",
    "df_result = (\n",
    "    df\n",
    "    .with_columns([\n",
    "        pl.Series(\"selected\", preds)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# æŸ¥çœ‹å‰å¹¾ç­†\n",
    "print(df_result.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3aa1e6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å„²å­˜åŸå§‹ submission: model_output/selected_features_xgb/one_model/features_v1_with_company_ID/k-fold/top80_v2/raw_submission.parquet\n",
      "shape: (6_897_776, 4)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Id       â”† ranker_id                       â”† selected  â”† __index_level_0__ â”‚\n",
      "â”‚ ---      â”† ---                             â”† ---       â”† ---               â”‚\n",
      "â”‚ i64      â”† str                             â”† f64       â”† i64               â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 18144679 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† -0.585417 â”† 18144679          â”‚\n",
      "â”‚ 18144680 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† 0.031562  â”† 18144680          â”‚\n",
      "â”‚ 18144681 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† -2.518357 â”† 18144681          â”‚\n",
      "â”‚ 18144682 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† -0.986068 â”† 18144682          â”‚\n",
      "â”‚ 18144683 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† -0.944413 â”† 18144683          â”‚\n",
      "â”‚ â€¦        â”† â€¦                               â”† â€¦         â”† â€¦                 â”‚\n",
      "â”‚ 25043143 â”† c5622e0de0594bde95a4dd8c1fcff7â€¦ â”† -1.971644 â”† 25043143          â”‚\n",
      "â”‚ 25043144 â”† c5622e0de0594bde95a4dd8c1fcff7â€¦ â”† -0.173718 â”† 25043144          â”‚\n",
      "â”‚ 25043145 â”† c5622e0de0594bde95a4dd8c1fcff7â€¦ â”† -1.973152 â”† 25043145          â”‚\n",
      "â”‚ 25043146 â”† c5622e0de0594bde95a4dd8c1fcff7â€¦ â”† -0.84862  â”† 25043146          â”‚\n",
      "â”‚ 25043147 â”† c5622e0de0594bde95a4dd8c1fcff7â€¦ â”† -2.055072 â”† 25043147          â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "âœ… å·²å„²å­˜rank submission: model_output/selected_features_xgb/one_model/features_v1_with_company_ID/k-fold/top80_v2/rank_submission.parquet\n",
      "shape: (6_897_776, 4)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Id       â”† ranker_id                       â”† selected â”† __index_level_0__ â”‚\n",
      "â”‚ ---      â”† ---                             â”† ---      â”† ---               â”‚\n",
      "â”‚ i64      â”† str                             â”† u32      â”† i64               â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 18144679 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† 30       â”† 18144679          â”‚\n",
      "â”‚ 18144680 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† 9        â”† 18144680          â”‚\n",
      "â”‚ 18144681 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† 116      â”† 18144681          â”‚\n",
      "â”‚ 18144682 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† 46       â”† 18144682          â”‚\n",
      "â”‚ 18144683 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† 45       â”† 18144683          â”‚\n",
      "â”‚ â€¦        â”† â€¦                               â”† â€¦        â”† â€¦                 â”‚\n",
      "â”‚ 25043143 â”† c5622e0de0594bde95a4dd8c1fcff7â€¦ â”† 9        â”† 25043143          â”‚\n",
      "â”‚ 25043144 â”† c5622e0de0594bde95a4dd8c1fcff7â€¦ â”† 3        â”† 25043144          â”‚\n",
      "â”‚ 25043145 â”† c5622e0de0594bde95a4dd8c1fcff7â€¦ â”† 10       â”† 25043145          â”‚\n",
      "â”‚ 25043146 â”† c5622e0de0594bde95a4dd8c1fcff7â€¦ â”† 7        â”† 25043146          â”‚\n",
      "â”‚ 25043147 â”† c5622e0de0594bde95a4dd8c1fcff7â€¦ â”† 11       â”† 25043147          â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "from scripts.group_wise import export_submission_parquets\n",
    "\n",
    "\n",
    "n_top =100\n",
    "export_submission_parquets(\n",
    "    test_filled_with_preds=df_result,   # ä½ çš„å¸¶æœ‰ selected åˆ†æ•¸çš„ DataFrame\n",
    "    output_dir=\"model_output/selected_features_xgb/one_model/features_v1_with_company_ID/k-fold/top80_v2\",\n",
    "    ranked_filename = \"rank_submission.parquet\",\n",
    "    raw_filename =\"raw_submission.parquet\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FlightRank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
