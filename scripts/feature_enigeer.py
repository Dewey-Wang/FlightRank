import os
import polars as pl

def build_price_features(
    df: pl.DataFrame,
    output_dir: str = None
) -> pl.DataFrame:
    """
    çµ¦ä¸€å€‹ DataFrameï¼Œç”¢ç”Ÿåƒ¹æ ¼ç‰¹å¾µã€‚
    å¦‚æœ output_dir æœ‰çµ¦ï¼ŒæœƒæŠŠçµæœå­˜æˆ output_dir/price_features.parquetã€‚

    å›å‚³ï¼šåƒ…åŒ…å« Id + æ–°å¢ç‰¹å¾µ
    """
    price_features = df.select([
        pl.col("Id"),
        
        (pl.col("totalPrice") / (pl.col("taxes") + 1)).alias("price_per_tax"),
        (pl.col("taxes") / (pl.col("totalPrice") + 1)).alias("tax_rate"),
        pl.col("totalPrice").log1p().alias("log_price"),

        pl.col("totalPrice")
          .rank(method="dense", descending=False)
          .over("ranker_id")
          .alias("totalPrice_rank"),

        (
            pl.col("totalPrice") == pl.col("totalPrice").min().over("ranker_id")
        ).cast(pl.Int8).alias("is_cheapest"),

        (
            (pl.col("totalPrice") - pl.col("totalPrice").median().over("ranker_id")) /
            (pl.col("totalPrice").std().over("ranker_id") + 1)
        ).alias("price_from_median_zscore"),

        (
            pl.col("totalPrice")
              .rank("average")
              .over("ranker_id")
            / pl.col("totalPrice").count().over("ranker_id")
        ).alias("price_percentile")
    ])
    
    print("âœ… å·²å®Œæˆåƒ¹æ ¼ç‰¹å¾µå·¥ç¨‹")
    
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, "1_price_features.parquet")
        price_features.write_parquet(output_path)
        print(f"âœ… å·²å„²å­˜ Parquet: {output_path}")
    
    return price_features



import os
import polars as pl

def build_duration_features(
    df: pl.DataFrame,
    output_dir: str = None
) -> pl.DataFrame:
    """
    å° Duration/Price per Duration åšç‰¹å¾µå·¥ç¨‹:
    - æ–‡å­— duration è½‰æ›æˆåˆ†é˜
    - total_duration
    - ranker_id åˆ†ç¾¤æ’å
    - price_per_duration & æ’å

    å¦‚æœ output_dir çµ¦å®šï¼Œæœƒè¼¸å‡º duration_features.parquet
    """
    duration_cols = [
        "legs0_duration",
        "legs1_duration",
        "legs0_segments0_duration",
        "legs0_segments1_duration",
        "legs0_segments2_duration",
        "legs0_segments3_duration",
        "legs1_segments0_duration",
        "legs1_segments1_duration",
        "legs1_segments2_duration",
        "legs1_segments3_duration"
    ]

    # durationæ¬„ä½è½‰åˆ†é˜
    duration_exprs = [
        pl.when(pl.col(c).is_in([None, "missing"]))
          .then(0)
          .otherwise(
              pl.col(c).str.extract(r"^(\d+):", 1).cast(pl.Int64) * 60 +
              pl.col(c).str.extract(r":(\d+):", 1).cast(pl.Int64)
          )
          .alias(c)
        for c in duration_cols if c in df.columns
    ]

    df = df.with_columns(duration_exprs)

    # åŠ ç¸½ total_duration
    if all(c in df.columns for c in ["legs0_duration", "legs1_duration"]):
        df = df.with_columns([
            (pl.col("legs0_duration") + pl.col("legs1_duration")).alias("total_duration")
        ])

    # rankè¡¨é”å¼
    rank_exprs = [
        pl.col(c)
          .rank(method="dense", descending=False)
          .over("ranker_id")
          .cast(pl.Int32)
          .alias(f"{c}_rank")
        for c in (duration_cols + ["total_duration"]) if c in df.columns
    ]
    df = df.with_columns(rank_exprs)

    # price_per_duration
    df = df.with_columns([
        (pl.col("totalPrice") / (pl.col("total_duration") + 1)).alias("price_per_duration")
    ])

    # price_per_duration_rank
    df = df.with_columns([
        pl.col("price_per_duration")
          .rank(method="dense", descending=False)
          .over("ranker_id")
          .alias("price_per_duration_rank")
    ])

    print("âœ… å·²å®Œæˆ Duration ç‰¹å¾µå·¥ç¨‹ (å«æ’åèˆ‡ price_per_duration)")

    # åªä¿ç•™ Id èˆ‡æ–°ç‰¹å¾µ
    keep_cols = ["Id"] + [
        c for c in df.columns
        if c not in ["ranker_id", "totalPrice"] and (
            c.endswith("_duration") or
            c.endswith("_rank") or
            c in ["total_duration", "price_per_duration", "price_per_duration_rank"]
        )
    ]

    duration_features = df.select(keep_cols)

    # è¼¸å‡º parquet
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, "2_duration_features.parquet")
        duration_features.write_parquet(output_path)
        print(f"âœ… å·²è¼¸å‡º Parquet: {output_path}")

    return duration_features



import polars as pl
import os

def build_frequent_flyer_match_features(
    df: pl.DataFrame,
    output_dir: str = None,
    output_filename: str = "3_frequent_flyer_features.parquet"
) -> pl.DataFrame:
    """
    å°‡ frequentFlyer èˆ‡å„æ®µèˆªæ®µ carrier_code æ¯”å°ï¼Œå»ºç«‹ä¸‹åˆ—ç‰¹å¾µï¼š
    - has_frequentFlyer
    - n_ff_programs
    - carrier match flags
    - matched/unmatched duration
    - å„ç¨®æ’å
    """
    # 0. FrequentFlyer è¡ç”Ÿç‰¹å¾µ
    df = df.with_columns([
        pl.col("frequentFlyer").cast(pl.Utf8).fill_null("missing").alias("frequentFlyer")
    ])
    
    df = df.with_columns([
        (
            (pl.col("frequentFlyer") != "") &
            (pl.col("frequentFlyer") != "missing")
        ).cast(pl.Int8).alias("has_frequentFlyer"),
        (
            pl.col("frequentFlyer").map_elements(
                lambda s: 0 if s in ("", "missing") else s.count("/") + 1,
                return_dtype=pl.Int32
            ).alias("n_ff_programs")
        )
    ])

    # 1. clean frequentFlyer
    cleaned_ff = (
        pl.col("frequentFlyer")
        .fill_null("")
        .str.replace_all("missing", "")
        .str.split("/")
    )

    # 2. segments
    segments = [
        "legs0_segments0",
        "legs0_segments1",
        "legs0_segments2",
        "legs0_segments3",
        "legs1_segments0",
        "legs1_segments1",
        "legs1_segments2",
        "legs1_segments3"
    ]

    # 3. æ˜¯å¦ in_ff
    exprs = []
    for seg in segments:
        for carrier_type in ["marketingCarrier_code", "operatingCarrier_code"]:
            carrier_col = f"{seg}_{carrier_type}"
            exprs.append(
                pl.col(carrier_col)
                .fill_null("")
                .is_in(cleaned_ff)
                .cast(pl.Int8)
                .alias(f"{carrier_col}_in_ff")
            )

    df = df.with_columns(exprs)

    # 4. durationæ¬„ä½è½‰åˆ†é˜
    duration_cols = [
        "legs0_duration",
        "legs1_duration",
        "legs0_segments0_duration",
        "legs0_segments1_duration",
        "legs0_segments2_duration",
        "legs0_segments3_duration",
        "legs1_segments0_duration",
        "legs1_segments1_duration",
        "legs1_segments2_duration",
        "legs1_segments3_duration"
    ]

    duration_exprs = [
        pl.when(pl.col(c).is_in([None, "missing"]))
          .then(0)
          .otherwise(
              pl.col(c).str.extract(r"^(\d+):", 1).cast(pl.Int64) * 60 +
              pl.col(c).str.extract(r":(\d+):", 1).cast(pl.Int64)
          )
          .alias(c)
        for c in duration_cols if c in df.columns
    ]

    df = df.with_columns(duration_exprs)

    # 5. total_duration
    if all(c in df.columns for c in ["legs0_duration", "legs1_duration"]):
        df = df.with_columns([
            (pl.col("legs0_duration") + pl.col("legs1_duration")).alias("total_duration")
        ])

    # 6. ç´¯ç©matched duration
    legs0_matched_duration_sum = pl.sum_horizontal([
        pl.col(f"legs0_segments{i}_duration") *
        (
            pl.col(f"legs0_segments{i}_marketingCarrier_code_in_ff") |
            pl.col(f"legs0_segments{i}_operatingCarrier_code_in_ff")
        ).cast(pl.Int8)
        for i in range(4)
    ]).alias("legs0_matched_duration_sum")

    legs1_matched_duration_sum = pl.sum_horizontal([
        pl.col(f"legs1_segments{i}_duration") *
        (
            pl.col(f"legs1_segments{i}_marketingCarrier_code_in_ff") |
            pl.col(f"legs1_segments{i}_operatingCarrier_code_in_ff")
        ).cast(pl.Int8)
        for i in range(4)
    ]).alias("legs1_matched_duration_sum")

    df = df.with_columns([
        legs0_matched_duration_sum,
        legs1_matched_duration_sum,
    ])
    df = df.with_columns([

        (pl.col("legs0_matched_duration_sum") + pl.col("legs1_matched_duration_sum")).alias("all_matched_duration_sum"),
    ])
    # unmatched
    df = df.with_columns([
        (pl.col("total_duration") - pl.col("all_matched_duration_sum")).alias("unmatched_duration")
    ])

    # 7. æ’å
    rank_exprs = [
        pl.col("legs0_matched_duration_sum")
          .rank(method="dense", descending=True)
          .over("ranker_id")
          .cast(pl.Int32)
          .alias("legs0_matched_duration_sum_rank"),

        pl.col("legs1_matched_duration_sum")
          .rank(method="dense", descending=True)
          .over("ranker_id")
          .cast(pl.Int32)
          .alias("legs1_matched_duration_sum_rank"),

        pl.col("unmatched_duration")
          .rank(method="dense", descending=False)
          .over("ranker_id")
          .cast(pl.Int32)
          .alias("unmatched_duration_rank")
    ]

    duration_rank_exprs = [
        pl.col(c)
          .rank(method="dense", descending=False)
          .over("ranker_id")
          .cast(pl.Int32)
          .alias(f"{c}_rank")
        for c in (duration_cols + ["total_duration"]) if c in df.columns
    ]

    df = df.with_columns(rank_exprs + duration_rank_exprs)

    # 8. è¼¸å‡º
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, output_filename)
        df.write_parquet(output_path)
        print(f"âœ… å·²å„²å­˜ Parquet: {output_path}")

    print("âœ… å·²å®Œæˆ frequentFlyer ç‰¹å¾µ + match ç‰¹å¾µ + duration ç‰¹å¾µç”Ÿæˆ")
    return df


import os
import polars as pl

def build_baggage_fee_features(
    df: pl.DataFrame,
    output_dir: str = None,
    output_filename: str = "4_baggage_fee_features.parquet"
) -> pl.DataFrame:
    """
    å»ºç«‹è¡Œæèˆ‡è²»ç”¨ç›¸é—œç‰¹å¾µ:
    - baggage_total: legs0 + legs1 è¡Œææ•¸
    - total_fees: miniRules0 + miniRules1 çš„è²»ç”¨
    - has_baggage: æ˜¯å¦æœ‰ä»»ä½•è¡Œæ
    - has_fees: æ˜¯å¦æœ‰ä»»ä½•è²»ç”¨
    - price_per_fee: totalPrice / (total_fees + 1)
    - price_minus_fee: totalPrice - total_fees
    - groupby ranker_id æ’å
    """

    # è¡Œæç¸½æ•¸
    baggage_total = (
        pl.col("legs0_segments0_baggageAllowance_quantity").fill_null(0) +
        pl.col("legs1_segments0_baggageAllowance_quantity").fill_null(0)
    ).alias("baggage_total")

    # è²»ç”¨ç¸½é¡
    total_fees = (
        pl.col("miniRules0_monetaryAmount").fill_null(0) +
        pl.col("miniRules1_monetaryAmount").fill_null(0)
    ).alias("total_fees")

    # æ˜¯å¦æœ‰è¡Œæ
    has_baggage = (
        (pl.col("baggage_total") > 0)
    ).cast(pl.Int32).alias("has_baggage")

    # æ˜¯å¦æœ‰è²»ç”¨
    has_fees = (
        (pl.col("total_fees") > 0)
    ).cast(pl.Int32).alias("has_fees")

    # totalPrice / (total_fees + 1)
    price_per_fee = (
        (pl.col("totalPrice") / (pl.col("total_fees") + 1))
        .alias("price_per_fee")
    )

    # totalPrice - total_fees
    price_minus_fee = (
        (pl.col("totalPrice") - pl.col("total_fees"))
        .alias("price_minus_fee")
    )

    # åŠ å…¥ä¸»è¦æ¬„ä½
    df = df.with_columns([
        baggage_total,
        total_fees,
    ])
    df = df.with_columns([
        has_baggage,
        has_fees,
        price_per_fee,
        price_minus_fee
    ])
    # æ’å
    rank_exprs = [
        # baggage_total (æ•¸å­—è¶Šå¤§ rankè¶Šä½)
        pl.col("baggage_total")
          .rank(method="dense", descending=True)
          .over("ranker_id")
          .cast(pl.Int32)
          .alias("baggage_total_rank"),

        # price_per_fee (æ•¸å­—è¶Šå¤§ rankè¶Šä½)
        pl.col("price_per_fee")
          .rank(method="dense", descending=True)
          .over("ranker_id")
          .cast(pl.Int32)
          .alias("price_per_fee_rank"),

        # price_minus_fee (æ•¸å­—è¶Šå¤§ rankè¶Šä½)
        pl.col("price_minus_fee")
          .rank(method="dense", descending=True)
          .over("ranker_id")
          .cast(pl.Int32)
          .alias("price_minus_fee_rank"),

        # total_fees (æ•¸å­—è¶Šå° rankè¶Šä½)
        pl.col("total_fees")
          .rank(method="dense", descending=False)
          .over("ranker_id")
          .cast(pl.Int32)
          .alias("total_fees_rank")
    ]

    df = df.with_columns(rank_exprs)

    # è¼¸å‡º parquet
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, output_filename)
        df.write_parquet(output_path)
        print(f"âœ… å·²å„²å­˜ Parquet: {output_path}")

    return df



import os
import polars as pl

def build_cabin_features(
    df: pl.DataFrame,
    output_dir: str = None,
    output_filename: str = "5_cabin_features.parquet"
) -> pl.DataFrame:
    """
    å»ºç«‹è‰™ç­‰ç‰¹å¾µï¼ŒåŒ…å«:
    - å¹³å‡è‰™ç­‰
    - æœ€é•·segmentè‰™ç­‰
    - æ˜¯å¦æœ€é•·segmentè‰™ç­‰ç›¸åŒ
    - åŠ æ¬Šå¹³å‡è‰™ç­‰
    - legs0/legs1è‰™ç­‰æ˜¯å¦ä¸€è‡´

    æœƒå…ˆå°‡durationæ¬„ä½è½‰åˆ†é˜ã€‚
    """

    # æ¬„ä½
    legs0_cabin_cols = [
        "legs0_segments0_cabinClass",
        "legs0_segments1_cabinClass",
        "legs0_segments2_cabinClass",
        "legs0_segments3_cabinClass"
    ]
    legs1_cabin_cols = [
        "legs1_segments0_cabinClass",
        "legs1_segments1_cabinClass",
        "legs1_segments2_cabinClass",
        "legs1_segments3_cabinClass"
    ]
    legs0_duration_cols = [
        "legs0_segments0_duration",
        "legs0_segments1_duration",
        "legs0_segments2_duration",
        "legs0_segments3_duration"
    ]
    legs1_duration_cols = [
        "legs1_segments0_duration",
        "legs1_segments1_duration",
        "legs1_segments2_duration",
        "legs1_segments3_duration"
    ]

    duration_cols = legs0_duration_cols + legs1_duration_cols

    # Step1: durationæ¬„ä½è½‰åˆ†é˜
    duration_exprs = [
        pl.when(pl.col(c).is_in([None, "missing"]))
          .then(0)
          .otherwise(
              pl.col(c).str.extract(r"^(\d+):", 1).cast(pl.Int64) * 60 +
              pl.col(c).str.extract(r":(\d+):", 1).cast(pl.Int64)
          )
          .alias(c)
        for c in duration_cols if c in df.columns
    ]
    df = df.with_columns(duration_exprs)

    # Step2: cabinæ¬„ä½è½‰Int64
    for c in legs0_cabin_cols + legs1_cabin_cols:
        df = df.with_columns(pl.col(c).cast(pl.Int64))

    # å¹³å‡è‰™ç­‰
    legs0_mean = (
        pl.concat_list([pl.col(c) for c in legs0_cabin_cols])
        .list.eval(pl.element().filter(pl.element() > 0))
        .list.mean()
        .fill_null(0)
        .alias("legs0_mean_cabin")
    )
    legs1_mean = (
        pl.concat_list([pl.col(c) for c in legs1_cabin_cols])
        .list.eval(pl.element().filter(pl.element() > 0))
        .list.mean()
        .fill_null(0)
        .alias("legs1_mean_cabin")
    )

    is_same_cabin = (
        (
            pl.concat_list([pl.col(c) for c in legs0_cabin_cols]).list.unique().sort()
            ==
            pl.concat_list([pl.col(c) for c in legs1_cabin_cols]).list.unique().sort()
        )
        .cast(pl.Int8)
        .alias("is_legs0_legs1_cabin_same")
    )

    # æœ€é•·segment index
    def longest_segment_idx(durations):
        if all(d is None for d in durations):
            return None
        idx = max(
            ((i, int(d) if d is not None else -1) for i, d in enumerate(durations)),
            key=lambda x: x[1]
        )[0]
        return idx

    def max_duration_cabin(row, dur_cols, cabin_cols):
        durations = [row[c] for c in dur_cols]
        cabins = [row[c] for c in cabin_cols]
        idx = longest_segment_idx(durations)
        if idx is None:
            return 0
        return cabins[idx] if cabins[idx] is not None else 0

    df = df.with_columns([
        pl.struct(legs0_duration_cols + legs0_cabin_cols)
        .map_elements(lambda row: max_duration_cabin(row, legs0_duration_cols, legs0_cabin_cols))
        .alias("legs0_max_duration_cabin"),

        pl.struct(legs1_duration_cols + legs1_cabin_cols)
        .map_elements(lambda row: max_duration_cabin(row, legs1_duration_cols, legs1_cabin_cols))
        .alias("legs1_max_duration_cabin"),
    ])

    df = df.with_columns(
        (
            (pl.col("legs0_max_duration_cabin") == pl.col("legs1_max_duration_cabin"))
            .cast(pl.Int8)
            .alias("is_max_duration_cabin_same")
        )
    )

    def weighted_mean(durations, cabins):
        pairs = [(d, c) for d, c in zip(durations, cabins) if d is not None and c not in (0, None)]
        if not pairs:
            return 0
        num = sum(d * c for d, c in pairs)
        denom = sum(d for d, _ in pairs)
        return num / denom if denom > 0 else 0

    df = df.with_columns([
        pl.struct(legs0_duration_cols + legs0_cabin_cols)
        .map_elements(lambda row: weighted_mean(
            [row[c] for c in legs0_duration_cols],
            [row[c] for c in legs0_cabin_cols]
        ))
        .alias("legs0_weighted_mean_cabin"),

        pl.struct(legs1_duration_cols + legs1_cabin_cols)
        .map_elements(lambda row: weighted_mean(
            [row[c] for c in legs1_duration_cols],
            [row[c] for c in legs1_cabin_cols]
        ))
        .alias("legs1_weighted_mean_cabin"),
    ])

    all_duration_cols = legs0_duration_cols + legs1_duration_cols
    all_cabin_cols = legs0_cabin_cols + legs1_cabin_cols

    df = df.with_columns(
        pl.struct(all_duration_cols + all_cabin_cols)
        .map_elements(lambda row: weighted_mean(
            [row[c] for c in all_duration_cols],
            [row[c] for c in all_cabin_cols]
        ))
        .alias("total_weighted_mean_cabin")
    )

    df = df.with_columns([
        legs0_mean,
        legs1_mean,
        is_same_cabin
    ])

    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, output_filename)
        df.write_parquet(output_path)
        print(f"âœ… å·²å„²å­˜ Parquet: {output_path}")

    return df

import polars as pl
import os

def build_time_features(
    df: pl.DataFrame,
    output_dir: str = None,
    output_filename: str = "6_time_features.parquet"
) -> pl.DataFrame:
    """
    å»ºç«‹èˆ‡æ™‚é–“ç›¸é—œçš„ç‰¹å¾µ:
    - æ™‚é–“åˆ†æ®µ
    - æ˜¯å¦é€±æœ«
    - æ˜¯å¦ round trip
    - å‡ºç™¼åˆ°æŠµé”çš„å¤©æ•¸
    - é è¨‚åˆ°å‡ºç™¼çš„å¤©æ•¸
    """
    time_cols = ["legs0_departureAt", "legs0_arrivalAt", "legs1_departureAt", "legs1_arrivalAt"]
    time_exprs = []

    for col in time_cols:
        if col in df.columns:
            cleaned_col = (
                pl.when(pl.col(col) == "missing")
                  .then(None)
                  .otherwise(pl.col(col))
            )

            dt = cleaned_col.str.to_datetime(strict=False)
            h = dt.dt.hour()

            period = (
                pl.when(h.is_between(0,5)).then(0)
                .when(h.is_between(6,11)).then(1)
                .when(h.is_between(12,17)).then(2)
                .when(h.is_between(18,23)).then(3)
            )

            is_weekend = (
                (dt.dt.weekday() >= 5)
            ).cast(pl.Int32).fill_null(-1)

            time_exprs.extend([
                h.fill_null(-1).alias(f"{col}_hour"),
                dt.dt.weekday().fill_null(-1).alias(f"{col}_weekday"),
                (
                    ((h >= 6) & (h <= 9)) | ((h >= 17) & (h <= 20))
                ).cast(pl.Int32).fill_null(-1).alias(f"{col}_business_time"),
                period.fill_null(-1).alias(f"{col}_day_period"),
                is_weekend.alias(f"{col}_is_weekend")
            ])

    # æ˜¯å¦ round trip
    round_trip_flag = (
        (
            (pl.col("legs1_departureAt").is_not_null() & (pl.col("legs1_departureAt") != "missing"))
            |
            (pl.col("legs1_arrivalAt").is_not_null() & (pl.col("legs1_arrivalAt") != "missing"))
        )
        .cast(pl.Int8)
        .alias("is_round_trip")
    )

    # legs0_departureAt datetime
    depart_dt = (
        pl.when(pl.col("legs0_departureAt") == "missing")
          .then(None)
          .otherwise(pl.col("legs0_departureAt"))
    ).str.to_datetime(strict=False)

    # legs1_arrivalAt datetime
    arrive_dt = (
        pl.when(pl.col("legs1_arrivalAt") == "missing")
          .then(None)
          .otherwise(pl.col("legs1_arrivalAt"))
    ).str.to_datetime(strict=False)

    # å‡ºç™¼ ~ æŠµé”å¤©æ•¸
    duration_ms_arrive = (arrive_dt - depart_dt).dt.total_milliseconds()
    days_between = (
        (duration_ms_arrive / (1000 * 60 * 60 * 24))
        .floor()
        .cast(pl.Int32)
        .fill_null(0)
        .alias("days_between_departure_arrival")
    )

    # requestDate ~ å‡ºç™¼å¤©æ•¸
    request_dt = pl.col("requestDate")
    duration_ms_request = (depart_dt - request_dt).dt.total_milliseconds()
    days_before_departure = (
        (duration_ms_request / (1000 * 60 * 60 * 24))
        .floor()
        .cast(pl.Int32)
        .fill_null(-1)
        .alias("days_before_departure")
    )

    # åŠ å…¥å…¨éƒ¨ç‰¹å¾µ
    df = df.with_columns(
        time_exprs +
        [round_trip_flag, days_between, days_before_departure]
    )

    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, output_filename)
        df.write_parquet(output_path)
        print(f"âœ… å·²å„²å­˜ Parquet: {output_path}")

    print("âœ… æ‰€æœ‰æ™‚é–“ç‰¹å¾µå·²ç”Ÿæˆå®Œæˆ")
    return df



import polars as pl
import os

def build_corporate_access_route_features(
    df: pl.DataFrame,
    output_dir: str = None,
    output_filename: str = "7_corporate_access_route_features.parquet"
) -> pl.DataFrame:
    """
    å»ºç«‹ä»¥ä¸‹ç‰¹å¾µï¼š
    - æ˜¯å¦æœ‰ corporate tariff
    - æ˜¯å¦æœ‰ access TP
    - æ˜¯å¦ç‚ºç†±é–€èˆªç·š
    """
    df = df.with_columns([
        pl.col("corporateTariffCode").is_not_null().cast(pl.Int32).alias("has_corporate_tariff"),
        (pl.col("pricingInfo_isAccessTP") == 1).cast(pl.Int32).alias("has_access_tp"),
        pl.col("searchRoute").is_in([
            "MOWLED/LEDMOW",
            "LEDMOW/MOWLED",
            "MOWLED",
            "LEDMOW",
            "MOWAER/AERMOW"
        ]).cast(pl.Int32).alias("is_popular_route")
    ])

    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, output_filename)
        df.write_parquet(output_path)
        print(f"âœ… å·²å„²å­˜ Parquet: {output_path}")

    print("âœ… å·²å®Œæˆ corporate/access/route ç‰¹å¾µç”Ÿæˆ")
    return df


import polars as pl
import os

def build_transfer_features(
    df: pl.DataFrame,
    output_dir: str = None,
    output_filename: str = "8_transfer_features.parquet"
) -> pl.DataFrame:
    """
    å»ºç«‹è½‰æ©Ÿç›¸é—œç‰¹å¾µï¼ŒåŒ…å«ï¼š
    - legs0/legs1/ç¸½è½‰æ©Ÿæ¬¡æ•¸
    - duration_ratio
    - æ˜¯å¦ç›´é£›
    - groupby ranker_id æ’å
    - æ˜¯å¦æœ€å°‘è½‰æ©Ÿ
    """
    # å…ˆæŠŠ duration æ¬„ä½è½‰åˆ†é˜
    for dur_col in ["legs0_duration", "legs1_duration"]:
        if dur_col in df.columns and df[dur_col].dtype == pl.Utf8:
            df = df.with_columns(
                pl.when(pl.col(dur_col).is_in([None, "missing"]))
                .then(0)
                .otherwise(
                    pl.col(dur_col).str.extract(r"^(\d+):", 1).cast(pl.Int64) * 60 +
                    pl.col(dur_col).str.extract(r":(\d+):", 1).cast(pl.Int64)
                )
                .alias(dur_col)
            )

    # Legs0 segments1~3
    legs0_segment_cols = [
        "legs0_segments1_departureFrom_airport_iata",
        "legs0_segments2_departureFrom_airport_iata",
        "legs0_segments3_departureFrom_airport_iata"
    ]

    legs1_segment_cols = [
        "legs1_segments1_departureFrom_airport_iata",
        "legs1_segments2_departureFrom_airport_iata",
        "legs1_segments3_departureFrom_airport_iata"
    ]

    # legs0è½‰æ©Ÿæ¬¡æ•¸
    legs0_num_segments = (
        pl.sum_horizontal([
            ((pl.col(c).is_not_null()) & (pl.col(c) != "missing")).cast(pl.Int8)
            for c in legs0_segment_cols if c in df.columns
        ])
        .alias("legs0_num_transfers")
    )

    # legs1è½‰æ©Ÿæ¬¡æ•¸
    legs1_num_segments = (
        pl.sum_horizontal([
            ((pl.col(c).is_not_null()) & (pl.col(c) != "missing")).cast(pl.Int8)
            for c in legs1_segment_cols if c in df.columns
        ])
        .alias("legs1_num_transfers")
    )

    df = df.with_columns([
        legs0_num_segments,
        legs1_num_segments
    ])

    # legs0 + legs1ç¸½è½‰æ©Ÿæ¬¡æ•¸
    df = df.with_columns([
        (pl.col("legs0_num_transfers") + pl.col("legs1_num_transfers")).alias("total_num_transfers"),
        pl.when(pl.col("legs1_duration").fill_null(0) > 0)
            .then(pl.col("legs0_duration") / (pl.col("legs1_duration") + 1))
            .otherwise(1.0)
            .alias("duration_ratio")
    ])

    # æ˜¯å¦ç›´é£›
    df = df.with_columns([
        (pl.col("legs0_num_transfers") == 0).cast(pl.Int8).alias("legs0_is_direct"),
        (pl.col("legs1_num_transfers") == 0).cast(pl.Int8).alias("legs1_is_direct"),
        (
            (pl.col("legs0_num_transfers") == 0) & (pl.col("legs1_num_transfers") == 0)
        ).cast(pl.Int8).alias("both_legs_direct")
    ])

    # æ’å
    df = df.with_columns([
        pl.col("legs0_num_transfers").rank(method="dense", descending=False).over("ranker_id").alias("legs0_num_transfers_rank"),
        pl.col("legs1_num_transfers").rank(method="dense", descending=False).over("ranker_id").alias("legs1_num_transfers_rank"),
        pl.col("total_num_transfers").rank(method="dense", descending=False).over("ranker_id").alias("total_num_transfers_rank")
    ])

    # æ˜¯å¦æœ€å°‘è½‰æ©Ÿ
    df = df.with_columns([
        (pl.col("legs0_num_transfers") == pl.col("legs0_num_transfers").min().over("ranker_id"))
            .cast(pl.Int8).alias("legs0_is_min_transfers"),
        (pl.col("legs1_num_transfers") == pl.col("legs1_num_transfers").min().over("ranker_id"))
            .cast(pl.Int8).alias("legs1_is_min_transfers"),
        (pl.col("total_num_transfers") == pl.col("total_num_transfers").min().over("ranker_id"))
            .cast(pl.Int8).alias("total_is_min_transfers")
    ])

    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, output_filename)
        df.write_parquet(output_path)
        print(f"âœ… å·²å„²å­˜ Parquet: {output_path}")

    print("âœ… å·²å®Œæˆè½‰æ©Ÿç‰¹å¾µç”Ÿæˆ")
    return df



from typing import Optional
import os
def build_carrier_consistency_features(
    df: pl.DataFrame,
    output_dir: str = None,
    output_filename: str = "9_carrier_consistency_features.parquet",
    transform_config: Optional[dict] = None
) -> pl.DataFrame:
    """
    å»ºç«‹ legs0/legs1 ä¸» Carrier ä¸€è‡´æ€§ç‰¹å¾µ (è‡ªå‹•å…ˆè¨ˆç®—è½‰æ©Ÿæ¬¡æ•¸)ã€‚
    å¯é¸: ä½¿ç”¨transform_configé€²è¡Œå…±ç”¨Carrier/Departureç·¨ç¢¼ã€‚
    """

    # legs0è½‰æ©Ÿåˆ¤æ–·æ¬„
    legs0_segment_cols = [
        "legs0_segments1_departureFrom_airport_iata",
        "legs0_segments2_departureFrom_airport_iata",
        "legs0_segments3_departureFrom_airport_iata"
    ]
    legs1_segment_cols = [
        "legs1_segments1_departureFrom_airport_iata",
        "legs1_segments2_departureFrom_airport_iata",
        "legs1_segments3_departureFrom_airport_iata"
    ]

    # legs0è½‰æ©Ÿæ¬¡æ•¸
    legs0_num_segments = (
        pl.sum_horizontal([
            ((pl.col(c).is_not_null()) & (pl.col(c) != "missing")).cast(pl.Int8)
            for c in legs0_segment_cols if c in df.columns
        ])
        .alias("legs0_num_transfers")
    )
    # legs1è½‰æ©Ÿæ¬¡æ•¸
    legs1_num_segments = (
        pl.sum_horizontal([
            ((pl.col(c).is_not_null()) & (pl.col(c) != "missing")).cast(pl.Int8)
            for c in legs1_segment_cols if c in df.columns
        ])
        .alias("legs1_num_transfers")
    )

    df = df.with_columns([
        legs0_num_segments,
        legs1_num_segments
    ])

    # legs0ä¸»carrier
    legs0_marketing_cols = [
        f"legs0_segments{s}_marketingCarrier_code"
        for s in range(4) if f"legs0_segments{s}_marketingCarrier_code" in df.columns
    ]
    legs1_marketing_cols = [
        f"legs1_segments{s}_marketingCarrier_code"
        for s in range(4) if f"legs1_segments{s}_marketingCarrier_code" in df.columns
    ]

    legs0_main_carrier = (
        pl.coalesce([pl.col(c) for c in legs0_marketing_cols])
        .alias("legs0_main_carrier")
    )
    legs1_main_carrier = (
        pl.coalesce([pl.col(c) for c in legs1_marketing_cols])
        .alias("legs1_main_carrier")
    )

    df = df.with_columns([
        legs0_main_carrier,
        legs1_main_carrier
    ])

    # å…©è…¿ä¸€è‡´æ€§æ¨™è¨˜
    legs0_all_same = (
        pl.when(pl.col("legs0_num_transfers") == 0)
        .then(1)
        .otherwise(
            pl.all_horizontal([
                (pl.col(c) == pl.col("legs0_main_carrier")) & pl.col(c).is_not_null()
                for c in legs0_marketing_cols
            ]).cast(pl.Int8)
        )
        .alias("legs0_all_segments_carrier_same")
    )
    legs1_all_same = (
        pl.when(pl.col("legs1_num_transfers") == 0)
        .then(1)
        .otherwise(
            pl.all_horizontal([
                (pl.col(c) == pl.col("legs1_main_carrier")) & pl.col(c).is_not_null()
                for c in legs1_marketing_cols
            ]).cast(pl.Int8)
        )
        .alias("legs1_all_segments_carrier_same")
    )

    df = df.with_columns([
        legs0_all_same,
        legs1_all_same
    ])

    both_legs_all_same = (
        (
            (pl.col("legs0_all_segments_carrier_same") == 1) &
            (pl.col("legs1_all_segments_carrier_same") == 1) &
            (pl.col("legs0_main_carrier") == pl.col("legs1_main_carrier")) &
            pl.col("legs0_main_carrier").is_not_null() &
            pl.col("legs1_main_carrier").is_not_null()
        ).cast(pl.Int8)
        .alias("both_legs_carrier_all_same")
    )

    df = df.with_columns([
        both_legs_all_same
    ])

    # âœ… å¦‚æœæä¾›transform_configå°±åšå…±ç”¨encoding
    if transform_config:
        carrier_enc = transform_config["label_encoders"]["carrier_cols"]
        mapping_df = pl.DataFrame({
            "value": carrier_enc["values"],
            "rank_id": carrier_enc["codes"]
        })
        # éœ€è¦å…±ç”¨encodingçš„æ¬„ä½
        cols_to_encode = ["legs0_main_carrier", "legs1_main_carrier"]
        # departureæ¬„ä½
        departure_cols = [
            "legs0_segments0_departureFrom_airport_iata",
            "legs1_segments0_departureFrom_airport_iata"
        ]
        cols_to_encode += [c for c in departure_cols if c in df.columns]
        print(f"âœ… æ­£åœ¨å…±ç”¨carrier encodingè™•ç† {cols_to_encode}")

        for col in cols_to_encode:
            df_col = (
                df.select([col])
                .with_columns(pl.col(col).cast(pl.Utf8))
                .join(
                    mapping_df.rename({"value": col}),
                    on=col,
                    how="left"
                )
                .with_columns(
                    pl.col("rank_id").fill_null(-1).cast(pl.Int32).alias(f"{col}_encoded")
                )
                .drop([col, "rank_id"])      # <<=== é€™è¡Œå°±æœƒç¢ºä¿ rank_id æ°¸é ä¸ç•™
                .rename({f"{col}_encoded": col})
            )
            df = df.with_columns(df_col)


    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, output_filename)
        df.write_parquet(output_path)
        print(f"âœ… å·²å„²å­˜ Parquet: {output_path}")

    print("âœ… å·²å®Œæˆä¸»Carrierä¸€è‡´æ€§èˆ‡è½‰æ©Ÿæ¬¡æ•¸ç‰¹å¾µ")
    return df




import polars as pl
import pickle
import os
from typing import Optional, Dict, Tuple

def build_label_encoding_features(
    df: pl.DataFrame,
    output_dir: Optional[str] = None,
    transform_config: Optional[Dict] = None,
    id_col: str = "Id"
) -> Tuple[pl.DataFrame, dict]:
    """
    å¿«é€Ÿæ‰¹æ¬¡Label Encodingï¼Œä¿ç•™Idä¾›å¾ŒçºŒjoinã€‚
    å°airport_colså…±ç”¨åŒä¸€å€‹encodingï¼Œcarrier_colså…±ç”¨åŒä¸€å€‹encodingã€‚
    """
    all_cols = df.columns
    label_enc_cols = []

    if id_col not in all_cols:
        raise ValueError(f"'{id_col}' ä¸å­˜åœ¨æ–¼df.columnsï¼Œç„¡æ³•ä½œç‚ºä¸»éµ")

    # Collect target columns
    aircraft_cols = [c for c in all_cols if c.endswith("_aircraft_code")]
    label_enc_cols += aircraft_cols

    flightnum_cols = [c for c in all_cols if c.endswith("_flightNumber")]
    label_enc_cols += flightnum_cols

    airport_cols = [c for c in all_cols if "_arrivalTo_airport_" in c or "_departureFrom_airport_" in c]
    label_enc_cols += airport_cols

    carrier_cols = [c for c in all_cols if c.endswith("_marketingCarrier_code") or c.endswith("_operatingCarrier_code")]
    label_enc_cols += carrier_cols

    if "searchRoute" in all_cols:
        label_enc_cols.append("searchRoute")

    # é‚„åŸæ¨¡å¼
    if transform_config:
        label_encoders = transform_config["label_encoders"]
        df_result = df.select([id_col])
        for key, enc in label_encoders.items():
            cols = enc["columns"]
            mapping_df = pl.DataFrame({
                "value": enc["values"],
                "rank_id": enc["codes"]
            })
            for col in cols:
                df_col = (
                    df.select([id_col, col])
                    .with_columns(pl.col(col).cast(pl.Utf8))
                    .join(mapping_df.rename({"value": col}), on=col, how="left")
                    .with_columns(
                        pl.col("rank_id").fill_null(-1).cast(pl.Int32).alias(col)
                    )
                    .drop("rank_id")
                )
                df_result = df_result.join(df_col, on=id_col, how="left")
        print("âœ… å·²å®Œæˆé‚„åŸæ¨¡å¼ Label Encoding (ä½¿ç”¨transform_config)")
                # âœ… å¦‚æœæŒ‡å®šè¼¸å‡ºç›®éŒ„ï¼Œå„²å­˜é‚„åŸå¾Œçš„df
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            restored_path = os.path.join(output_dir, "10_df_restored_features.parquet")
            df_result.write_parquet(restored_path)
            print(f"âœ… å·²å„²å­˜é‚„åŸå¾Œç‰¹å¾µ: {restored_path}")
            
        return df_result, transform_config

    # æ–°è¨“ç·´æ¨¡å¼
    label_encoders = {}
    df_encoded = df.select([id_col])

    def encode_shared(cols: list, encoder_name: str):
        """
        å°å¤šå€‹æ¬„ä½å…±ç”¨åŒä¸€å€‹encoding
        """
        mapping_df = (
            df.select(cols)
            .melt()
            .select(pl.col("value").unique())
            .with_columns(
                (pl.col("value").rank("dense") - 1).fill_null(-1).cast(pl.Int32).alias("rank_id")
            )
            .sort("rank_id")
        )
        label_encoders[encoder_name] = {
            "columns": cols,
            "values": mapping_df["value"].to_list(),
            "codes": mapping_df["rank_id"].to_list()
        }
        for col in cols:
            encoded = (
                df.select([id_col, col])
                .with_columns(pl.col(col).cast(pl.Utf8))
                .join(mapping_df.rename({"value": col}), on=col, how="left")
                .with_columns(
                    pl.col("rank_id").fill_null(-1).cast(pl.Int32).alias(col)
                )
                .drop("rank_id")
            )
            nonlocal df_encoded
            df_encoded = df_encoded.join(encoded, on=id_col, how="left")

    def encode_individual(col: str):
        """
        å°å–®å€‹æ¬„ä½encoding
        """
        mapping_df = (
            df.select(pl.col(col))
            .unique()
            .with_columns(
                (pl.col(col).rank("dense") - 1).fill_null(-1).cast(pl.Int32).alias("rank_id")
            )
            .sort("rank_id")
        )
        label_encoders[col] = {
            "columns": [col],
            "values": mapping_df[col].to_list(),
            "codes": mapping_df["rank_id"].to_list()
        }
        encoded = (
            df.select([id_col, col])
            .with_columns(pl.col(col).cast(pl.Utf8))
            .join(mapping_df, on=col, how="left")
            .with_columns(
                pl.col("rank_id").fill_null(-1).cast(pl.Int32).alias(col)
            )
            .drop("rank_id")
        )
        nonlocal df_encoded
        df_encoded = df_encoded.join(encoded, on=id_col, how="left")

    # å…ˆå°å…±ç”¨æ¬„ä½åšencoding
    if airport_cols:
        encode_shared(airport_cols, "airport_cols")
    if carrier_cols:
        encode_shared(carrier_cols, "carrier_cols")
    if aircraft_cols:
        encode_shared(aircraft_cols, "aircraft_cols")
    if flightnum_cols:
        encode_shared(flightnum_cols, "flightnum_cols")

    # å†å°å…¶ä»–æ¬„ä½åšencoding
    other_cols = []
    if "searchRoute" in all_cols:
        other_cols.append("searchRoute")


    for c in other_cols:
        encode_individual(c)
    # è¼¸å‡ºtransform_config
    config = {
        "label_encoders": label_encoders
    }

    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        # å„²å­˜config
        config_path = os.path.join(output_dir, "transform_config_rank.pkl")
        with open(config_path, "wb") as f:
            pickle.dump(config, f)
        print(f"âœ… å·²å„²å­˜ transform_config: {config_path}")
        # å„²å­˜encoded df
        encoded_path = os.path.join(output_dir, "10_df_encoded_features.parquet")
        df_encoded.write_parquet(encoded_path)
        print(f"âœ… å·²å„²å­˜ç·¨ç¢¼å¾Œç‰¹å¾µ: {encoded_path}")

    print("âœ… æ–°è¨“ç·´Label Encodingå®Œæˆ (ä¿ç•™Idï¼Œå…±ç”¨airport/carrierç·¨ç¢¼)")
    return df_encoded, config





def clean_fill_and_cast_columns(
    df: pl.DataFrame,
    test: bool = False
) -> pl.DataFrame:
    """
    æ¸…ç†è³‡æ–™ï¼š
    - å°‡æ‰€æœ‰ç©ºå­—ä¸²è¦–ç‚ºnullï¼Œå†å¡«å…¥'missing'
    - æ•¸å€¼æ¬„å¡«0
    - Booleanæ¬„è½‰0/1
    - å¦‚æœ test=Trueï¼Œduration_cols å…¨éƒ¨è½‰æˆå­—ä¸²ä¸¦å¡«'missing'
    """

    # æ‰¾å­—ä¸²æ¬„
    str_cols = [c for c in df.columns if df[c].dtype in (pl.Utf8, pl.String)]
    # æ‰¾æ•¸å€¼æ¬„
    numeric_cols = [c for c in df.columns if df[c].dtype.is_numeric()]
    # æ‰¾å¸ƒæ—æ¬„
    bool_cols = [c for c in df.columns if df[c].dtype == pl.Boolean]

    print(f"âœ… å…±æ‰¾åˆ° {len(str_cols)} å€‹å­—ä¸²æ¬„ä½")
    print(f"âœ… å…±æ‰¾åˆ° {len(numeric_cols)} å€‹æ•¸å€¼æ¬„ä½")
    print(f"âœ… å…±æ‰¾åˆ° {len(bool_cols)} å€‹å¸ƒæ—æ¬„ä½")

    # æŠŠç©ºå­—ä¸²è®Šnull
    string_exprs = [
        pl.when(pl.col(c).str.strip_chars() == "")
          .then(None)
          .otherwise(pl.col(c))
          .alias(c)
        for c in str_cols
    ]
    df = df.with_columns(string_exprs)

    # å¡«è£œç¼ºå¤±
    df = df.with_columns(
        [pl.col(c).fill_null("missing") for c in str_cols] +
        [pl.col(c).fill_null(0) for c in numeric_cols]
    )

    # å¸ƒæ—è½‰0/1
    df = df.with_columns([
        pl.col(c).cast(pl.Int8).alias(c) for c in bool_cols
    ])

    # âœ… å¦‚æœ test=Trueï¼Œè™•ç† duration_cols
    if test:
        duration_cols = [
            "legs0_duration",
            "legs1_duration",
            "legs0_segments0_duration",
            "legs0_segments1_duration",
            "legs0_segments2_duration",
            "legs0_segments3_duration",
            "legs1_segments0_duration",
            "legs1_segments1_duration",
            "legs1_segments2_duration",
            "legs1_segments3_duration"
        ]
        duration_cols_exist = [c for c in duration_cols if c in df.columns]
        if duration_cols_exist:
            duration_exprs = [
                (
                    pl.col(c)
                    .cast(pl.Utf8)
                    .fill_null("missing")
                    .alias(c)
                )
                for c in duration_cols_exist
            ]
            df = df.with_columns(duration_exprs)
            print(f"âœ… test=True: å·²å°‡ {len(duration_cols_exist)} å€‹durationæ¬„ä½è½‰strä¸¦å¡«'missing'")

    print("âœ… å·²å®Œæˆç©ºå­—ä¸²è™•ç†ã€ç¼ºå¤±è£œå€¼ã€å¸ƒæ—è½‰0/1")
    return df



import polars as pl
import os
import glob
import re

def merge_original_with_extra_features(
    base_parquet_path: str,
    extra_features_dir: str,
    id_col: str = "Id"
) -> pl.DataFrame:
    """
    æŠŠåŸå§‹Parquetå’ŒæŒ‡å®šè³‡æ–™å¤¾ä¸­æ‰€æœ‰Parquetæª”æ¡ˆä¾æ“šIdåˆä½µã€‚
    å¦‚æœfeatureåç¨±é‡è¤‡ï¼Œä»¥æ–°æª”æ¡ˆçš„å€¼è¦†è“‹ã€‚

    åƒæ•¸:
    - base_parquet_path: åŸå§‹è³‡æ–™è·¯å¾‘ï¼Œä¾‹å¦‚ "data/train.parquet"
    - extra_features_dir: é¡å¤–ç‰¹å¾µè³‡æ–™å¤¾è·¯å¾‘ï¼Œä¾‹å¦‚ "data/extra_features/train/"
    - id_col: ä¸»éµæ¬„ä½ï¼Œé è¨­ "Id"

    å›å‚³:
    - åˆä½µå¾Œçš„DataFrame
    """

    print(f"âœ… è®€å–åŸå§‹è³‡æ–™: {base_parquet_path}")
    df = pl.read_parquet(base_parquet_path)

    if id_col not in df.columns:
        raise ValueError(f"'{id_col}' æ¬„ä½ä¸å­˜åœ¨æ–¼åŸå§‹è³‡æ–™ï¼")
    
    original_cols = set(df.columns)
    bool_cols = [c for c in df.columns if df[c].dtype == pl.Boolean]

    # å¸ƒæ—è½‰0/1
    df = df.with_columns([
        pl.col(c).cast(pl.Int8).alias(c) for c in bool_cols
    ])
    # æœå°‹æ‰€æœ‰ parquet
    pattern = os.path.join(extra_features_dir, "**/*.parquet")
    extra_files = glob.glob(pattern, recursive=True)

    def extract_number(file_path):
        base = os.path.basename(file_path)
        m = re.match(r"(\d+)_", base)
        return int(m.group(1)) if m else float("inf")

    extra_files = sorted(extra_files, key=extract_number)


    if not extra_files:
        print(f"âš ï¸ æ‰¾ä¸åˆ°ä»»ä½•Parquetæª”æ–¼ {extra_features_dir}")
        return df

    print(f"âœ… å…±æ‰¾åˆ° {len(extra_files)} å€‹ Parquet è¦åˆä½µ")

    # é€ä¸€åˆä½µ
    for i, file_path in enumerate(extra_files):
        print(f"ğŸ”¹ åˆä½µç¬¬ {i+1}/{len(extra_files)} å€‹: {file_path}")

        df_extra = pl.read_parquet(file_path)

        if id_col not in df_extra.columns:
            raise ValueError(f"'{id_col}' æ¬„ä½ä¸å­˜åœ¨æ–¼ {file_path}")

        # è‹¥df_extraæœ‰å’Œdfé‡è¤‡çš„æ¬„ï¼Œå…ˆç§»é™¤dfè£¡çš„
        overlap_cols = set(df.columns) & set(df_extra.columns) - {id_col}
        if overlap_cols:
            print(f"âš ï¸ {len(overlap_cols)} å€‹ç‰¹å¾µå°‡è¢«æ–°æª”æ¡ˆè¦†è“‹: {list(overlap_cols)}")
            df = df.drop(overlap_cols)

        df = df.join(df_extra, on=id_col, how="left")

    merged_cols = set(df.columns)
    added_cols = merged_cols - original_cols

    print("âœ… å·²å®Œæˆæ‰€æœ‰æª”æ¡ˆåˆä½µ")
    print(f"âœ… å…±æ–°å¢ {len(added_cols)} å€‹æ–°ç‰¹å¾µ")
    if added_cols:
        print(f"ğŸ”¹ æ–°å¢æ¬„ä½: {sorted(added_cols)}")

    return df

import pickle
import os
import polars as pl
from typing import Optional
import json

def enrich_flight_view_features(
    df: pl.DataFrame,
    output_dir: Optional[str] = None,
    output_filename: str = "11_flight_view_features.parquet",
    transform_config: Optional[dict] = None
) -> tuple[pl.DataFrame, dict]:
    def make_leg_segment_keys(leg_prefix):
        keys = []
        for i in range(4):
            key_name = f"{leg_prefix}_segments{i}_key"
            dep = pl.col(f"{leg_prefix}_segments{i}_departureFrom_airport_iata").fill_null("missing")
            arr = pl.col(f"{leg_prefix}_segments{i}_arrivalTo_airport_iata").fill_null("missing")
            keys.append((dep + "-" + arr).alias(key_name))
        return keys

    df = df.with_columns(make_leg_segment_keys("legs0") + make_leg_segment_keys("legs1"))

    all_segments = [f"legs0_segments{i}_key" for i in range(4)] + [f"legs1_segments{i}_key" for i in range(4)]

    if transform_config is None:
        segment_counts = (
            df.melt(id_vars=[], value_vars=all_segments)
            .filter(pl.col("value") != "missing-missing")
            .group_by("value")
            .agg(pl.count().alias("segment_view_count"))
        )
        segment_counts_dict = segment_counts.to_dict(as_series=False)
    else:
        segment_counts = pl.DataFrame(transform_config["segment_counts"])

    for seg_col in all_segments:
        df = df.join(
            segment_counts,
            left_on=seg_col,
            right_on="value",
            how="left"
        ).with_columns(
            pl.col("segment_view_count").fill_null(0).alias(f"{seg_col}_view_count")
        ).drop("segment_view_count")

    def make_leg_full_key(leg_prefix):
        seg_keys = [f"{leg_prefix}_segments{i}_key" for i in range(4)]
        return pl.concat_str([pl.col(k) for k in seg_keys], separator="|").alias(f"{leg_prefix}_key")

    df = df.with_columns([
        make_leg_full_key("legs0"),
        make_leg_full_key("legs1"),
        (
            pl.concat_str([
                pl.concat_str([pl.col(f"legs0_segments{i}_key") for i in range(4)], separator="|"),
                pl.lit("||"),
                pl.concat_str([pl.col(f"legs1_segments{i}_key") for i in range(4)], separator="|"),
            ], separator="")
        ).alias("all_key")
    ])

    if transform_config is None:
        leg0_counts = df.group_by("legs0_key").agg(pl.count().alias("leg0_flight_view_count"))
        leg1_counts = df.group_by("legs1_key").agg(pl.count().alias("leg1_flight_view_count"))
        all_counts = df.group_by("all_key").agg(pl.count().alias("all_flight_view_count"))
        leg0_counts_dict = leg0_counts.to_dict(as_series=False)
        leg1_counts_dict = leg1_counts.to_dict(as_series=False)
        all_counts_dict = all_counts.to_dict(as_series=False)
    else:
        leg0_counts = pl.DataFrame(transform_config["leg0_counts"])
        leg1_counts = pl.DataFrame(transform_config["leg1_counts"])
        all_counts = pl.DataFrame(transform_config["all_counts"])

    df = df.join(leg0_counts, on="legs0_key", how="left")
    df = df.join(leg1_counts, on="legs1_key", how="left")
    df = df.join(all_counts, on="all_key", how="left")

    ranker_stats = df.group_by("ranker_id").agg([
        pl.max("leg0_flight_view_count").alias("leg0_view_max"),
        pl.max("leg1_flight_view_count").alias("leg1_view_max"),
        pl.max("all_flight_view_count").alias("all_view_max"),
    ])

    df = df.join(ranker_stats, on="ranker_id", how="left")

    df = df.with_columns([
        (pl.col("leg0_flight_view_count") / (pl.col("leg0_view_max") + 1e-5)).alias("leg0_view_norm"),
        (pl.col("leg1_flight_view_count") / (pl.col("leg1_view_max") + 1e-5)).alias("leg1_view_norm"),
        (pl.col("all_flight_view_count") / (pl.col("all_view_max") + 1e-5)).alias("all_view_norm"),
    ])

    ranker_stats_mean = df.group_by("ranker_id").agg([
        pl.mean("leg0_flight_view_count").alias("leg0_view_mean"),
        pl.mean("leg1_flight_view_count").alias("leg1_view_mean"),
        pl.mean("all_flight_view_count").alias("all_view_mean"),
    ])

    df = df.join(ranker_stats_mean, on="ranker_id", how="left")

    df = df.with_columns([
        (pl.col("leg0_flight_view_count") - pl.col("leg0_view_mean")).alias("leg0_view_diff_mean"),
        (pl.col("leg1_flight_view_count") - pl.col("leg1_view_mean")).alias("leg1_view_diff_mean"),
        (pl.col("all_flight_view_count") - pl.col("all_view_mean")).alias("all_view_diff_mean"),
    ])
    
    rank_features = [
        "leg0_flight_view_count",
        "leg1_flight_view_count",
        "all_flight_view_count",
    ] + [f"legs0_segments{i}_key_view_count" for i in range(4)] + [f"legs1_segments{i}_key_view_count" for i in range(4)]

    rank_exprs = []
    for col in rank_features:
        rank_exprs.append(
            pl.col(col).rank(method="dense").over("ranker_id").alias(f"{col}_rank")
        )

    df = df.with_columns(rank_exprs)

    output_config = None
    if transform_config is None:
        output_config = {
            "segment_counts": segment_counts_dict,
            "leg0_counts": leg0_counts_dict,
            "leg1_counts": leg1_counts_dict,
            "all_counts": all_counts_dict
        }

    # æœ€å¾Œè¦ drop çš„ columns
    columns_to_drop = (
        [
            "leg0_view_max", "leg1_view_max", "all_view_max",
            "leg0_view_mean", "leg1_view_mean", "all_view_mean",
            "legs0_key", "legs1_key", "all_key"
        ]
        + [f"legs0_segments{i}_key" for i in range(4)]
        + [f"legs1_segments{i}_key" for i in range(4)]
        + [
            f"{leg}_segments{i}_{x}_airport_iata"
            for leg in ["legs0", "legs1"]
            for i in range(4)
            for x in ["departureFrom", "arrivalTo"]
        ]
    )
    df = df.drop(columns_to_drop)

    # ä¿ç•™ Id èˆ‡æ‰€æœ‰æ–°æ¬„ä½
    keep_cols = ["Id"] + [col for col in df.columns if col != "Id"]
    df = df.select(keep_cols)

    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        df.write_parquet(os.path.join(output_dir, output_filename))
        print(f"âœ… å·²å„²å­˜flight viewç‰¹å¾µ: {os.path.join(output_dir, output_filename)}")
        if transform_config is None and output_config is not None:
            config_path = os.path.join(output_dir, "transform_flight_view_key_config.pkl")
            with open(config_path, "wb") as f:
                pickle.dump(output_config, f)
            print(f"âœ… å·²å„²å­˜ transform_config: {config_path}")

    return df, output_config


import polars as pl
import os
import pickle
from typing import Optional, Tuple, Dict

def build_company_loo_features(
    df: pl.DataFrame,
    output_dir: Optional[str] = None,
    transform_dict: Optional[Dict] = None
) -> Tuple[pl.DataFrame, Optional[Dict]]:
    """
    å»ºç«‹å…¬å¸ LOO aggregation ç‰¹å¾µï¼š
    - æ‰€æœ‰ mean ç‰¹å¾µï¼šselected==1 ä¸”æ’é™¤åŒ ranker_id
    - mode ç‰¹å¾µï¼šselected==1ï¼Œä¸åš LOO
    - å‡ºç¾æ¬¡æ•¸ï¼šæ‰€æœ‰ç´€éŒ„ï¼Œä¸åš LOO
    - ç•¶ companyID æœªå‡ºç¾ï¼Œä½¿ç”¨å…¨é«”å‡å€¼ fallback
    """
    save_transform = transform_dict
    target_col = "selected"
    company_col = "companyID"
    ranker_col = "ranker_id"

    df = df.with_columns([
        pl.col(target_col).cast(pl.Int8)
    ])

    # Durationè½‰åˆ†é˜
    duration_cols = ["legs0_duration", "legs1_duration"]
    duration_exprs = [
        pl.when(pl.col(c).is_in([None, "missing"]))
        .then(None)
        .otherwise(
            pl.col(c).str.extract(r"^(\d+):", 1).cast(pl.Int64) * 60 +
            pl.col(c).str.extract(r":(\d+):", 1).cast(pl.Int64)
        )
        .alias(c)
        for c in duration_cols
    ]
    df = df.with_columns(duration_exprs)

    # æ™‚é–“ç‰¹å¾µ
    time_cols = ["legs0_departureAt", "legs0_arrivalAt", "legs1_departureAt", "legs1_arrivalAt"]
    time_exprs = []
    for col in time_cols:
        cleaned_col = (
            pl.when(pl.col(col).is_in(["missing", None, ""]))
            .then(None)
            .otherwise(pl.col(col))
        )
        dt = cleaned_col.str.to_datetime(strict=False)
        h = dt.dt.hour()
        time_exprs.append(
            h.fill_null(-1).alias(f"{col}_hour")
        )
    df = df.with_columns(time_exprs)

    # Cabin class
    if "legs0_segments0_cabinClass" in df.columns:
        df = df.with_columns(
            pl.col("legs0_segments0_cabinClass").cast(pl.Float32).alias("cabin_class")
        )
    else:
        df = df.with_columns(
            pl.lit(None).alias("cabin_class")
        )

    # Transfer
    df = df.with_columns([
        pl.sum_horizontal([
            ((pl.col(f"legs0_segments{i}_departureFrom_airport_iata").is_not_null()) &
             (pl.col(f"legs0_segments{i}_departureFrom_airport_iata") != "missing")).cast(pl.Int8)
            for i in range(1, 4)
        ]).alias("legs0_num_transfers"),
        pl.sum_horizontal([
            ((pl.col(f"legs1_segments{i}_departureFrom_airport_iata").is_not_null()) &
             (pl.col(f"legs1_segments{i}_departureFrom_airport_iata") != "missing")).cast(pl.Int8)
            for i in range(1, 4)
        ]).alias("legs1_num_transfers")
    ])
    df = df.with_columns([
        (pl.col("legs0_num_transfers") + pl.col("legs1_num_transfers")).fill_null(0).cast(pl.Int64).alias("total_num_transfers"),
        ((pl.col("legs0_num_transfers") + pl.col("legs1_num_transfers")) > 0).cast(pl.Int8).alias("has_transfer")
    ])

    agg_cols = [
        "totalPrice", "taxes",
        "legs0_duration", "legs1_duration",
        "cabin_class",
        "total_num_transfers"
    ] + [f"{c}_hour" for c in time_cols]
    
    stats_cols = [company_col] + [f"{c}_mean" for c in agg_cols] + ["selected_count"]

    if transform_dict is None:

        # selected==1 mean
        all_stats = (
            df.filter(pl.col(target_col) == 1)
            .group_by(company_col)
            .agg([
                *(pl.mean(c).alias(f"{c}_mean") for c in agg_cols),
                pl.count().alias("selected_count")
            ])
        )

        # å…¨é«”å‡å€¼ fallback
        global_mean_row = (
            df.filter(pl.col(target_col) == 1)
            .select([
                pl.lit(-1).alias(company_col),
                *(pl.mean(c).alias(f"{c}_mean") for c in agg_cols),
                pl.count().alias("selected_count")
            ])
        )
        # ç¢ºä¿æ¬„ä½åç¨±å’Œé †åºä¸€è‡´
        global_mean_row = global_mean_row.select(all_stats.columns)

        # å¼·åˆ¶åŒé †åº


        # mode
        def mode_table(col, alias, dtype):
            m = (
                df.filter(pl.col(target_col)==1)
                .group_by(company_col)
                .agg([
                    pl.col(col)
                    .value_counts(sort=True)
                    .struct.field(col)
                    .first()
                    .cast(dtype)
                    .alias(alias)
                ])
            )
            global_mode = (
                df.filter(pl.col(target_col)==1)
                .select([
                    pl.col(col)
                    .value_counts(sort=True)
                    .struct.field(col)
                    .first()
                    .cast(dtype)
                    .alias(alias)
                ])
                .with_columns(pl.lit(-1).alias(company_col))
            )
            return m, global_mode

        cabin_mode, global_cabin = mode_table("cabin_class","mode_cabin_class",pl.Int32)
        transfer_mode, global_transfer = mode_table("has_transfer","mode_has_transfer",pl.Int8)
        transfer_num_mode, global_transfer_num = mode_table("total_num_transfers","mode_transfer_num",pl.Int64)

        ranker_stats = (
            df.filter(pl.col(target_col) == 1)
            .group_by([ranker_col, company_col])
            .agg([
                *(pl.sum(c).alias(f"{c}_sum") for c in agg_cols),
                pl.count().alias("count")
            ])
        )
        total_counts = (
            df.group_by(company_col)
            .agg(pl.count().alias("total_occurrences"))
        )

        transform_dict = {
            "all_stats": all_stats.to_dict(as_series=False),
            "global_mean": global_mean_row.to_dict(as_series=False),
            "cabin_mode": cabin_mode.to_dict(as_series=False),
            "global_cabin": global_cabin.to_dict(as_series=False),
            "transfer_mode": transfer_mode.to_dict(as_series=False),
            "global_transfer": global_transfer.to_dict(as_series=False),
            "transfer_num_mode": transfer_num_mode.to_dict(as_series=False),
            "global_transfer_num": global_transfer_num.to_dict(as_series=False),
            "ranker_stats": ranker_stats.to_dict(as_series=False),
            "total_counts": total_counts.to_dict(as_series=False)
        }
    else:
        all_stats = pl.DataFrame(transform_dict["all_stats"])
        global_mean_row = pl.DataFrame(transform_dict["global_mean"])
        global_mean_row = global_mean_row.select(stats_cols)

        cabin_mode = pl.DataFrame(transform_dict["cabin_mode"])
        global_cabin = pl.DataFrame(transform_dict["global_cabin"])
        cabin_mode = cabin_mode.select([company_col, "mode_cabin_class"])
        global_cabin = global_cabin.select([company_col, "mode_cabin_class"])

        transfer_mode = pl.DataFrame(transform_dict["transfer_mode"])
        global_transfer = pl.DataFrame(transform_dict["global_transfer"])
        transfer_mode = transfer_mode.select([company_col, "mode_has_transfer"])
        global_transfer = global_transfer.select([company_col, "mode_has_transfer"])

        transfer_num_mode = pl.DataFrame(transform_dict["transfer_num_mode"])
        global_transfer_num = pl.DataFrame(transform_dict["global_transfer_num"])
        transfer_num_mode = transfer_num_mode.select([company_col, "mode_transfer_num"])
        global_transfer_num = global_transfer_num.select([company_col, "mode_transfer_num"])

        ranker_stats = pl.DataFrame(transform_dict["ranker_stats"])
        total_counts = pl.DataFrame(transform_dict["total_counts"])

        # concat global fallback row
        all_stats = pl.concat([all_stats, global_mean_row])
        cabin_mode = pl.concat([cabin_mode, global_cabin])
        transfer_mode = pl.concat([transfer_mode, global_transfer])
        transfer_num_mode = pl.concat([transfer_num_mode, global_transfer_num])


    # join
    df = df.join(all_stats, on=company_col, how="left")
    df = df.join(cabin_mode, on=company_col, how="left")
    df = df.join(transfer_mode, on=company_col, how="left")
    df = df.join(transfer_num_mode, on=company_col, how="left")
    df = df.join(ranker_stats, on=[ranker_col, company_col], how="left")
    df = df.join(total_counts, on=company_col, how="left")

    # LOO mean
    new_cols = []
    for c in agg_cols:
        new_cols.append(
                pl.col(f"{c}_mean").alias(f"{company_col}_loo_mean_{c}")
        )
    new_cols.append(
            pl.col("selected_count").alias(f"{company_col}_loo_selected_count")
        )

    # mode å’Œ occurrenceä¸è®Š
    new_cols.append(pl.col("mode_cabin_class").alias(f"{company_col}_mode_cabin_class"))
    new_cols.append(pl.col("mode_has_transfer").alias(f"{company_col}_mode_has_transfer"))
    new_cols.append(pl.col("mode_transfer_num").alias(f"{company_col}_mode_transfer_num"))
    new_cols.append(pl.col("total_occurrences").alias(f"{company_col}_total_occurrences"))

    df = df.with_columns(new_cols)

    kept_cols = ["Id"] + [c.meta.output_name() for c in new_cols]
    df = df.select(kept_cols)

    # å„²å­˜
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        df_path = os.path.join(output_dir, "12_companyID_features.parquet")
        df.write_parquet(df_path)
        print(f"âœ… å·²å„²å­˜ transform_dict: {df_path}")
        if save_transform is None:
            config_path = os.path.join(output_dir, "transform_dict_companyID.pkl")
            with open(config_path, "wb") as f:
                pickle.dump(transform_dict, f)
            print(f"âœ… å·²å„²å­˜ transform_dict: {config_path}")

    return df, transform_dict

import os
import pickle
import polars as pl
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans

def build_cluster_transform_dict(
    transform_path: str,
    output_path: str,
    k: int = 3
):
    """
    å¾ transform_dict_companyID.pkl è®€å–ï¼Œé€²è¡Œ KMeans clusteringï¼Œç”Ÿæˆ cluster summaryï¼Œä¸¦å­˜æˆ transform_dict_cluster.pkl
    """
    # === è®€å– transform_dict ===
    with open(transform_path, "rb") as f:
        transform_dict = pickle.load(f)

    all_stats = pl.DataFrame(transform_dict["all_stats"])
    cabin_mode = pl.DataFrame(transform_dict["cabin_mode"])
    transfer_mode = pl.DataFrame(transform_dict["transfer_mode"])
    transfer_num_mode = pl.DataFrame(transform_dict["transfer_num_mode"])
    total_counts = pl.DataFrame(transform_dict["total_counts"])
    global_mean = pl.DataFrame(transform_dict["global_mean"])

    # åŠ ä¸Š fallback row
    all_stats = pl.concat([all_stats, global_mean])

    # åˆä½µæˆ summary
    company_summary = (
        all_stats
        .join(cabin_mode, on="companyID", how="left")
        .join(transfer_mode, on="companyID", how="left")
        .join(transfer_num_mode, on="companyID", how="left")
        .join(total_counts, on="companyID", how="left")
    )

    # Null -> 0
    company_summary_filled = company_summary.fill_null(0)

    # Numeric columns
    exclude_cols = {"companyID"}
    numeric_cols = [
        c for c, dtype in company_summary_filled.schema.items()
        if c not in exclude_cols and dtype in pl.NUMERIC_DTYPES
    ]

    # Scaling
    X_np = company_summary_filled.select(numeric_cols).to_numpy()
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X_np)

    # Clustering
    kmeans = KMeans(n_clusters=k, random_state=42, n_init="auto")
    labels = kmeans.fit_predict(X_scaled)

    company_summary_clustered = company_summary_filled.with_columns(
        pl.Series("cluster_label", labels)
    )

    # èšåˆæ¯å€‹ cluster
    base_features = [c for c in company_summary_clustered.columns if c not in {"companyID", "cluster_label"}]
    agg_exprs = [
        pl.col(feat).mean().alias(f"{feat}_mean") for feat in base_features
    ]

    cluster_summary = (
        company_summary_clustered
        .group_by("cluster_label")
        .agg(agg_exprs)
        .sort("cluster_label")
    )

    # Merge cluster summary back
    company_with_cluster_features = (
        company_summary_clustered
        .join(cluster_summary, on="cluster_label", how="left")
    )

    final_df = company_with_cluster_features.select(
        ["companyID", "cluster_label"] + cluster_summary.drop("cluster_label").columns
    )

    # å„²å­˜ transform_dict
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    cluster_transform_dict = {
        "cluster_summary": final_df.to_dict(as_series=False)
    }

    with open(output_path, "wb") as f:
        pickle.dump(cluster_transform_dict, f)

    print(f"âœ… å·²å„²å­˜ transform_dict: {output_path}")
    
    return cluster_transform_dict


import os
import pickle
import polars as pl

import os
import pickle
import polars as pl
import os
import pickle
import polars as pl

def add_cluster_features_and_save(
    df: pl.DataFrame,
    transform_dict_path: str,
    output_dir: str
):
    """
    æ ¹æ“š transform_dict (cluster) å°æ‡‰ companyID åŠ å…¥ cluster featuresï¼Œè‹¥æ‰¾ä¸åˆ°å‰‡ç”¨ fallback (-1)ã€‚
    
    Args:
        df: è¦åŠ ä¸Š features çš„ DataFrame (å¿…é ˆæœ‰ "companyID")
        transform_dict_path: transform_dict_cluster.pkl è·¯å¾‘
        output_dir: è¼¸å‡ºç›®éŒ„
    """
    if "companyID" not in df.columns:
        raise ValueError("âŒ DataFrame ç¼ºå°‘ 'companyID' æ¬„ä½")
    
    # è®€å– transform_dict
    with open(transform_dict_path, "rb") as f:
        transform_dict = pickle.load(f)
    
    cluster_features_df = pl.DataFrame(transform_dict["cluster_summary"])
    
    # fallback row
    fallback_row = cluster_features_df.filter(pl.col("companyID") == -1)
    
    # å° df å…ˆ left join cluster_features
    df_joined = df.join(
        cluster_features_df,
        on="companyID",
        how="left"
    )

    # å†ä¾åºå°æ¯å€‹æ¬„ coalesce() fallback
    feature_cols = [c for c in cluster_features_df.columns if c != "companyID"]
    for col in feature_cols:
        fallback_value = fallback_row[col].to_numpy()[0] if fallback_row.height else None
        df_joined = df_joined.with_columns(
            pl.col(col).fill_null(fallback_value)
        )

    # è¼¸å‡º
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, "13_cluster_features.parquet")
    df_joined.write_parquet(output_path)
    
    print(f"âœ… å·²å„²å­˜ cluster features: {output_path}")
    print(df_joined.head())
    
    return df_joined


import polars as pl
def drop_constant_numeric_columns(
    df: pl.DataFrame,
    threshold: float = 1.0
) -> pl.DataFrame:
    """
    æª¢æŸ¥æ‰€æœ‰ numeric æ¬„ä½ï¼Œè‹¥æœ€å¸¸è¦‹å€¼ä½”æ¯” >= thresholdï¼Œå‰‡ç§»é™¤è©²æ¬„ã€‚
    """
    if not (0 < threshold <= 1.0):
        raise ValueError("threshold å¿…é ˆåœ¨ (0, 1]")

    numeric_cols = [c for c, dtype in df.schema.items() if dtype in pl.NUMERIC_DTYPES]
    if not numeric_cols:
        print("âš ï¸ DataFrame ä¸­æ²’æœ‰ numeric æ¬„ä½ï¼Œç„¡éœ€æª¢æŸ¥")
        return df

    columns_to_drop = []

    for col in numeric_cols:
        vc_df = (
            df.select(pl.col(col).value_counts())
            .unnest(col)
            .sort(by=["count", col], descending=[True, False])
        )
        most_common_count = vc_df[0, "count"]
        ratio = most_common_count / df.height
        if ratio >= threshold:
            print(f"ğŸš® æ¬„ä½ {col} æœ€å¸¸è¦‹å€¼ä½”æ¯” {ratio:.4f} >= {threshold}, å°‡ç§»é™¤")
            columns_to_drop.append(col)

    if columns_to_drop:
        df = df.drop(columns_to_drop)
        print(f"âœ… å·²ç§»é™¤ {len(columns_to_drop)} å€‹å¹¾ä¹ç„¡è®ŠåŒ–çš„ numeric æ¬„ä½: {columns_to_drop}")
    else:
        print("âœ… æ‰€æœ‰ numeric æ¬„ä½è®Šç•°æ€§è¶³å¤ ï¼Œç„¡éœ€åˆªé™¤")
    print(f"ç›®å‰æœ‰{len(df.columns)}")
    return df
