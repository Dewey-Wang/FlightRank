import polars as pl
import os
import json
import gc



def split_data_by_group_size(
    df: pl.DataFrame,
    bins: list,
    labels: list,
    output_dir: str,
    label_features: dict = None,
    unused_label_features: dict = None
):
    os.makedirs(output_dir, exist_ok=True)

    df = df.with_row_count("global_row_nr")

    # å…ˆè¨ˆç®—æ¯å€‹ranker_idæœ‰å¹¾ç­†
    group_counts = (
        df.group_by("ranker_id")
          .agg(pl.count().alias("n_rows"))
          .filter(pl.col("n_rows") >= bins[0])
    )

    bins_fixed = bins.copy()
    if bins_fixed[-1] is None:
        max_value = group_counts["n_rows"].max()
        bins_fixed[-1] = int(max_value) + 1

    if len(labels) != len(bins_fixed) - 1:
        raise ValueError(f"bins={bins_fixed} æœ‰ {len(bins_fixed)-1}å€‹å€é–“ï¼Œä½†labelsæ•¸={len(labels)}")

    # è¨ˆç®—æ¯å€‹ ranker_id çš„ group label
    cond = (
        pl.when((pl.col("n_rows") >= bins_fixed[0]) & (pl.col("n_rows") < bins_fixed[1]))
          .then(pl.lit(labels[0]))
    )
    for i in range(1, len(labels)):
        cond = cond.when(
            (pl.col("n_rows") >= bins_fixed[i]) & (pl.col("n_rows") < bins_fixed[i+1])
        ).then(pl.lit(labels[i]))
    cond = cond.otherwise(pl.lit("unknown"))

    group_counts = group_counts.with_columns(cond.alias("group_category"))

    written_files = {}
    effective_label_features = {}

    # æ¯å€‹labelä¸€æ¬¡è™•ç†
    for lbl in labels:
        # å…ˆæŠ“è©²labelçš„ranker_idæ¸…å–®
        rankers_this_label = (
            group_counts
            .filter(pl.col("group_category") == lbl)
            .get_column("ranker_id")
            .to_list()
        )
        if not rankers_this_label:
            print(f"âš ï¸ {lbl} æ²’æœ‰è³‡æ–™ï¼Œè·³é")
            continue

        # ç›´æ¥ filter
        subset = df.filter(pl.col("ranker_id").is_in(rankers_this_label))

        if subset.is_empty():
            print(f"âš ï¸ {lbl} æ²’æœ‰è³‡æ–™ï¼Œè·³é")
            continue

        # è¨ˆç®—ç‰¹å¾µ
        if label_features is None:
            feats = [c for c in df.columns]
        else:
            feats = label_features.get(lbl, [])

        if unused_label_features:
            if isinstance(unused_label_features, dict):
                unused_feats = set(unused_label_features.get(lbl, []))
            else:
                unused_feats = set(unused_label_features)
            feats = [f for f in feats if f not in unused_feats]

        effective_label_features[lbl] = feats

        base_cols = ["selected", "ranker_id", "global_row_nr"]
        all_cols = list(dict.fromkeys(feats + base_cols))
        subset = subset.select([c for c in all_cols if c in subset.columns])

        mem_mb = subset.estimated_size() / (1024 * 1024)
        print(f"âœ… {lbl}: {subset.height} rows, approx {mem_mb:.2f} MB")

        out_path = os.path.join(output_dir, f"{lbl}.parquet")
        subset.write_parquet(out_path)
        print(f"ğŸ’¾ å·²å¯«å…¥ {out_path}")
        written_files[lbl] = out_path

        del subset
        gc.collect()

    # çµ±è¨ˆ
    summary = (
        group_counts.group_by("group_category")
        .agg([
            pl.count().alias("n_groups"),
            pl.col("n_rows").sum().alias("total_rows"),
            pl.col("n_rows").mean().alias("avg_rows_per_group")
        ])
        .sort("group_category")
    )

    print("âœ… åˆ†ç¾¤çµ±è¨ˆï¼š")
    print(summary)

    grouping_config = {
        "bins": bins_fixed,
        "labels": labels
    }
    config_path = os.path.join(output_dir, "grouping_config.json")
    with open(config_path, "w") as f:
        json.dump(grouping_config, f, indent=4)
    print(f"âœ… å·²å„²å­˜ grouping_config: {config_path}")

    return {
        "summary": summary,
        "written_files": written_files,
        "used_label_features": effective_label_features
    }



def split_data_by_group_size_test(
    df: pl.DataFrame,
    bins: list = [3, 21, 162, None],
    labels: list = ["small", "medium", "large"]
):
    """
    æ ¹æ“šranker_idåˆ†ç¾¤ï¼Œä¿ç•™åŸå§‹å…¨åŸŸå”¯ä¸€é †åº global_row_nrã€‚
    """
    df = df.with_row_count("global_row_nr")

    # ä¸å† filter
    group_counts = (
        df.group_by("ranker_id")
          .agg(pl.count().alias("n_rows"))
          .filter(pl.col("n_rows") >= bins[0])
    )

    bins_fixed = bins.copy()
    if bins_fixed[-1] is None:
        max_value = group_counts["n_rows"].max()
        bins_fixed[-1] = int(max_value) + 1

    if len(labels) != len(bins_fixed) - 1:
        raise ValueError(f"bins={bins_fixed} æœ‰ {len(bins_fixed)-1}å€‹å€é–“ï¼Œä½†labelsæ•¸={len(labels)}")

    cond = (
        pl.when((pl.col("n_rows") >= bins_fixed[0]) & (pl.col("n_rows") < bins_fixed[1]))
          .then(pl.lit(labels[0]))
    )
    for i in range(1, len(labels)):
        cond = cond.when(
            (pl.col("n_rows") >= bins_fixed[i]) & (pl.col("n_rows") < bins_fixed[i+1])
        ).then(pl.lit(labels[i]))
    cond = cond.otherwise(pl.lit("unknown"))

    group_counts = group_counts.with_columns([
        cond.alias("group_category")
    ])

    df = df.join(group_counts, on="ranker_id", how="left")

    split_data = {}
    for lbl in labels:
        subset = df.filter(pl.col("group_category") == lbl)
        mem_mb = subset.estimated_size() / (1024*1024)
        print(f"âœ… {lbl}: {subset.height} rows, approx {mem_mb:.2f} MB")
        split_data[lbl] = subset

    summary = (
        group_counts.group_by("group_category")
        .agg([
            pl.count().alias("n_groups"),
            pl.col("n_rows").sum().alias("total_rows"),
            pl.col("n_rows").mean().alias("avg_rows_per_group")
        ])
        .sort("group_category")
    )

    print("âœ… åˆ†ç¾¤çµ±è¨ˆï¼š")
    print(summary)

    return {
        "data_all": df,
        "split_data": split_data,
        "summary": summary
    }



import numpy as np
import xgboost as xgb

def prepare_train_val_split(
    result: dict,
    split_label: str,
    feature_cols: list,
    train_fraction: float = 0.8,
    random_seed: int = 42
):
    """
    æ ¹æ“šåˆ†ç¾¤çµæœåˆ‡ train/val ä¸¦è½‰æˆ numpyï¼Œä¿è­‰ä¸æ´©æ¼ã€‚
    
    åƒæ•¸:
    - result: split_data_by_group_size() çš„è¼¸å‡º
    - split_label: è¦ä½¿ç”¨çš„åˆ†ç¾¤åç¨± ("small"/"medium"/"large")
    - feature_cols: feature æ¬„ä½
    - train_fraction: trainæ¯”ä¾‹ (é è¨­0.8)
    - random_seed: éš¨æ©Ÿç¨®å­
    
    å›å‚³:
    dict {
        X_train_np, y_train_np, groups_train_np,
        X_val_np, y_val_np, groups_val_np,
        group_sizes_train, group_sizes_val,
        val_ids: valç”¨çš„ranker_id set
    }
    """
    # å–å¾—åˆ†ç¾¤ DataFrame
    df = result["split_data"][split_label]

    if feature_cols is None:
        feature_cols = [c for c in df.columns if c not in ("selected", "ranker_id", "global_row_nr", "frequentFlyer")]

    # æŠ“æ‰€æœ‰ ranker_id
    unique_rankers = df.select("ranker_id").unique().to_series().to_list()
    np.random.seed(random_seed)
    np.random.shuffle(unique_rankers)

    # åˆ‡ train/val
    n_train = int(len(unique_rankers) * train_fraction)
    train_ids = set(unique_rankers[:n_train])
    val_ids = set(unique_rankers[n_train:])

    # æ¨™è¨˜ train
    is_train = df.select(pl.col("ranker_id").is_in(list(train_ids)).alias("is_train"))
    exclude_cols = {"selected", "ranker_id", "global_row_nr", "frequentFlyer", "Id"}
    feature_cols = [f for f in feature_cols if f not in exclude_cols]

    # å»ºç«‹ feature/label/groups DataFrame
    X_all = df.select(feature_cols)
    y_all = df.select("selected")
    groups_all = df.select("ranker_id")

    # åŠ å…¥mask
    X_with_mask = X_all.with_columns(is_train)
    y_with_mask = y_all.with_columns(is_train)
    groups_with_mask = groups_all.with_columns(is_train)

    # åˆ†train/val
    X_train_df = X_with_mask.filter(pl.col("is_train"))
    X_val_df = X_with_mask.filter(~pl.col("is_train"))
    y_train_df = y_with_mask.filter(pl.col("is_train"))
    y_val_df = y_with_mask.filter(~pl.col("is_train"))
    groups_train_df = groups_with_mask.filter(pl.col("is_train"))
    groups_val_df = groups_with_mask.filter(~pl.col("is_train"))

    # è½‰ numpy
    X_train_np = X_train_df.drop("is_train").to_numpy()
    y_train_np = y_train_df.drop("is_train").to_numpy().flatten()
    groups_train_np = groups_train_df.drop("is_train").to_numpy().flatten()

    X_val_np = X_val_df.drop("is_train").to_numpy()
    y_val_np = y_val_df.drop("is_train").to_numpy().flatten()
    groups_val_np = groups_val_df.drop("is_train").to_numpy().flatten()

    # è¨ˆç®— group size
    group_sizes_train = (
        pl.DataFrame({"ranker_id": groups_train_np})
        .group_by("ranker_id", maintain_order=True)
        .agg(pl.len())['len']
        .to_numpy()
    )
    group_sizes_val = (
        pl.DataFrame({"ranker_id": groups_val_np})
        .group_by("ranker_id", maintain_order=True)
        .agg(pl.len())['len']
        .to_numpy()
    )

    # è¼¸å‡º
    print(f"âœ… Train: {X_train_np.shape[0]} rows, {len(np.unique(groups_train_np))} groups")
    print(f"âœ… Val: {X_val_np.shape[0]} rows, {len(np.unique(groups_val_np))} groups")
    dtrain = xgb.DMatrix(
        X_train_np,
        label= y_train_np,
        feature_names=feature_cols
    )
    dtrain.set_group(group_sizes_train)

    dval = xgb.DMatrix(
        X_val_np,
        label=y_val_np,
        feature_names=feature_cols

    )
    dval.set_group(group_sizes_val)

    return {
        "dtrain":dtrain,
        "dval":dval,
        "X_train_np": X_train_np,
        "y_train_np": y_train_np,
        "groups_train_np": groups_train_np,
        "X_val_np": X_val_np,
        "y_val_np": y_val_np,
        "groups_val_np": groups_val_np,
        "group_sizes_train": group_sizes_train,
        "group_sizes_val": group_sizes_val,
        "val_ids": val_ids
    }
    
def prepare_prediction(
    result: dict,
    split_label: str,
    feature_cols: list,
):
    """
    ç›´æ¥æº–å‚™æ‰€æœ‰è³‡æ–™ç”¨æ–¼é æ¸¬ï¼Œä¸åˆ†train/valã€‚
    """
    
    df = result["split_data"][split_label]
   # åªä¿ç•™å­˜åœ¨çš„æ¬„ï¼Œä¸”æŒ‰dfé †åº
    final_features = [c for c in df.columns if c in feature_cols]

    # å…¨éƒ¨ rows
    groups_np = df.select("ranker_id").to_numpy().flatten()
    rows_np = df.select("global_row_nr").to_numpy().flatten()

    group_sizes = (
        pl.DataFrame({"ranker_id": groups_np})
        .group_by("ranker_id", maintain_order=True)
        .agg(pl.len())['len']
        .to_numpy()
    )

    print(f"âœ… {split_label} åˆ†çµ„å¤§å°: {len(group_sizes)}")
    return {
        "groups_np": groups_np,
        "global_row_np": rows_np,
        "group_sizes": group_sizes,
        "feature_cols": final_features,
    }
    
    
import os
import xgboost as xgb
import pandas as pd

def export_xgb_feature_importance(
    model_dir: str,
    label: str,
) -> pd.DataFrame:
    """
    è¼¸å‡ºXGBoostæ¨¡å‹çš„ç‰¹å¾µé‡è¦æ€§(ä¸ç”¨feature_names)ï¼Œ
    è‡ªå‹•å¾æ¨¡å‹æå–åç¨±ä¸¦å„²å­˜CSVã€‚

    åƒæ•¸:
    - model_dir: æ¨¡å‹è³‡æ–™å¤¾
    - label: åˆ†ç¾¤åç¨±
    - top_n: è¦é¡¯ç¤ºå‰å¹¾å (é è¨­50)

    å›å‚³:
    - æ’åºå¾Œçš„DataFrame
    """
    # è®€å–æ¨¡å‹
    model_path = os.path.join(model_dir, f"xgb_ranker_{label}.bin")
    booster = xgb.Booster()
    booster.load_model(model_path)
    print(f"âœ… å·²è®€å–æ¨¡å‹ {model_path}")

    # å–é‡è¦æ€§
    importance_types = ["weight", "gain", "cover"]
    importance_all = {}

    all_features = set()
    for imp_type in importance_types:
        imp_raw = booster.get_score(importance_type=imp_type)
        all_features.update(imp_raw.keys())
        sorted_imp = sorted(imp_raw.items(), key=lambda x: x[1], reverse=True)
        importance_all[imp_type] = sorted_imp
    all_features = booster.feature_names

    # å»ºå®Œæ•´ç‰¹å¾µè¡¨
    df_all = pd.DataFrame({"feature": sorted(all_features)})

    # å€‹åˆ¥DataFrame
    df_weight = pd.DataFrame(importance_all["weight"], columns=["feature", "weight"])
    df_weight["weight_rank_pos"] = df_weight.index

    df_gain = pd.DataFrame(importance_all["gain"], columns=["feature", "gain"])
    df_gain["gain_rank_pos"] = df_gain.index

    df_cover = pd.DataFrame(importance_all["cover"], columns=["feature", "cover"])
    df_cover["cover_rank_pos"] = df_cover.index

    # åˆä½µ
    df_merged = (
        df_all
        .merge(df_weight, on="feature", how="left")
        .merge(df_gain, on="feature", how="left")
        .merge(df_cover, on="feature", how="left")
    )

    # è£œrank_pos
    df_merged["weight_rank_pos"] = df_merged["weight_rank_pos"].fillna(9999)
    df_merged["gain_rank_pos"] = df_merged["gain_rank_pos"].fillna(9999)
    df_merged["cover_rank_pos"] = df_merged["cover_rank_pos"].fillna(9999)

    # æœ€å°æ’å
    df_merged["min_rank"] = df_merged[["weight_rank_pos", "gain_rank_pos", "cover_rank_pos"]].min(axis=1)

    # æ’åº
    df_merged_sorted = df_merged.sort_values("min_rank")

    # å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
    model_importance_dir = os.path.join(model_dir, "model_importance")
    os.makedirs(model_importance_dir, exist_ok=True)

    # è¼¸å‡ºCSV
    csv_path = os.path.join(model_importance_dir, f"feature_importance_{label}_all_features.csv")
    df_merged_sorted.to_csv(csv_path, index=False)
    print(f"âœ… å·²è¼¸å‡ºæ‰€æœ‰ç‰¹å¾µé‡è¦æ€§åˆ° {csv_path}")

    return df_merged_sorted


import os
import pandas as pd

def export_common_feature_ranks(
    labels: list,
    model_importance_dir: str
) -> pd.DataFrame:
    """
    è®€å–å¤šå€‹æ¨¡å‹çš„ç‰¹å¾µé‡è¦æ€§CSVï¼Œæ‰¾å…±åŒç‰¹å¾µï¼Œ
    è¨ˆç®—å¹³å‡rank/æœ€å¤§rank/æœ€å°rankï¼Œè¼¸å‡ºå½™ç¸½è¡¨ã€‚

    åƒæ•¸:
    - labels: ["small", "medium", ...]
    - model_importance_dir: æ¨¡å‹é‡è¦æ€§CSVç›®éŒ„
    - top_n: è¦é¡¯ç¤ºçš„Top N (é è¨­50)

    å›å‚³:
    - æ’åºå¾ŒDataFrame
    """
    dfs = {}
    for label in labels:
        csv_path = os.path.join(model_importance_dir, f"feature_importance_{label}_all_features.csv")
        df = pd.read_csv(csv_path)
        dfs[label] = df[["feature", "min_rank"]].copy()
        dfs[label].rename(columns={"min_rank": f"min_rank_{label}"}, inplace=True)
        print(f"âœ… å·²è®€ {label}: {len(df)} rows")

    # ä¾åº inner merge
    df_merged = dfs[labels[0]]
    for label in labels[1:]:
        df_merged = df_merged.merge(dfs[label], on="feature", how="inner")

    print(f"\nğŸ¯ æ‰€æœ‰æ¨¡å‹å…±åŒå‡ºç¾ç‰¹å¾µ: {len(df_merged)}")

    # è¨ˆç®—ç¶œåˆæ’å
    rank_cols = [f"min_rank_{label}" for label in labels]
    df_merged["avg_rank"] = df_merged[rank_cols].mean(axis=1)
    df_merged["max_rank"] = df_merged[rank_cols].max(axis=1)
    df_merged["min_rank_overall"] = df_merged[rank_cols].min(axis=1)

    # æ’åº
    df_sorted = df_merged.sort_values("avg_rank")


    # è¼¸å‡ºCSV
    csv_common = os.path.join(model_importance_dir, "common_features_with_ranks.csv")
    df_sorted.to_csv(csv_common, index=False)
    print(f"\nâœ… å·²è¼¸å‡ºå…±åŒç‰¹å¾µåˆ° {csv_common}")

    return df_sorted


import os
import pandas as pd

def load_used_features_from_importance_csv(
    model_importance_dir: str,
    labels: list
) -> dict:
    """
    è®€å–æ¨¡å‹feature importance CSVï¼Œ
    å›å‚³æ¯å€‹labelå¯¦éš›æœ‰ç”¨åˆ°çš„ç‰¹å¾µlistã€‚

    åƒæ•¸:
    - model_importance_dir: å­˜æ”¾CSVçš„è³‡æ–™å¤¾
    - labels: label list

    å›å‚³:
    - dict {label: features list}
    """
    if labels is None:
        csv_path = os.path.join(model_importance_dir, f"feature_importance.csv")
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {csv_path}")

        df = pd.read_csv(csv_path)

        # æŒ‘å‡ºmin_rank < 9999ä»£è¡¨æœ‰ç”¨åˆ°çš„feature
        result = df["feature"].tolist()

        print(f"âœ… å…± {len(result)} å€‹ç”¨åˆ°çš„ç‰¹å¾µ")
    else:
        result = {}

        for label in labels:
            csv_path = os.path.join(model_importance_dir, f"feature_importance_{label}_all_features.csv")
            if not os.path.exists(csv_path):
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {csv_path}")

            df = pd.read_csv(csv_path)

            used_features = df["feature"].tolist()

            print(f"âœ… {label}: å…± {len(used_features)} å€‹ç”¨åˆ°çš„ç‰¹å¾µ")

            result[label] = used_features

    return result

import os
import pandas as pd

def load_label_features(
    model_dir: str,
    split_labels: list,
    top_n: int | list | None = None
):
    """
    å¾æ¨¡å‹è³‡æ–™å¤¾è®€å–æ¯å€‹åˆ†ç¾¤çš„ç‰¹å¾µé‡è¦æ€§æª”æ¡ˆï¼Œå›å‚³æ¯å€‹labelçš„features listã€‚

    åƒæ•¸:
    - model_dir: æ¨¡å‹è³‡æ–™å¤¾
    - split_labels: åˆ†ç¾¤åç¨±list
    - top_n: int æˆ– listï¼Œå¦‚æœæŒ‡å®šï¼Œå–å‰Nå€‹ç‰¹å¾µï¼ˆlistæ™‚æ¯å€‹labelä¸åŒï¼‰ï¼›å¦å‰‡ç”¨ç¬¬ä¸€å€‹min_rank=9999ç‚ºæ­¢

    å›å‚³:
    - dict(label -> features list)
    """
    label_features = {}

    # æª¢æŸ¥ top_n
    if isinstance(top_n, list):
        if len(top_n) != len(split_labels):
            raise ValueError(f"å¦‚æœ top_n æ˜¯ listï¼Œé•·åº¦å¿…é ˆèˆ‡ split_labels ä¸€è‡´ (got {len(top_n)} vs {len(split_labels)})")

    for i, label in enumerate(split_labels):
        model_importance_dir = os.path.join(model_dir, "model_importance")
        csv_path = os.path.join(model_importance_dir, f"feature_importance_{label}_all_features.csv")

        df = pd.read_csv(csv_path)

        # æ±ºå®šé€™å€‹labelè¦ç”¨å¹¾å€‹
        n = None
        if isinstance(top_n, int):
            n = top_n
        elif isinstance(top_n, list):
            n = top_n[i]

        if n is not None:
            feats = df.iloc[:n]["feature"].tolist()
            print(f"\nâœ… {label}: å–å‰ {n} å€‹ç‰¹å¾µ")
        else:
            idx_first_unused = df[df["min_rank"] == 9999].index.min()
            feats = df.iloc[:idx_first_unused]["feature"].tolist()
            print(f"\nâœ… {label}: ç¬¬ {idx_first_unused} åå¾Œéƒ½æ˜¯å®Œå…¨æœªä½¿ç”¨çš„ç‰¹å¾µ")
            print("âœ… ç¬¬ä¸€å€‹æœªä½¿ç”¨ç‰¹å¾µï¼š")
            print(df.iloc[idx_first_unused])

        label_features[label] = feats

    # å°å‡ºæ‰€æœ‰åˆ†ç¾¤featuresæ•¸é‡
    for label in split_labels:
        print(f"{label}: {len(label_features[label])} features")

    return label_features



import os
import polars as pl

def export_submission_parquets(
    test_filled_with_preds: pl.DataFrame,
    output_dir: str,
    raw_filename: str = "raw_submission.parquet",
    ranked_filename: str = "rank_submission.parquet"
):
    """
    æ ¹æ“š test_filled_with_preds è¼¸å‡ºå…©å€‹ parquet:
    1. åŸå§‹åˆ†æ•¸ (selected)
    2. rank æ’åº (selected)
    """
    # æª¢æŸ¥ç›®éŒ„
    os.makedirs(output_dir, exist_ok=True)

    # Subset + __index_level_0__
    subset_df = (
        test_filled_with_preds
        .select(["Id", "ranker_id", "selected"])
        .with_columns(
            pl.col("Id").alias("__index_level_0__")
        )
        .with_columns([
            pl.col("Id").cast(pl.Int64),
            pl.col("ranker_id").cast(pl.Utf8),
            pl.col("selected").cast(pl.Float64),
            pl.col("__index_level_0__").cast(pl.Int64)
        ])
    )

    # å„²å­˜åŸå§‹ parquet
    raw_path = os.path.join(output_dir, raw_filename)
    subset_df.write_parquet(raw_path)
    print(f"âœ… å·²å„²å­˜åŸå§‹ submission: {raw_path}")
    print(subset_df)

    # Rank æ’å
    ranked_df = subset_df.with_columns(
        pl.col("selected")
          .rank(method="ordinal", descending=True)
          .over("ranker_id")
          .alias("selected")
    )

    # å„²å­˜æ’å parquet
    ranked_path = os.path.join(output_dir, ranked_filename)
    ranked_df.write_parquet(ranked_path)
    print(f"âœ… å·²å„²å­˜rank submission: {ranked_path}")
    print(ranked_df)