import polars as pl


def split_data_by_group_size_test(
    df: pl.DataFrame,
    bins: list = [3, 21, 162, None],
    labels: list = ["small", "medium", "large"]
):
    """
    æ ¹æ“šranker_idåˆ†ç¾¤ï¼Œä¿ç•™åŸå§‹å…¨åŸŸå”¯ä¸€é †åº global_row_nrã€‚
    """
    df = df.with_row_count("global_row_nr")

    # ä¸å† filter
    group_counts = (
        df.group_by("ranker_id")
          .agg(pl.count().alias("n_rows"))
          .filter(pl.col("n_rows") >= bins[0])
    )

    bins_fixed = bins.copy()
    if bins_fixed[-1] is None:
        max_value = group_counts["n_rows"].max()
        bins_fixed[-1] = int(max_value) + 1

    if len(labels) != len(bins_fixed) - 1:
        raise ValueError(f"bins={bins_fixed} æœ‰ {len(bins_fixed)-1}å€‹å€é–“ï¼Œä½†labelsæ•¸={len(labels)}")

    cond = (
        pl.when((pl.col("n_rows") >= bins_fixed[0]) & (pl.col("n_rows") < bins_fixed[1]))
          .then(pl.lit(labels[0]))
    )
    for i in range(1, len(labels)):
        cond = cond.when(
            (pl.col("n_rows") >= bins_fixed[i]) & (pl.col("n_rows") < bins_fixed[i+1])
        ).then(pl.lit(labels[i]))
    cond = cond.otherwise(pl.lit("unknown"))

    group_counts = group_counts.with_columns([
        cond.alias("group_category")
    ])

    df = df.join(group_counts, on="ranker_id", how="left")

    split_data = {}
    for lbl in labels:
        subset = df.filter(pl.col("group_category") == lbl)
        mem_mb = subset.estimated_size() / (1024*1024)
        print(f"âœ… {lbl}: {subset.height} rows, approx {mem_mb:.2f} MB")
        split_data[lbl] = subset

    summary = (
        group_counts.group_by("group_category")
        .agg([
            pl.count().alias("n_groups"),
            pl.col("n_rows").sum().alias("total_rows"),
            pl.col("n_rows").mean().alias("avg_rows_per_group")
        ])
        .sort("group_category")
    )

    print("âœ… åˆ†ç¾¤çµ±è¨ˆï¼š")
    print(summary)

    return {
        "data_all": df,
        "split_data": split_data,
        "summary": summary
    }



import numpy as np
import xgboost as xgb

def prepare_train_val_split(
    result: dict,
    split_label: str,
    feature_cols: list,
    train_fraction: float = 0.8,
    random_seed: int = 42
):
    """
    æ ¹æ“šåˆ†ç¾¤çµæœåˆ‡ train/val ä¸¦è½‰æˆ numpyï¼Œä¿è­‰ä¸æ´©æ¼ã€‚
    
    åƒæ•¸:
    - result: split_data_by_group_size() çš„è¼¸å‡º
    - split_label: è¦ä½¿ç”¨çš„åˆ†ç¾¤åç¨± ("small"/"medium"/"large")
    - feature_cols: feature æ¬„ä½
    - train_fraction: trainæ¯”ä¾‹ (é è¨­0.8)
    - random_seed: éš¨æ©Ÿç¨®å­
    
    å›å‚³:
    dict {
        X_train_np, y_train_np, groups_train_np,
        X_val_np, y_val_np, groups_val_np,
        group_sizes_train, group_sizes_val,
        val_ids: valç”¨çš„ranker_id set
    }
    """
    # å–å¾—åˆ†ç¾¤ DataFrame
    df = result["split_data"][split_label]

    if feature_cols is None:
        feature_cols = [c for c in df.columns if c not in ("selected", "ranker_id", "global_row_nr", "frequentFlyer")]

    # æŠ“æ‰€æœ‰ ranker_id
    unique_rankers = df.select("ranker_id").unique().to_series().to_list()
    np.random.seed(random_seed)
    np.random.shuffle(unique_rankers)

    # åˆ‡ train/val
    n_train = int(len(unique_rankers) * train_fraction)
    train_ids = set(unique_rankers[:n_train])
    val_ids = set(unique_rankers[n_train:])

    # æ¨™è¨˜ train
    is_train = df.select(pl.col("ranker_id").is_in(list(train_ids)).alias("is_train"))
    exclude_cols = {"selected", "ranker_id", "global_row_nr", "frequentFlyer", "Id"}
    feature_cols = [f for f in feature_cols if f not in exclude_cols]

    # å»ºç«‹ feature/label/groups DataFrame
    X_all = df.select(feature_cols)
    y_all = df.select("selected")
    groups_all = df.select("ranker_id")

    # åŠ å…¥mask
    X_with_mask = X_all.with_columns(is_train)
    y_with_mask = y_all.with_columns(is_train)
    groups_with_mask = groups_all.with_columns(is_train)

    # åˆ†train/val
    X_train_df = X_with_mask.filter(pl.col("is_train"))
    X_val_df = X_with_mask.filter(~pl.col("is_train"))
    y_train_df = y_with_mask.filter(pl.col("is_train"))
    y_val_df = y_with_mask.filter(~pl.col("is_train"))
    groups_train_df = groups_with_mask.filter(pl.col("is_train"))
    groups_val_df = groups_with_mask.filter(~pl.col("is_train"))

    # è½‰ numpy
    X_train_np = X_train_df.drop("is_train").to_numpy()
    y_train_np = y_train_df.drop("is_train").to_numpy().flatten()
    groups_train_np = groups_train_df.drop("is_train").to_numpy().flatten()

    X_val_np = X_val_df.drop("is_train").to_numpy()
    y_val_np = y_val_df.drop("is_train").to_numpy().flatten()
    groups_val_np = groups_val_df.drop("is_train").to_numpy().flatten()

    # è¨ˆç®— group size
    group_sizes_train = (
        pl.DataFrame({"ranker_id": groups_train_np})
        .group_by("ranker_id", maintain_order=True)
        .agg(pl.len())['len']
        .to_numpy()
    )
    group_sizes_val = (
        pl.DataFrame({"ranker_id": groups_val_np})
        .group_by("ranker_id", maintain_order=True)
        .agg(pl.len())['len']
        .to_numpy()
    )

    # è¼¸å‡º
    print(f"âœ… Train: {X_train_np.shape[0]} rows, {len(np.unique(groups_train_np))} groups")
    print(f"âœ… Val: {X_val_np.shape[0]} rows, {len(np.unique(groups_val_np))} groups")
    dtrain = xgb.DMatrix(
        X_train_np,
        label= y_train_np,
        feature_names=feature_cols
    )
    dtrain.set_group(group_sizes_train)

    dval = xgb.DMatrix(
        X_val_np,
        label=y_val_np,
        feature_names=feature_cols

    )
    dval.set_group(group_sizes_val)

    return {
        "dtrain":dtrain,
        "dval":dval,
        "X_train_np": X_train_np,
        "y_train_np": y_train_np,
        "groups_train_np": groups_train_np,
        "X_val_np": X_val_np,
        "y_val_np": y_val_np,
        "groups_val_np": groups_val_np,
        "group_sizes_train": group_sizes_train,
        "group_sizes_val": group_sizes_val,
        "val_ids": val_ids
    }
    
def prepare_prediction(
    result: dict,
    split_label: str,
    feature_cols: list,
):
    """
    ç›´æ¥æº–å‚™æ‰€æœ‰è³‡æ–™ç”¨æ–¼é æ¸¬ï¼Œä¸åˆ†train/valã€‚
    """
    
    df = result["split_data"][split_label]
   # åªä¿ç•™å­˜åœ¨çš„æ¬„ï¼Œä¸”æŒ‰dfé †åº
    final_features = [c for c in df.columns if c in feature_cols]

    # å…¨éƒ¨ rows
    groups_np = df.select("ranker_id").to_numpy().flatten()
    rows_np = df.select("global_row_nr").to_numpy().flatten()

    group_sizes = (
        pl.DataFrame({"ranker_id": groups_np})
        .group_by("ranker_id", maintain_order=True)
        .agg(pl.len())['len']
        .to_numpy()
    )

    print(f"âœ… {split_label} åˆ†çµ„å¤§å°: {len(group_sizes)}")
    return {
        "groups_np": groups_np,
        "global_row_np": rows_np,
        "group_sizes": group_sizes,
        "feature_cols": final_features,
    }
    
    
import os
import xgboost as xgb
import pandas as pd

def export_xgb_feature_importance(
    model_dir: str,
    label: str,
) -> pd.DataFrame:
    """
    è¼¸å‡ºXGBoostæ¨¡å‹çš„ç‰¹å¾µé‡è¦æ€§(ä¸ç”¨feature_names)ï¼Œ
    è‡ªå‹•å¾æ¨¡å‹æå–åç¨±ä¸¦å„²å­˜CSVã€‚

    åƒæ•¸:
    - model_dir: æ¨¡å‹è³‡æ–™å¤¾
    - label: åˆ†ç¾¤åç¨±
    - top_n: è¦é¡¯ç¤ºå‰å¹¾å (é è¨­50)

    å›å‚³:
    - æ’åºå¾Œçš„DataFrame
    """
    # è®€å–æ¨¡å‹
    model_path = os.path.join(model_dir, f"xgb_ranker_{label}.bin")
    booster = xgb.Booster()
    booster.load_model(model_path)
    print(f"âœ… å·²è®€å–æ¨¡å‹ {model_path}")

    # å–é‡è¦æ€§
    importance_types = ["weight", "gain", "cover"]
    importance_all = {}

    all_features = set()
    for imp_type in importance_types:
        imp_raw = booster.get_score(importance_type=imp_type)
        all_features.update(imp_raw.keys())
        sorted_imp = sorted(imp_raw.items(), key=lambda x: x[1], reverse=True)
        importance_all[imp_type] = sorted_imp
    all_features = booster.feature_names

    # å»ºå®Œæ•´ç‰¹å¾µè¡¨
    df_all = pd.DataFrame({"feature": sorted(all_features)})

    # å€‹åˆ¥DataFrame
    df_weight = pd.DataFrame(importance_all["weight"], columns=["feature", "weight"])
    df_weight["weight_rank_pos"] = df_weight.index

    df_gain = pd.DataFrame(importance_all["gain"], columns=["feature", "gain"])
    df_gain["gain_rank_pos"] = df_gain.index

    df_cover = pd.DataFrame(importance_all["cover"], columns=["feature", "cover"])
    df_cover["cover_rank_pos"] = df_cover.index

    # åˆä½µ
    df_merged = (
        df_all
        .merge(df_weight, on="feature", how="left")
        .merge(df_gain, on="feature", how="left")
        .merge(df_cover, on="feature", how="left")
    )

    # è£œrank_pos
    df_merged["weight_rank_pos"] = df_merged["weight_rank_pos"].fillna(9999)
    df_merged["gain_rank_pos"] = df_merged["gain_rank_pos"].fillna(9999)
    df_merged["cover_rank_pos"] = df_merged["cover_rank_pos"].fillna(9999)

    # æœ€å°æ’å
    df_merged["min_rank"] = df_merged[["weight_rank_pos", "gain_rank_pos", "cover_rank_pos"]].min(axis=1)

    # æ’åº
    df_merged_sorted = df_merged.sort_values("min_rank")

    # å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
    model_importance_dir = os.path.join(model_dir, "model_importance")
    os.makedirs(model_importance_dir, exist_ok=True)

    # è¼¸å‡ºCSV
    csv_path = os.path.join(model_importance_dir, f"feature_importance_{label}_all_features.csv")
    df_merged_sorted.to_csv(csv_path, index=False)
    print(f"âœ… å·²è¼¸å‡ºæ‰€æœ‰ç‰¹å¾µé‡è¦æ€§åˆ° {csv_path}")

    return df_merged_sorted


import os
import pandas as pd

def export_common_feature_ranks(
    labels: list,
    model_importance_dir: str
) -> pd.DataFrame:
    """
    è®€å–å¤šå€‹æ¨¡å‹çš„ç‰¹å¾µé‡è¦æ€§CSVï¼Œæ‰¾å…±åŒç‰¹å¾µï¼Œ
    è¨ˆç®—å¹³å‡rank/æœ€å¤§rank/æœ€å°rankï¼Œè¼¸å‡ºå½™ç¸½è¡¨ã€‚

    åƒæ•¸:
    - labels: ["small", "medium", ...]
    - model_importance_dir: æ¨¡å‹é‡è¦æ€§CSVç›®éŒ„
    - top_n: è¦é¡¯ç¤ºçš„Top N (é è¨­50)

    å›å‚³:
    - æ’åºå¾ŒDataFrame
    """
    dfs = {}
    for label in labels:
        csv_path = os.path.join(model_importance_dir, f"feature_importance_{label}_all_features.csv")
        df = pd.read_csv(csv_path)
        dfs[label] = df[["feature", "min_rank"]].copy()
        dfs[label].rename(columns={"min_rank": f"min_rank_{label}"}, inplace=True)
        print(f"âœ… å·²è®€ {label}: {len(df)} rows")

    # ä¾åº inner merge
    df_merged = dfs[labels[0]]
    for label in labels[1:]:
        df_merged = df_merged.merge(dfs[label], on="feature", how="inner")

    print(f"\nğŸ¯ æ‰€æœ‰æ¨¡å‹å…±åŒå‡ºç¾ç‰¹å¾µ: {len(df_merged)}")

    # è¨ˆç®—ç¶œåˆæ’å
    rank_cols = [f"min_rank_{label}" for label in labels]
    df_merged["avg_rank"] = df_merged[rank_cols].mean(axis=1)
    df_merged["max_rank"] = df_merged[rank_cols].max(axis=1)
    df_merged["min_rank_overall"] = df_merged[rank_cols].min(axis=1)

    # æ’åº
    df_sorted = df_merged.sort_values("avg_rank")


    # è¼¸å‡ºCSV
    csv_common = os.path.join(model_importance_dir, "common_features_with_ranks.csv")
    df_sorted.to_csv(csv_common, index=False)
    print(f"\nâœ… å·²è¼¸å‡ºå…±åŒç‰¹å¾µåˆ° {csv_common}")

    return df_sorted


import os
import pandas as pd

def load_used_features_from_importance_csv(
    model_importance_dir: str,
    labels: list
) -> dict:
    """
    è®€å–æ¨¡å‹feature importance CSVï¼Œ
    å›å‚³æ¯å€‹labelå¯¦éš›æœ‰ç”¨åˆ°çš„ç‰¹å¾µlistã€‚

    åƒæ•¸:
    - model_importance_dir: å­˜æ”¾CSVçš„è³‡æ–™å¤¾
    - labels: label list

    å›å‚³:
    - dict {label: features list}
    """
    if labels is None:
        csv_path = os.path.join(model_importance_dir, f"feature_importance.csv")
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {csv_path}")

        df = pd.read_csv(csv_path)

        # æŒ‘å‡ºmin_rank < 9999ä»£è¡¨æœ‰ç”¨åˆ°çš„feature
        result = df["feature"].tolist()

        print(f"âœ… å…± {len(result)} å€‹ç”¨åˆ°çš„ç‰¹å¾µ")
    else:
        result = {}

        for label in labels:
            csv_path = os.path.join(model_importance_dir, f"feature_importance_{label}_all_features.csv")
            if not os.path.exists(csv_path):
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {csv_path}")

            df = pd.read_csv(csv_path)

            # æŒ‘å‡ºmin_rank < 9999ä»£è¡¨æœ‰ç”¨åˆ°çš„feature
            used_features = df["feature"].tolist()

            print(f"âœ… {label}: å…± {len(used_features)} å€‹ç”¨åˆ°çš„ç‰¹å¾µ")

            result[label] = used_features

    return result
