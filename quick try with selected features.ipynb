{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bcca8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from lightgbm import LGBMRegressor\n",
    "from scipy.stats import pearsonr\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.base import clone\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.chdir(\"/Users/deweywang/Desktop/GitHub/FlightRank/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46537c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "786af087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ç¬¬ 78 åå¾Œéƒ½æ˜¯å®Œå…¨æœªä½¿ç”¨çš„ç‰¹å¾µ\n",
      "\n",
      "âœ… ç¬¬ä¸€å€‹æœªä½¿ç”¨ç‰¹å¾µï¼š\n",
      "feature            legs1_segments3_marketingCarrier_code\n",
      "weight                                               NaN\n",
      "weight_rank_pos                                   9999.0\n",
      "gain                                                 NaN\n",
      "gain_rank_pos                                     9999.0\n",
      "cover                                                NaN\n",
      "cover_rank_pos                                    9999.0\n",
      "min_rank                                          9999.0\n",
      "Name: 78, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "model_dir = \"model_output/all_features_xgb\"\n",
    "label = \"xlarge\"\n",
    "model_importance_dir = os.path.join(model_dir, \"model_importance\")\n",
    "csv_path = os.path.join(model_importance_dir, f\"feature_importance_{label}_all_features.csv\")\n",
    "\n",
    "# è®€å–\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# ç¬¬å¹¾åé–‹å§‹æ²’ç”¨é\n",
    "idx_first_unused = df[df[\"min_rank\"] == 9999].index.min()\n",
    "\n",
    "print(f\"\\nâœ… ç¬¬ {idx_first_unused} åå¾Œéƒ½æ˜¯å®Œå…¨æœªä½¿ç”¨çš„ç‰¹å¾µ\\n\")\n",
    "\n",
    "# çœ‹ç¬¬ä¸€å€‹\n",
    "print(\"âœ… ç¬¬ä¸€å€‹æœªä½¿ç”¨ç‰¹å¾µï¼š\")\n",
    "print(df.iloc[idx_first_unused])\n",
    "\n",
    "\n",
    "features = (df.iloc[:idx_first_unused]['feature'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55703332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def split_data_by_group_size_per_label_features(\n",
    "    df: pl.DataFrame,\n",
    "    bins: list,\n",
    "    labels: list,\n",
    "    label_features: dict = None\n",
    "):\n",
    "    \"\"\"\n",
    "    ä¸€æ¬¡åˆ‡åˆ†æ‰€æœ‰åˆ†ç¾¤ï¼Œæ¯å€‹åˆ†ç¾¤ç”¨å„è‡ªfeaturesã€‚\n",
    "    \n",
    "    df: polars.DataFrame\n",
    "    bins: åˆ†ç¾¤é‚Šç•Œ\n",
    "    labels: åˆ†ç¾¤åç¨±\n",
    "    label_features: dict(label -> feature list)ï¼Œæ¯å€‹labelè‡ªå·±çš„featureæ¬„ä½ã€‚\n",
    "                   å¦‚æœç‚ºNoneï¼Œå‰‡æ‰€æœ‰åˆ†ç¾¤éƒ½ç”¨dfçš„æ‰€æœ‰columnsã€‚\n",
    "    \"\"\"\n",
    "    df = df.with_row_count(\"global_row_nr\")\n",
    "\n",
    "    group_counts = (\n",
    "        df.group_by(\"ranker_id\")\n",
    "          .agg(pl.count().alias(\"n_rows\"))\n",
    "          .filter(pl.col(\"n_rows\") >= bins[0])\n",
    "    )\n",
    "\n",
    "    bins_fixed = bins.copy()\n",
    "    if bins_fixed[-1] is None:\n",
    "        max_value = group_counts[\"n_rows\"].max()\n",
    "        bins_fixed[-1] = int(max_value) + 1\n",
    "\n",
    "    if len(labels) != len(bins_fixed) - 1:\n",
    "        raise ValueError(f\"bins={bins_fixed} æœ‰ {len(bins_fixed)-1}å€‹å€é–“ï¼Œä½†labelsæ•¸={len(labels)}\")\n",
    "\n",
    "    cond = (\n",
    "        pl.when((pl.col(\"n_rows\") >= bins_fixed[0]) & (pl.col(\"n_rows\") < bins_fixed[1]))\n",
    "        .then(pl.lit(labels[0]))\n",
    "    )\n",
    "    for i in range(1, len(labels)):\n",
    "        cond = cond.when(\n",
    "            (pl.col(\"n_rows\") >= bins_fixed[i]) & (pl.col(\"n_rows\") < bins_fixed[i+1])\n",
    "        ).then(pl.lit(labels[i]))\n",
    "    cond = cond.otherwise(pl.lit(\"unknown\"))\n",
    "\n",
    "    group_counts = group_counts.with_columns([\n",
    "        cond.alias(\"group_category\")\n",
    "    ])\n",
    "\n",
    "    df = df.join(group_counts, on=\"ranker_id\", how=\"left\")\n",
    "\n",
    "    split_data = {}\n",
    "    for lbl in labels:\n",
    "        subset = df.filter(pl.col(\"group_category\") == lbl)\n",
    "\n",
    "        # å¦‚æœæ²’çµ¦ label_featuresï¼Œå°±ç”¨å…¨éƒ¨ columns\n",
    "        if label_features is None:\n",
    "            feats = [c for c in df.columns if c not in (\"group_category\")]\n",
    "        else:\n",
    "            feats = label_features.get(lbl, [])\n",
    "\n",
    "        base_cols = [\"selected\", \"ranker_id\", \"global_row_nr\", \"group_category\"]\n",
    "        all_cols = feats + base_cols\n",
    "        all_cols = list(dict.fromkeys(all_cols))  # å»é‡é˜²æ­¢é‡è¤‡\n",
    "\n",
    "        subset = subset.select([c for c in all_cols if c in subset.columns])\n",
    "\n",
    "        mem_mb = subset.estimated_size() / (1024 * 1024)\n",
    "        print(f\"âœ… {lbl}: {subset.height} rows, approx {mem_mb:.2f} MB\")\n",
    "        split_data[lbl] = subset\n",
    "\n",
    "    summary = (\n",
    "        group_counts.group_by(\"group_category\")\n",
    "        .agg([\n",
    "            pl.count().alias(\"n_groups\"),\n",
    "            pl.col(\"n_rows\").sum().alias(\"total_rows\"),\n",
    "            pl.col(\"n_rows\").mean().alias(\"avg_rows_per_group\")\n",
    "        ])\n",
    "        .sort(\"group_category\")\n",
    "    )\n",
    "\n",
    "    print(\"âœ… åˆ†ç¾¤çµ±è¨ˆï¼š\")\n",
    "    print(summary)\n",
    "\n",
    "    return {\n",
    "        \"data_all\": df,\n",
    "        \"split_data\": split_data,\n",
    "        \"summary\": summary\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ad0f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_label_features(\n",
    "    model_dir: str,\n",
    "    split_labels: list,\n",
    "    top_n: int = None\n",
    "):\n",
    "    \"\"\"\n",
    "    å¾æ¨¡å‹è³‡æ–™å¤¾è®€å–æ¯å€‹åˆ†ç¾¤çš„ç‰¹å¾µé‡è¦æ€§æª”æ¡ˆï¼Œå›å‚³æ¯å€‹labelçš„features listã€‚\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    - model_dir: æ¨¡å‹è³‡æ–™å¤¾\n",
    "    - split_labels: åˆ†ç¾¤åç¨±list\n",
    "    - top_n: å¦‚æœæŒ‡å®šï¼Œåªå–å‰Nå€‹ç‰¹å¾µï¼›å¦å‰‡ç”¨ç¬¬ä¸€å€‹min_rank=9999ç‚ºæ­¢\n",
    "\n",
    "    å›å‚³:\n",
    "    - dict(label -> features list)\n",
    "    \"\"\"\n",
    "    label_features = {}\n",
    "\n",
    "    for label in split_labels:\n",
    "        model_importance_dir = os.path.join(model_dir, \"model_importance\")\n",
    "        csv_path = os.path.join(model_importance_dir, f\"feature_importance_{label}_all_features.csv\")\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        if top_n is not None:\n",
    "            # ç›´æ¥å–å‰top_n\n",
    "            feats = df.iloc[:top_n][\"feature\"].tolist()\n",
    "            print(f\"\\nâœ… {label}: å–å‰ {top_n} å€‹ç‰¹å¾µ\")\n",
    "        else:\n",
    "            idx_first_unused = df[df[\"min_rank\"] == 9999].index.min()\n",
    "            feats = df.iloc[:idx_first_unused][\"feature\"].tolist()\n",
    "            print(f\"\\nâœ… {label}: ç¬¬ {idx_first_unused} åå¾Œéƒ½æ˜¯å®Œå…¨æœªä½¿ç”¨çš„ç‰¹å¾µ\")\n",
    "            print(\"âœ… ç¬¬ä¸€å€‹æœªä½¿ç”¨ç‰¹å¾µï¼š\")\n",
    "            print(df.iloc[idx_first_unused])\n",
    "\n",
    "        label_features[label] = feats\n",
    "\n",
    "    # å°å‡ºæ‰€æœ‰åˆ†ç¾¤featuresæ•¸é‡\n",
    "    for label in split_labels:\n",
    "        print(f\"{label}: {len(label_features[label])} features\")\n",
    "\n",
    "    return label_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7e74b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… small: å–å‰ 20 å€‹ç‰¹å¾µ\n",
      "\n",
      "âœ… medium: å–å‰ 20 å€‹ç‰¹å¾µ\n",
      "\n",
      "âœ… large: å–å‰ 20 å€‹ç‰¹å¾µ\n",
      "small: 20 features\n",
      "medium: 20 features\n",
      "large: 20 features\n",
      "âœ… small: 416866 rows, approx 66.84 MB\n",
      "âœ… medium: 3127131 rows, approx 480.52 MB\n",
      "âœ… large: 14595402 rows, approx 2340.18 MB\n",
      "âœ… åˆ†ç¾¤çµ±è¨ˆï¼š\n",
      "shape: (3, 4)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ group_category â”† n_groups â”† total_rows â”† avg_rows_per_group â”‚\n",
      "â”‚ ---            â”† ---      â”† ---        â”† ---                â”‚\n",
      "â”‚ str            â”† u32      â”† u32        â”† f64                â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ large          â”† 25590    â”† 14595402   â”† 570.355686         â”‚\n",
      "â”‚ medium         â”† 45114    â”† 3127131    â”† 69.316199          â”‚\n",
      "â”‚ small          â”† 31288    â”† 416866     â”† 13.323511          â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "model_dir = \"model_output/all_features_xgb\"\n",
    "\n",
    "# å…ˆæŠŠæ¯å€‹åˆ†ç¾¤éƒ½è®€ä¸€éfeature importance\n",
    "split_labels = [\"small\", \"medium\", \"large\"]\n",
    "\n",
    "label_features = load_label_features(\n",
    "    model_dir=model_dir,\n",
    "    split_labels=split_labels,\n",
    "    top_n=20\n",
    ")\n",
    "\n",
    "# è®€ train data\n",
    "train_filled = pl.read_parquet(\"data/train_filled.parquet\")\n",
    "\n",
    "# å‘¼å«æ–°ç‰ˆæœ¬åˆ†ç¾¤åˆ‡åˆ†\n",
    "result = split_data_by_group_size_per_label_features(\n",
    "    df=train_filled,\n",
    "    bins=[3, 27, 162,None],\n",
    "    labels=split_labels,\n",
    "    label_features=label_features\n",
    ")\n",
    "\n",
    "# æ¸…ç†è¨˜æ†¶é«”\n",
    "import gc\n",
    "del train_filled\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e79bc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model folder: model_output/selected_features_xgb/v2_top20/\n",
      "ğŸ’¾ å„²å­˜ small åˆ° model_output/selected_features_xgb/v2_top20/train_dataset/small.parquet\n",
      "ğŸ’¾ å„²å­˜ medium åˆ° model_output/selected_features_xgb/v2_top20/train_dataset/medium.parquet\n",
      "ğŸ’¾ å„²å­˜ large åˆ° model_output/selected_features_xgb/v2_top20/train_dataset/large.parquet\n",
      "âœ… split_data å·²æ¸…ç©ºï¼Œè¨˜æ†¶é«”æ‡‰å·²é‡‹æ”¾\n",
      "\n",
      "ğŸš€ è™•ç†åˆ†ç¾¤: small\n",
      "âœ… Train: 333058 rows, 25030 groups\n",
      "âœ… Val: 83808 rows, 6258 groups\n",
      "[0]\ttrain-ndcg@3:0.67297\tval-ndcg@3:0.66570\n",
      "[20]\ttrain-ndcg@3:0.77250\tval-ndcg@3:0.73017\n",
      "[40]\ttrain-ndcg@3:0.78823\tval-ndcg@3:0.73522\n",
      "[60]\ttrain-ndcg@3:0.79962\tval-ndcg@3:0.73904\n",
      "[80]\ttrain-ndcg@3:0.80932\tval-ndcg@3:0.74334\n",
      "[100]\ttrain-ndcg@3:0.81926\tval-ndcg@3:0.74608\n",
      "[120]\ttrain-ndcg@3:0.82810\tval-ndcg@3:0.74955\n",
      "[140]\ttrain-ndcg@3:0.83695\tval-ndcg@3:0.75290\n",
      "[160]\ttrain-ndcg@3:0.84280\tval-ndcg@3:0.75381\n",
      "[180]\ttrain-ndcg@3:0.85067\tval-ndcg@3:0.75518\n",
      "[200]\ttrain-ndcg@3:0.85743\tval-ndcg@3:0.75695\n",
      "[220]\ttrain-ndcg@3:0.86320\tval-ndcg@3:0.75742\n",
      "[240]\ttrain-ndcg@3:0.86858\tval-ndcg@3:0.75948\n",
      "[260]\ttrain-ndcg@3:0.87323\tval-ndcg@3:0.75982\n",
      "[280]\ttrain-ndcg@3:0.87744\tval-ndcg@3:0.75999\n",
      "[300]\ttrain-ndcg@3:0.88213\tval-ndcg@3:0.76125\n",
      "[320]\ttrain-ndcg@3:0.88627\tval-ndcg@3:0.76296\n",
      "[340]\ttrain-ndcg@3:0.89092\tval-ndcg@3:0.76455\n",
      "[360]\ttrain-ndcg@3:0.89442\tval-ndcg@3:0.76462\n",
      "[380]\ttrain-ndcg@3:0.89782\tval-ndcg@3:0.76624\n",
      "[400]\ttrain-ndcg@3:0.90086\tval-ndcg@3:0.76652\n",
      "[420]\ttrain-ndcg@3:0.90374\tval-ndcg@3:0.76762\n",
      "[440]\ttrain-ndcg@3:0.90644\tval-ndcg@3:0.76718\n",
      "[460]\ttrain-ndcg@3:0.90853\tval-ndcg@3:0.76832\n",
      "[480]\ttrain-ndcg@3:0.91083\tval-ndcg@3:0.76790\n",
      "[499]\ttrain-ndcg@3:0.91232\tval-ndcg@3:0.76785\n",
      "âœ… å·²å„²å­˜æ¨¡å‹: model_output/selected_features_xgb/v2_top20/xgb_ranker_small.bin\n",
      "âœ… HitRate@3 (groups size in [10, inf]): 0.7470\n",
      "ğŸ§¹ è¨˜æ†¶é«”å·²æ¸…ç†\n",
      "\n",
      "ğŸš€ è™•ç†åˆ†ç¾¤: medium\n",
      "âœ… Train: 2499164 rows, 36091 groups\n",
      "âœ… Val: 627967 rows, 9023 groups\n",
      "[0]\ttrain-ndcg@3:0.44289\tval-ndcg@3:0.43566\n",
      "[20]\ttrain-ndcg@3:0.55002\tval-ndcg@3:0.51323\n",
      "[40]\ttrain-ndcg@3:0.56872\tval-ndcg@3:0.52008\n",
      "[60]\ttrain-ndcg@3:0.58107\tval-ndcg@3:0.52864\n",
      "[80]\ttrain-ndcg@3:0.59230\tval-ndcg@3:0.53232\n",
      "[100]\ttrain-ndcg@3:0.60327\tval-ndcg@3:0.53594\n",
      "[120]\ttrain-ndcg@3:0.61173\tval-ndcg@3:0.53985\n",
      "[140]\ttrain-ndcg@3:0.62143\tval-ndcg@3:0.54311\n",
      "[160]\ttrain-ndcg@3:0.63164\tval-ndcg@3:0.54575\n",
      "[180]\ttrain-ndcg@3:0.64098\tval-ndcg@3:0.54934\n",
      "[200]\ttrain-ndcg@3:0.64885\tval-ndcg@3:0.55095\n",
      "[220]\ttrain-ndcg@3:0.65695\tval-ndcg@3:0.55403\n",
      "[240]\ttrain-ndcg@3:0.66453\tval-ndcg@3:0.55645\n",
      "[260]\ttrain-ndcg@3:0.67282\tval-ndcg@3:0.55953\n",
      "[280]\ttrain-ndcg@3:0.68138\tval-ndcg@3:0.56162\n",
      "[300]\ttrain-ndcg@3:0.68864\tval-ndcg@3:0.56407\n",
      "[320]\ttrain-ndcg@3:0.69626\tval-ndcg@3:0.56623\n",
      "[340]\ttrain-ndcg@3:0.70370\tval-ndcg@3:0.56909\n",
      "[360]\ttrain-ndcg@3:0.71127\tval-ndcg@3:0.57231\n",
      "[380]\ttrain-ndcg@3:0.71737\tval-ndcg@3:0.57414\n",
      "[400]\ttrain-ndcg@3:0.72380\tval-ndcg@3:0.57673\n",
      "[420]\ttrain-ndcg@3:0.72767\tval-ndcg@3:0.57899\n",
      "[440]\ttrain-ndcg@3:0.73235\tval-ndcg@3:0.57970\n",
      "[460]\ttrain-ndcg@3:0.73743\tval-ndcg@3:0.58080\n",
      "[480]\ttrain-ndcg@3:0.74312\tval-ndcg@3:0.58253\n",
      "[499]\ttrain-ndcg@3:0.74734\tval-ndcg@3:0.58329\n",
      "âœ… å·²å„²å­˜æ¨¡å‹: model_output/selected_features_xgb/v2_top20/xgb_ranker_medium.bin\n",
      "âœ… HitRate@3 (groups size in [10, inf]): 0.5312\n",
      "ğŸ§¹ è¨˜æ†¶é«”å·²æ¸…ç†\n",
      "\n",
      "ğŸš€ è™•ç†åˆ†ç¾¤: large\n",
      "âœ… Train: 11657259 rows, 20472 groups\n",
      "âœ… Val: 2938143 rows, 5118 groups\n",
      "[0]\ttrain-ndcg@3:0.50241\tval-ndcg@3:0.50740\n",
      "[20]\ttrain-ndcg@3:0.54347\tval-ndcg@3:0.52969\n",
      "[40]\ttrain-ndcg@3:0.55678\tval-ndcg@3:0.53213\n",
      "[60]\ttrain-ndcg@3:0.57269\tval-ndcg@3:0.53506\n",
      "[80]\ttrain-ndcg@3:0.58779\tval-ndcg@3:0.54140\n",
      "[100]\ttrain-ndcg@3:0.60006\tval-ndcg@3:0.54292\n",
      "[120]\ttrain-ndcg@3:0.61219\tval-ndcg@3:0.54508\n",
      "[140]\ttrain-ndcg@3:0.62411\tval-ndcg@3:0.54734\n",
      "[160]\ttrain-ndcg@3:0.63418\tval-ndcg@3:0.54917\n",
      "[180]\ttrain-ndcg@3:0.64254\tval-ndcg@3:0.55158\n",
      "[200]\ttrain-ndcg@3:0.65179\tval-ndcg@3:0.55264\n",
      "[220]\ttrain-ndcg@3:0.65986\tval-ndcg@3:0.55397\n",
      "[240]\ttrain-ndcg@3:0.66444\tval-ndcg@3:0.55449\n",
      "[260]\ttrain-ndcg@3:0.67096\tval-ndcg@3:0.55521\n",
      "[280]\ttrain-ndcg@3:0.67699\tval-ndcg@3:0.55705\n",
      "[300]\ttrain-ndcg@3:0.68482\tval-ndcg@3:0.55798\n",
      "[320]\ttrain-ndcg@3:0.69070\tval-ndcg@3:0.55887\n",
      "[340]\ttrain-ndcg@3:0.69641\tval-ndcg@3:0.56046\n",
      "[360]\ttrain-ndcg@3:0.70163\tval-ndcg@3:0.56123\n",
      "[380]\ttrain-ndcg@3:0.70641\tval-ndcg@3:0.56240\n",
      "[400]\ttrain-ndcg@3:0.71153\tval-ndcg@3:0.56260\n",
      "[420]\ttrain-ndcg@3:0.71478\tval-ndcg@3:0.56293\n",
      "[440]\ttrain-ndcg@3:0.71904\tval-ndcg@3:0.56397\n",
      "[460]\ttrain-ndcg@3:0.72218\tval-ndcg@3:0.56497\n",
      "[480]\ttrain-ndcg@3:0.72641\tval-ndcg@3:0.56619\n",
      "[499]\ttrain-ndcg@3:0.72925\tval-ndcg@3:0.56730\n",
      "âœ… å·²å„²å­˜æ¨¡å‹: model_output/selected_features_xgb/v2_top20/xgb_ranker_large.bin\n",
      "âœ… HitRate@3 (groups size in [10, inf]): 0.2290\n",
      "ğŸ§¹ è¨˜æ†¶é«”å·²æ¸…ç†\n",
      "\n",
      "âš¡ åˆä½µæ‰€æœ‰é©—è­‰çµæœè¨ˆç®— HitRate âš¡\n",
      "âœ… HitRate@3 (groups size in [10, inf]): 0.4945\n",
      "\n",
      "ğŸ’¡ æ¯çµ„ Hitrateï¼š\n",
      "small: 0.7470\n",
      "medium: 0.5312\n",
      "large: 0.2290\n",
      "\n",
      "ğŸ¯ å…¨éƒ¨åˆä½µ HitRate: 0.4945\n",
      "\n",
      "âœ… å·²å„²å­˜æ‰€æœ‰ Hitrate çµæœè‡³ model_output/selected_features_xgb/v2_top20/hitrate_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from scripts.group_wise import prepare_train_val_split\n",
    "from scripts.hitrate import compute_hitrate_at_3\n",
    "\n",
    "# è‡ªè¨‚æ¨¡å‹å„²å­˜ç›®éŒ„\n",
    "model_dir = \"model_output/selected_features_xgb/v2_top20/\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "print(f\"âœ… Model folder: {model_dir}\")\n",
    "\n",
    "# é è¨­ XGB åƒæ•¸\n",
    "xgb_params = {\n",
    "    'objective': 'rank:pairwise',\n",
    "    'eval_metric': 'ndcg@3',\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 10,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'lambda': 10.0,\n",
    "    'learning_rate': 0.05,\n",
    "    'seed': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "split_labels = [\"small\", \"medium\", \"large\"]\n",
    "\n",
    "split_data = result[\"split_data\"]\n",
    "\n",
    "# å»ºç«‹ train_dataset å­è³‡æ–™å¤¾\n",
    "train_dataset_dir = os.path.join(model_dir, \"train_dataset\")\n",
    "os.makedirs(train_dataset_dir, exist_ok=True)\n",
    "\n",
    "# å­˜ parquet åˆ° train_dataset è³‡æ–™å¤¾\n",
    "for label, df in split_data.items():\n",
    "    path = os.path.join(train_dataset_dir, f\"{label}.parquet\")\n",
    "    print(f\"ğŸ’¾ å„²å­˜ {label} åˆ° {path}\")\n",
    "    df.write_parquet(path)\n",
    "\n",
    "\n",
    "# æ¸…ç†è¨˜æ†¶é«”\n",
    "split_data.clear()\n",
    "del result\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… split_data å·²æ¸…ç©ºï¼Œè¨˜æ†¶é«”æ‡‰å·²é‡‹æ”¾\")\n",
    "\n",
    "all_groups_val_np = []\n",
    "all_y_val_np = []\n",
    "all_val_preds = []\n",
    "hitrate_per_group = {}\n",
    "\n",
    "for label in split_labels:\n",
    "    print(f\"\\nğŸš€ è™•ç†åˆ†ç¾¤: {label}\")\n",
    "\n",
    "    data = pl.read_parquet(os.path.join(train_dataset_dir, f\"{label}.parquet\"))\n",
    "\n",
    "    split_result = prepare_train_val_split(\n",
    "        result={\"split_data\": {label: data}},\n",
    "        split_label=label,\n",
    "        feature_cols=label_features[label],\n",
    "        train_fraction=0.8\n",
    "    )\n",
    "\n",
    "    dtrain = xgb.DMatrix(\n",
    "        split_result[\"X_train_np\"],\n",
    "        label=split_result[\"y_train_np\"]\n",
    "    )\n",
    "    dtrain.set_group(split_result[\"group_sizes_train\"])\n",
    "\n",
    "    dval = xgb.DMatrix(\n",
    "        split_result[\"X_val_np\"],\n",
    "        label=split_result[\"y_val_np\"]\n",
    "    )\n",
    "    dval.set_group(split_result[\"group_sizes_val\"])\n",
    "\n",
    "    xgb_model = xgb.train(\n",
    "        xgb_params,\n",
    "        dtrain,\n",
    "        num_boost_round=500,\n",
    "        evals=[(dtrain, \"train\"), (dval, \"val\")],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=20\n",
    "    )\n",
    "\n",
    "    model_path = os.path.join(model_dir, f\"xgb_ranker_{label}.bin\")\n",
    "    xgb_model.save_model(model_path)\n",
    "    print(f\"âœ… å·²å„²å­˜æ¨¡å‹: {model_path}\")\n",
    "\n",
    "    val_preds = xgb_model.predict(dval)\n",
    "\n",
    "    hitrate = compute_hitrate_at_3(\n",
    "        split_result[\"groups_val_np\"],\n",
    "        split_result[\"y_val_np\"],\n",
    "        val_preds\n",
    "    )\n",
    "    hitrate_per_group[label] = hitrate\n",
    "\n",
    "    all_groups_val_np.append(split_result[\"groups_val_np\"])\n",
    "    all_y_val_np.append(split_result[\"y_val_np\"])\n",
    "    all_val_preds.append(val_preds)\n",
    "\n",
    "    del dtrain, dval, xgb_model, val_preds, split_result, data\n",
    "    gc.collect()\n",
    "    print(\"ğŸ§¹ è¨˜æ†¶é«”å·²æ¸…ç†\")\n",
    "\n",
    "print(\"\\nâš¡ åˆä½µæ‰€æœ‰é©—è­‰çµæœè¨ˆç®— HitRate âš¡\")\n",
    "all_groups_val_np = np.concatenate(all_groups_val_np)\n",
    "all_y_val_np = np.concatenate(all_y_val_np)\n",
    "all_val_preds = np.concatenate(all_val_preds)\n",
    "\n",
    "overall_hitrate = compute_hitrate_at_3(\n",
    "    all_groups_val_np,\n",
    "    all_y_val_np,\n",
    "    all_val_preds\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ’¡ æ¯çµ„ Hitrateï¼š\")\n",
    "for label, h in hitrate_per_group.items():\n",
    "    print(f\"{label}: {h:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ å…¨éƒ¨åˆä½µ HitRate: {overall_hitrate:.4f}\")\n",
    "\n",
    "# å°‡ hitrate å¯«å…¥ CSV\n",
    "hitrate_records = [\n",
    "    {\"split_label\": label, \"hitrate\": h}\n",
    "    for label, h in hitrate_per_group.items()\n",
    "]\n",
    "hitrate_records.append({\"split_label\": \"overall\", \"hitrate\": overall_hitrate})\n",
    "\n",
    "hitrate_df = pl.DataFrame(hitrate_records)\n",
    "csv_path = os.path.join(model_dir, \"hitrate_summary.csv\")\n",
    "hitrate_df.write_csv(csv_path)\n",
    "print(f\"\\nâœ… å·²å„²å­˜æ‰€æœ‰ Hitrate çµæœè‡³ {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd815cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "def export_xgb_feature_importance(\n",
    "    model_dir: str,\n",
    "    split_labels: list,\n",
    "    label_features: dict\n",
    "    ):\n",
    "    \"\"\"\n",
    "    è®€å–å¤šå€‹åˆ†ç¾¤çš„xgbæ¨¡å‹ï¼Œè¼¸å‡ºå®Œæ•´feature importance CSVã€‚\n",
    "\n",
    "    åƒæ•¸:\n",
    "    - model_dir: æ¨¡å‹è³‡æ–™å¤¾\n",
    "    - split_labels: åˆ†ç¾¤labelåˆ—è¡¨ (ex: [\"small\", \"medium\", ...])\n",
    "    - label_features: dict(label -> è©²åˆ†ç¾¤ä½¿ç”¨çš„ç‰¹å¾µlist)\n",
    "    - top_n: é¡¯ç¤º/åˆ—å°å‰Né‡è¦ç‰¹å¾µ\n",
    "\n",
    "    æ¯å€‹åˆ†ç¾¤æœƒè¼¸å‡ºä¸€å€‹CSVåœ¨ model_importance å­è³‡æ–™å¤¾ã€‚\n",
    "    \"\"\"\n",
    "    model_importance_dir = os.path.join(model_dir, \"model_importance\")\n",
    "    os.makedirs(model_importance_dir, exist_ok=True)\n",
    "\n",
    "    for label in split_labels:\n",
    "        features_this_label = label_features[label]\n",
    "        print(f\"\\nğŸš€ è™•ç†åˆ†ç¾¤ {label}... (ç‰¹å¾µæ•¸: {len(features_this_label)})\")\n",
    "\n",
    "        model_path = os.path.join(model_dir, f\"xgb_ranker_{label}.bin\")\n",
    "        booster = xgb.Booster()\n",
    "        booster.load_model(model_path)\n",
    "        print(f\"âœ… å·²è®€å–æ¨¡å‹: {model_path}\")\n",
    "\n",
    "        importance_types = [\"weight\", \"gain\", \"cover\"]\n",
    "        importance_all = {}\n",
    "\n",
    "        # å–å¾—ä¸‰ç¨®é‡è¦æ€§\n",
    "        for imp_type in importance_types:\n",
    "            imp_raw = booster.get_score(importance_type=imp_type)\n",
    "            imp_named = {}\n",
    "            for k, v in imp_raw.items():\n",
    "                idx = int(k[1:])\n",
    "                real_name = features_this_label[idx]\n",
    "                imp_named[real_name] = v\n",
    "            sorted_imp = sorted(imp_named.items(), key=lambda x: x[1], reverse=True)\n",
    "            importance_all[imp_type] = sorted_imp\n",
    "\n",
    "        # å„åˆ¥DataFrame\n",
    "        df_weight = pd.DataFrame(importance_all[\"weight\"], columns=[\"feature\", \"weight\"])\n",
    "        df_weight[\"weight_rank_pos\"] = df_weight.index\n",
    "\n",
    "        df_gain = pd.DataFrame(importance_all[\"gain\"], columns=[\"feature\", \"gain\"])\n",
    "        df_gain[\"gain_rank_pos\"] = df_gain.index\n",
    "\n",
    "        df_cover = pd.DataFrame(importance_all[\"cover\"], columns=[\"feature\", \"cover\"])\n",
    "        df_cover[\"cover_rank_pos\"] = df_cover.index\n",
    "\n",
    "        # å»ºå®Œæ•´featureè¡¨\n",
    "        df_all = pd.DataFrame({\"feature\": features_this_label})\n",
    "\n",
    "        # åˆä½µ\n",
    "        df_merged = (\n",
    "            df_all\n",
    "            .merge(df_weight, on=\"feature\", how=\"left\")\n",
    "            .merge(df_gain, on=\"feature\", how=\"left\")\n",
    "            .merge(df_cover, on=\"feature\", how=\"left\")\n",
    "        )\n",
    "\n",
    "        # è£œrank_pos\n",
    "        df_merged[\"weight_rank_pos\"] = df_merged[\"weight_rank_pos\"].fillna(9999)\n",
    "        df_merged[\"gain_rank_pos\"] = df_merged[\"gain_rank_pos\"].fillna(9999)\n",
    "        df_merged[\"cover_rank_pos\"] = df_merged[\"cover_rank_pos\"].fillna(9999)\n",
    "\n",
    "        # è¨ˆç®—æœ€å°rank\n",
    "        df_merged[\"min_rank\"] = df_merged[[\"weight_rank_pos\", \"gain_rank_pos\", \"cover_rank_pos\"]].min(axis=1)\n",
    "\n",
    "        # æ’åº\n",
    "        df_merged_sorted = df_merged.sort_values(\"min_rank\")\n",
    "\n",
    "        # è¼¸å‡º\n",
    "        csv_path = os.path.join(model_importance_dir, f\"feature_importance_{label}_all_features.csv\")\n",
    "        df_merged_sorted.to_csv(csv_path, index=False)\n",
    "        print(f\"âœ… å·²è¼¸å‡ºç‰¹å¾µé‡è¦æ€§åˆ° {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "208226e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ è™•ç†åˆ†ç¾¤ small... (ç‰¹å¾µæ•¸: 95)\n",
      "âœ… å·²è®€å–æ¨¡å‹: model_output/selected_features_xgb/v1/xgb_ranker_small.bin\n",
      "âœ… å·²è¼¸å‡ºç‰¹å¾µé‡è¦æ€§åˆ° model_output/selected_features_xgb/v1/model_importance/feature_importance_small_all_features.csv\n",
      "\n",
      "ğŸš€ è™•ç†åˆ†ç¾¤ medium... (ç‰¹å¾µæ•¸: 99)\n",
      "âœ… å·²è®€å–æ¨¡å‹: model_output/selected_features_xgb/v1/xgb_ranker_medium.bin\n",
      "âœ… å·²è¼¸å‡ºç‰¹å¾µé‡è¦æ€§åˆ° model_output/selected_features_xgb/v1/model_importance/feature_importance_medium_all_features.csv\n",
      "\n",
      "ğŸš€ è™•ç†åˆ†ç¾¤ large... (ç‰¹å¾µæ•¸: 95)\n",
      "âœ… å·²è®€å–æ¨¡å‹: model_output/selected_features_xgb/v1/xgb_ranker_large.bin\n",
      "âœ… å·²è¼¸å‡ºç‰¹å¾µé‡è¦æ€§åˆ° model_output/selected_features_xgb/v1/model_importance/feature_importance_large_all_features.csv\n",
      "\n",
      "ğŸš€ è™•ç†åˆ†ç¾¤ xlarge... (ç‰¹å¾µæ•¸: 78)\n",
      "âœ… å·²è®€å–æ¨¡å‹: model_output/selected_features_xgb/v1/xgb_ranker_xlarge.bin\n",
      "âœ… å·²è¼¸å‡ºç‰¹å¾µé‡è¦æ€§åˆ° model_output/selected_features_xgb/v1/model_importance/feature_importance_xlarge_all_features.csv\n"
     ]
    }
   ],
   "source": [
    "export_xgb_feature_importance(\n",
    "    model_dir=model_dir,\n",
    "    split_labels=split_labels,\n",
    "    label_features=label_features,  # è«‹ç”¨è¨“ç·´ç”¨çš„ç‰¹å¾µ\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213109c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DataFrame ç¾åœ¨åªæœ‰ 52 æ¬„ä½: ['totalPrice', 'legs0_segments0_cabinClass', 'companyID', 'legs0_segments1_departureFrom_airport_iata', 'legs0_segments0_marketingCarrier_code', 'legs1_segments1_departureFrom_airport_iata', 'legs0_segments0_flightNumber', 'both_legs_direct', 'legs1_segments1_arrivalTo_airport_city_iata', 'legs1_segments0_flightNumber', 'legs0_main_carrier', 'days_before_departure', 'legs0_segments0_baggageAllowance_quantity', 'legs1_segments0_cabinClass', 'legs0_segments1_arrivalTo_airport_city_iata', 'legs1_segments0_seatsAvailable', 'legs0_segments1_baggageAllowance_quantity', 'legs0_segments0_seatsAvailable', 'miniRules0_monetaryAmount', 'legs0_arrivalAt_hour', 'legs1_segments0_baggageAllowance_quantity', 'legs1_arrivalAt_hour', 'legs0_departureAt_hour', 'legs0_segments1_flightNumber', 'nationality', 'legs1_departureAt_hour', 'legs1_segments1_flightNumber', 'corporateTariffCode', 'legs1_main_carrier', 'legs1_segments0_departureFrom_airport_iata', 'taxes', 'legs1_departureAt_weekday', 'legs0_segments1_arrivalTo_airport_iata', 'miniRules1_monetaryAmount', 'pricingInfo_isAccessTP', 'legs0_departureAt_weekday', 'legs0_arrivalAt_weekday', 'legs0_duration', 'legs1_segments0_duration', 'legs1_arrivalAt_weekday', 'legs0_arrivalAt_day_period', 'frequentFlyer', 'miniRules1_statusInfos', 'legs0_segments0_arrivalTo_airport_iata', 'legs1_duration', 'legs1_segments0_marketingCarrier_code', 'legs0_segments0_aircraft_code', 'searchRoute', 'legs1_segments0_aircraft_code', 'isVip', 'selected', 'ranker_id']\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "# è®€å– parquet\n",
    "train_filled = pl.read_parquet(\"train_filled.parquet\")\n",
    "\n",
    "# è®€å–top50 features\n",
    "\n",
    "\n",
    "df_top50 = pd.read_csv(\"feature_importance_top50.csv\")\n",
    "top50_features = df_top50[\"feature\"].tolist()\n",
    "\n",
    "# ä½ è¦ä¿ç•™çš„æ¬„ä½ (Top50 + target + group id)\n",
    "cols_to_keep = top50_features + [\"selected\", \"ranker_id\"]\n",
    "\n",
    "# åªä¿ç•™é€™äº›æ¬„ä½\n",
    "train_filled = train_filled.select(cols_to_keep)\n",
    "\n",
    "print(f\"âœ… DataFrame ç¾åœ¨åªæœ‰ {len(train_filled.columns)} æ¬„ä½: {train_filled.columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064a83b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆªæ‰å¾Œé‚„æœ‰ 18075751 è¡Œè³‡æ–™\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import polars as pl\n",
    "\n",
    "# ç¬¬ä¸€æ­¥ï¼šè¨ˆç®—æ¯å€‹ranker_idçš„å‡ºç¾æ¬¡æ•¸\n",
    "group_counts = (\n",
    "    train_filled\n",
    "    .group_by(\"ranker_id\")\n",
    "    .agg(pl.count().alias(\"n_rows\"))\n",
    ")\n",
    "\n",
    "# ç¬¬äºŒæ­¥ï¼šæŒ‘é¸ >=10 çš„ ranker_id\n",
    "valid_ranker_ids = (\n",
    "    group_counts\n",
    "    .filter(pl.col(\"n_rows\") >= 10)\n",
    "    .select(\"ranker_id\")\n",
    ")\n",
    "\n",
    "# ç¬¬ä¸‰æ­¥ï¼šéæ¿¾æ‰ <10 çš„\n",
    "train_filled = (\n",
    "    train_filled\n",
    "    .join(valid_ranker_ids, on=\"ranker_id\", how=\"inner\")\n",
    ")\n",
    "\n",
    "print(f\"åˆªæ‰å¾Œé‚„æœ‰ {train_filled.height} è¡Œè³‡æ–™\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e60b2977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 50 features\n"
     ]
    }
   ],
   "source": [
    "exclude_cols = [\n",
    "    'Id', 'ranker_id', 'selected'\n",
    "]\n",
    "\n",
    "feature_cols = [col for col in train_filled.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Using {len(feature_cols)} features\")\n",
    "\n",
    "X = train_filled.select(feature_cols)\n",
    "y = train_filled.select('selected')\n",
    "groups = train_filled.select('ranker_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be5ea68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# ç¢ºèªé€™äº›ç‰©ä»¶éƒ½æ˜¯Polars DataFrame\n",
    "# X, y, groups\n",
    "# éƒ½æ˜¯ shape [n_rows, n_cols]\n",
    "\n",
    "# å…ˆæŠŠ ranker_idè½‰list\n",
    "unique_rankers = groups.select(\"ranker_id\").unique().to_series().to_list()\n",
    "\n",
    "# æ‰“äº‚\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(unique_rankers)\n",
    "\n",
    "# åˆ‡8:2\n",
    "n_train = int(0.8 * len(unique_rankers))\n",
    "train_rankers = set(unique_rankers[:n_train])\n",
    "val_rankers = set(unique_rankers[n_train:])\n",
    "\n",
    "# ç”¨ Polars éæ¿¾ train/val\n",
    "is_train = groups.select(pl.col(\"ranker_id\").is_in(list(train_rankers)).alias(\"is_train\"))\n",
    "\n",
    "# å…ˆ concat mask\n",
    "X_with_mask = X.with_columns(is_train)\n",
    "y_with_mask = y.with_columns(is_train)\n",
    "groups_with_mask = groups.with_columns(is_train)\n",
    "\n",
    "# åˆ†å‰² DataFrame\n",
    "X_train_df = X_with_mask.filter(pl.col(\"is_train\"))\n",
    "X_val_df = X_with_mask.filter(~pl.col(\"is_train\"))\n",
    "y_train_df = y_with_mask.filter(pl.col(\"is_train\"))\n",
    "y_val_df = y_with_mask.filter(~pl.col(\"is_train\"))\n",
    "groups_train_df = groups_with_mask.filter(pl.col(\"is_train\"))\n",
    "groups_val_df = groups_with_mask.filter(~pl.col(\"is_train\"))\n",
    "\n",
    "# å†è½‰ numpy (åˆ†æ‰¹)\n",
    "X_train_np = X_train_df.drop(\"is_train\").to_numpy()\n",
    "X_val_np = X_val_df.drop(\"is_train\").to_numpy()\n",
    "y_train_np = y_train_df.drop(\"is_train\").to_numpy().flatten()\n",
    "y_val_np = y_val_df.drop(\"is_train\").to_numpy().flatten()\n",
    "groups_train_np = groups_train_df.drop(\"is_train\").to_numpy().flatten()\n",
    "groups_val_np = groups_val_df.drop(\"is_train\").to_numpy().flatten()\n",
    "\n",
    "# æœ€å¾Œè¨ˆç®— group sizes\n",
    "group_sizes_train = (\n",
    "    groups_train_df.drop(\"is_train\")\n",
    "    .group_by(\"ranker_id\")\n",
    "    .agg(pl.len().alias(\"size\"))\n",
    "    .sort(\"ranker_id\")[\"size\"]\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "group_sizes_val = (\n",
    "    groups_val_df.drop(\"is_train\")\n",
    "    .group_by(\"ranker_id\")\n",
    "    .agg(pl.len().alias(\"size\"))\n",
    "    .sort(\"ranker_id\")[\"size\"]\n",
    "    .to_numpy()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dedc82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# DMatrix å»ºç«‹ (ä¸å†ç”¨ X.columns)\n",
    "dtrain = xgb.DMatrix(\n",
    "    X_train_np,\n",
    "    label=y_train_np,\n",
    ")\n",
    "dtrain.set_group(group_sizes_train)\n",
    "\n",
    "dval = xgb.DMatrix(\n",
    "    X_val_np,\n",
    "    label=y_val_np,\n",
    ")\n",
    "dval.set_group(group_sizes_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28b43402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-ndcg@3:0.72792\tval-ndcg@3:0.73653\n",
      "[20]\ttrain-ndcg@3:0.77023\tval-ndcg@3:0.76656\n",
      "[40]\ttrain-ndcg@3:0.77818\tval-ndcg@3:0.76950\n",
      "[60]\ttrain-ndcg@3:0.78411\tval-ndcg@3:0.77374\n",
      "[80]\ttrain-ndcg@3:0.78976\tval-ndcg@3:0.77621\n",
      "[100]\ttrain-ndcg@3:0.79574\tval-ndcg@3:0.77886\n",
      "[120]\ttrain-ndcg@3:0.80164\tval-ndcg@3:0.78131\n",
      "[140]\ttrain-ndcg@3:0.80678\tval-ndcg@3:0.78428\n",
      "[160]\ttrain-ndcg@3:0.81189\tval-ndcg@3:0.78623\n",
      "[180]\ttrain-ndcg@3:0.81691\tval-ndcg@3:0.78766\n",
      "[200]\ttrain-ndcg@3:0.82134\tval-ndcg@3:0.78926\n",
      "[220]\ttrain-ndcg@3:0.82588\tval-ndcg@3:0.79097\n",
      "[240]\ttrain-ndcg@3:0.82996\tval-ndcg@3:0.79216\n",
      "[260]\ttrain-ndcg@3:0.83413\tval-ndcg@3:0.79359\n",
      "[280]\ttrain-ndcg@3:0.83805\tval-ndcg@3:0.79404\n",
      "[300]\ttrain-ndcg@3:0.84146\tval-ndcg@3:0.79494\n",
      "[320]\ttrain-ndcg@3:0.84446\tval-ndcg@3:0.79546\n",
      "[340]\ttrain-ndcg@3:0.84737\tval-ndcg@3:0.79649\n",
      "[360]\ttrain-ndcg@3:0.85052\tval-ndcg@3:0.79670\n",
      "[380]\ttrain-ndcg@3:0.85287\tval-ndcg@3:0.79780\n",
      "[400]\ttrain-ndcg@3:0.85523\tval-ndcg@3:0.79829\n",
      "[420]\ttrain-ndcg@3:0.85710\tval-ndcg@3:0.79888\n",
      "[440]\ttrain-ndcg@3:0.85930\tval-ndcg@3:0.79967\n",
      "[460]\ttrain-ndcg@3:0.86135\tval-ndcg@3:0.80056\n",
      "[480]\ttrain-ndcg@3:0.86324\tval-ndcg@3:0.80074\n",
      "[499]\ttrain-ndcg@3:0.86500\tval-ndcg@3:0.80101\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# åƒæ•¸\n",
    "xgb_params = {\n",
    "    'objective': 'rank:pairwise',\n",
    "    'eval_metric': 'ndcg@3',\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 10,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'lambda': 10.0,\n",
    "    'learning_rate': 0.05,\n",
    "    'seed': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "# è¨“ç·´\n",
    "xgb_model = xgb.train(\n",
    "    xgb_params,\n",
    "    dtrain,\n",
    "    num_boost_round=500,\n",
    "    evals=[(dtrain, \"train\"), (dval, \"val\")],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12133ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… HitRate@3 (groups >10): 0.5079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5079179212668674"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "def compute_hitrate_at_3(groups_np, labels_np, preds_np):\n",
    "    \"\"\"\n",
    "    è¨ˆç®— HitRate@3 (å«æ‰€æœ‰ groups èˆ‡ groups >10)\n",
    "    åƒæ•¸:\n",
    "        groups_np: np.array of ranker_id\n",
    "        labels_np: np.array of true labels (0/1)\n",
    "        preds_np: np.array of predicted scores\n",
    "    å›å‚³:\n",
    "        (hitrate_all, hitrate_large_groups)\n",
    "    \"\"\"\n",
    "    # å»ºç«‹ DataFrame\n",
    "    val_df = pl.DataFrame({\n",
    "        \"ranker_id\": groups_np,\n",
    "        \"label\": labels_np,\n",
    "        \"score\": preds_np\n",
    "    })\n",
    "\n",
    "    # æ’åº\n",
    "    val_df = val_df.sort([\"ranker_id\", \"score\"], descending=[False, True])\n",
    "\n",
    "    # rank_in_group\n",
    "    val_df = val_df.with_columns([\n",
    "        pl.col(\"score\").rank(method=\"dense\", descending=True).over(\"ranker_id\").alias(\"rank_in_group\")\n",
    "    ])\n",
    "    # å°æ¯çµ„æ‰¾ label=1 çš„ row\n",
    "    hit_df = (\n",
    "        val_df.filter(pl.col(\"label\") == 1)\n",
    "              .with_columns([\n",
    "                  (pl.col(\"rank_in_group\") <=3).cast(pl.Int8).alias(\"is_hit\")\n",
    "              ])\n",
    "    )\n",
    "\n",
    "    # è¨ˆç®—æ¯çµ„å¤§å°\n",
    "    group_size_df = (\n",
    "        val_df.group_by(\"ranker_id\")\n",
    "              .agg(pl.len().alias(\"group_size\"))\n",
    "    )\n",
    "\n",
    "    # Join å›å»\n",
    "    hit_with_size = hit_df.join(group_size_df, on=\"ranker_id\")\n",
    "\n",
    "    # éæ¿¾ >10\n",
    "    hit_large_groups = hit_with_size.filter((pl.col(\"group_size\") > 10))\n",
    "\n",
    "    # HitRate >10 groups\n",
    "    num_large_groups = hit_large_groups.height\n",
    "    num_large_hits = hit_large_groups[\"is_hit\"].sum()\n",
    "    if num_large_groups > 0:\n",
    "        hitrate_large = num_large_hits / num_large_groups\n",
    "    else:\n",
    "        hitrate_large = 0.0\n",
    "\n",
    "    # è¼¸å‡º\n",
    "    print(f\"âœ… HitRate@3 (groups >10): {hitrate_large:.4f}\")\n",
    "\n",
    "    return hitrate_large\n",
    "# é æ¸¬\n",
    "val_preds = xgb_model.predict(dval)\n",
    "\n",
    "# è¨ˆç®— HitRate\n",
    "compute_hitrate_at_3(groups_val_np, y_val_np, val_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5112913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0], shape=(3769685,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09b4b212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å„²å­˜æ¨¡å‹åˆ° 'xgb_ranker_model.bin'\n"
     ]
    }
   ],
   "source": [
    "# å­˜æˆäºŒé€²ä½æ¨¡å‹\n",
    "xgb_model.save_model(\"xgb_ranker_model.bin\")\n",
    "print(\"âœ… å·²å„²å­˜æ¨¡å‹åˆ° 'xgb_ranker_model.bin'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65f784f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â­ Top 20 Feature Importances (by weight):\n",
      "totalPrice: 8578.0\n",
      "companyID: 6165.0\n",
      "legs0_segments0_flightNumber: 6092.0\n",
      "legs1_segments0_flightNumber: 5554.0\n",
      "days_before_departure: 5454.0\n",
      "legs1_segments0_seatsAvailable: 3737.0\n",
      "legs0_segments0_seatsAvailable: 3721.0\n",
      "legs0_arrivalAt_hour: 3577.0\n",
      "legs1_arrivalAt_hour: 3280.0\n",
      "legs0_departureAt_hour: 3152.0\n",
      "legs1_departureAt_hour: 2996.0\n",
      "corporateTariffCode: 2835.0\n",
      "taxes: 2525.0\n",
      "legs1_departureAt_weekday: 1990.0\n",
      "legs0_departureAt_weekday: 1817.0\n",
      "legs0_arrivalAt_weekday: 1588.0\n",
      "legs1_arrivalAt_weekday: 1501.0\n",
      "frequentFlyer: 1446.0\n",
      "legs1_duration: 1376.0\n",
      "legs0_duration: 1354.0\n"
     ]
    }
   ],
   "source": [
    "# å–å¾—é‡è¦æ€§ (index key: f0, f1, ...)\n",
    "importance_dict = xgb_model.get_score(importance_type=\"weight\")\n",
    "\n",
    "# æŠŠç‰¹å¾µåç¨±å°æ‡‰å› X.columns\n",
    "# å¦‚æœXæ˜¯ Polars DataFrame\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "# é‡æ–°å‘½å\n",
    "importance_named = {}\n",
    "for k, v in importance_dict.items():\n",
    "    # k = 'f0', 'f1', ...\n",
    "    idx = int(k[1:])  # å–æ•¸å­—\n",
    "    real_name = feature_names[idx]\n",
    "    importance_named[real_name] = v\n",
    "\n",
    "# æ’åº\n",
    "sorted_importance = sorted(importance_named.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# åˆ—å°\n",
    "print(\"â­ Top 20 Feature Importances (by weight):\")\n",
    "for feat, score in sorted_importance[:20]:\n",
    "    print(f\"{feat}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21c37925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        feature  weight_rank  gain_rank  \\\n",
      "76                                   totalPrice       8578.0   4.235002   \n",
      "22                   legs0_segments0_cabinClass        109.0  42.103062   \n",
      "1                                     companyID       6165.0   3.913192   \n",
      "34   legs0_segments1_departureFrom_airport_iata          1.0  41.605595   \n",
      "26        legs0_segments0_marketingCarrier_code        153.0   3.633880   \n",
      "62   legs1_segments1_departureFrom_airport_iata          2.0   7.080189   \n",
      "25                 legs0_segments0_flightNumber       6092.0   4.140559   \n",
      "0                              both_legs_direct         35.0  21.166515   \n",
      "60  legs1_segments1_arrivalTo_airport_city_iata          1.0  18.557640   \n",
      "55                 legs1_segments0_flightNumber       5554.0   4.186655   \n",
      "16                           legs0_main_carrier         22.0   3.867804   \n",
      "3                         days_before_departure       5454.0   3.778232   \n",
      "20    legs0_segments0_baggageAllowance_quantity        367.0  18.194492   \n",
      "52                   legs1_segments0_cabinClass         82.0   6.713810   \n",
      "30  legs0_segments1_arrivalTo_airport_city_iata         85.0  14.179173   \n",
      "58               legs1_segments0_seatsAvailable       3737.0   3.428240   \n",
      "32    legs0_segments1_baggageAllowance_quantity         13.0   9.738872   \n",
      "28               legs0_segments0_seatsAvailable       3721.0   3.500748   \n",
      "66                    miniRules0_monetaryAmount        387.0  13.252401   \n",
      "9                          legs0_arrivalAt_hour       3577.0   4.667577   \n",
      "51    legs1_segments0_baggageAllowance_quantity        337.0   9.030958   \n",
      "40                         legs1_arrivalAt_hour       3280.0   4.884501   \n",
      "13                       legs0_departureAt_hour       3152.0   4.933882   \n",
      "36                 legs0_segments1_flightNumber        113.0   6.896737   \n",
      "71                                  nationality         56.0   4.966136   \n",
      "44                       legs1_departureAt_hour       2996.0   4.557615   \n",
      "64                 legs1_segments1_flightNumber         96.0   4.868381   \n",
      "2                           corporateTariffCode       2835.0   4.091713   \n",
      "47                           legs1_main_carrier         33.0   4.998295   \n",
      "53   legs1_segments0_departureFrom_airport_iata        760.0   6.379269   \n",
      "75                                        taxes       2525.0   4.494143   \n",
      "45                    legs1_departureAt_weekday       1990.0   4.234576   \n",
      "31       legs0_segments1_arrivalTo_airport_iata         20.0   6.194692   \n",
      "68                    miniRules1_monetaryAmount        627.0   6.089478   \n",
      "72                       pricingInfo_isAccessTP        666.0   5.068095   \n",
      "14                    legs0_departureAt_weekday       1817.0   3.998265   \n",
      "10                      legs0_arrivalAt_weekday       1588.0   4.056602   \n",
      "15                               legs0_duration       1354.0   5.694603   \n",
      "54                     legs1_segments0_duration       1025.0   5.470428   \n",
      "41                      legs1_arrivalAt_weekday       1501.0   4.133005   \n",
      "8                    legs0_arrivalAt_day_period        108.0   4.562662   \n",
      "4                                 frequentFlyer       1446.0   3.581264   \n",
      "69                       miniRules1_statusInfos        203.0   5.436374   \n",
      "19       legs0_segments0_arrivalTo_airport_iata        640.0   5.419785   \n",
      "46                               legs1_duration       1376.0   5.175110   \n",
      "56        legs1_segments0_marketingCarrier_code         96.0   3.987250   \n",
      "17                legs0_segments0_aircraft_code       1332.0   4.612083   \n",
      "73                                  searchRoute        478.0   5.087680   \n",
      "48                legs1_segments0_aircraft_code       1112.0   3.876847   \n",
      "6                                         isVip        195.0   5.082795   \n",
      "\n",
      "     cover_rank  \n",
      "76  1051.144165  \n",
      "22  8636.604492  \n",
      "1    557.099731  \n",
      "34  1809.537231  \n",
      "26  4330.587402  \n",
      "62  3745.372803  \n",
      "25   611.121948  \n",
      "0   2513.184814  \n",
      "60  1159.031616  \n",
      "55   713.426208  \n",
      "16  3589.774902  \n",
      "3    406.564484  \n",
      "20  3101.126709  \n",
      "52  3487.523926  \n",
      "30  2815.055420  \n",
      "58   373.021179  \n",
      "32  3072.663086  \n",
      "28   348.634918  \n",
      "66  2770.843994  \n",
      "9    748.977051  \n",
      "51  1435.975708  \n",
      "40   731.923035  \n",
      "13   998.648132  \n",
      "36   751.836304  \n",
      "71  2507.800049  \n",
      "44   835.805725  \n",
      "64  2271.052979  \n",
      "2    586.894470  \n",
      "47  2204.699951  \n",
      "53  1588.734009  \n",
      "75   717.460022  \n",
      "45   435.478180  \n",
      "31  1545.135498  \n",
      "68   854.931519  \n",
      "72  1675.085205  \n",
      "14   630.044739  \n",
      "10   683.635254  \n",
      "15   729.967468  \n",
      "54  1176.345093  \n",
      "41   537.996094  \n",
      "8   1456.471558  \n",
      "4    387.877686  \n",
      "69   925.390869  \n",
      "19   750.591248  \n",
      "46   723.918091  \n",
      "56  1380.463501  \n",
      "17   873.642334  \n",
      "73  1161.782471  \n",
      "48   615.264282  \n",
      "6   1061.007690  \n",
      "âœ… å·²è¼¸å‡º feature_importance_top50.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# å…ˆæº–å‚™ç‰¹å¾µåç¨±\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "# å–ä¸‰ç¨®é‡è¦æ€§\n",
    "importance_types = [\"weight\", \"gain\", \"cover\"]\n",
    "importance_all = {}\n",
    "\n",
    "for imp_type in importance_types:\n",
    "    imp_raw = xgb_model.get_score(importance_type=imp_type)\n",
    "    imp_named = {}\n",
    "    for k, v in imp_raw.items():\n",
    "        idx = int(k[1:])\n",
    "        real_name = feature_names[idx]\n",
    "        imp_named[real_name] = v\n",
    "    # æ’åº\n",
    "    sorted_imp = sorted(imp_named.items(), key=lambda x: x[1], reverse=True)\n",
    "    importance_all[imp_type] = sorted_imp\n",
    "\n",
    "# æŠŠä¸‰å€‹æ¦œå–®æ”¾æˆDataFrameæ–¹ä¾¿æ¯”å°\n",
    "df_weight = pd.DataFrame(importance_all[\"weight\"], columns=[\"feature\", \"weight_rank\"])\n",
    "df_weight[\"weight_rank_pos\"] = df_weight.index\n",
    "\n",
    "df_gain = pd.DataFrame(importance_all[\"gain\"], columns=[\"feature\", \"gain_rank\"])\n",
    "df_gain[\"gain_rank_pos\"] = df_gain.index\n",
    "\n",
    "df_cover = pd.DataFrame(importance_all[\"cover\"], columns=[\"feature\", \"cover_rank\"])\n",
    "df_cover[\"cover_rank_pos\"] = df_cover.index\n",
    "\n",
    "# åˆä½µ\n",
    "df_merged = (\n",
    "    df_weight\n",
    "    .merge(df_gain, on=\"feature\", how=\"outer\")\n",
    "    .merge(df_cover, on=\"feature\", how=\"outer\")\n",
    ")\n",
    "\n",
    "# æŠŠä¸å­˜åœ¨çš„rankè£œå¤§æ•¸å­—\n",
    "df_merged[\"weight_rank_pos\"] = df_merged[\"weight_rank_pos\"].fillna(9999)\n",
    "df_merged[\"gain_rank_pos\"] = df_merged[\"gain_rank_pos\"].fillna(9999)\n",
    "df_merged[\"cover_rank_pos\"] = df_merged[\"cover_rank_pos\"].fillna(9999)\n",
    "\n",
    "# è¨ˆç®—ã€Œä¸‰å€‹æ¦œå–®ä¸­æœ€æ—©å‡ºç¾çš„ä½ç½®ã€\n",
    "df_merged[\"min_rank\"] = df_merged[[\"weight_rank_pos\", \"gain_rank_pos\", \"cover_rank_pos\"]].min(axis=1)\n",
    "\n",
    "# æ’åº\n",
    "df_merged_sorted = df_merged.sort_values(\"min_rank\")\n",
    "\n",
    "# å–å‰50\n",
    "top50 = df_merged_sorted.head(50)\n",
    "\n",
    "# é¡¯ç¤º\n",
    "print(top50[[\"feature\", \"weight_rank\", \"gain_rank\", \"cover_rank\"]])\n",
    "\n",
    "# å¦‚æœæƒ³è¼¸å‡ºCSV\n",
    "top50.to_csv(\"feature_importance_top50.csv\", index=False)\n",
    "print(\"âœ… å·²è¼¸å‡º feature_importance_top50.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b576b3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (18_145_372, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Id</th><th>ranker_id</th><th>selected</th><th>profileId</th><th>requestDate</th><th>miniRules0_percentage</th><th>miniRules1_percentage</th><th>frequentFlyer</th><th>pricingInfo_passengerCount</th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>datetime[ns]</td><td>f64</td><td>f64</td><td>i32</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>&quot;98ce0dabf6964640b63079fbafd42câ€¦</td><td>1</td><td>2087645</td><td>2024-05-17 03:03:08</td><td>0.0</td><td>0.0</td><td>95</td><td>1</td></tr><tr><td>1</td><td>&quot;98ce0dabf6964640b63079fbafd42câ€¦</td><td>0</td><td>2087645</td><td>2024-05-17 03:03:08</td><td>0.0</td><td>0.0</td><td>95</td><td>1</td></tr><tr><td>2</td><td>&quot;98ce0dabf6964640b63079fbafd42câ€¦</td><td>0</td><td>2087645</td><td>2024-05-17 03:03:08</td><td>0.0</td><td>0.0</td><td>95</td><td>1</td></tr><tr><td>3</td><td>&quot;98ce0dabf6964640b63079fbafd42câ€¦</td><td>0</td><td>2087645</td><td>2024-05-17 03:03:08</td><td>0.0</td><td>0.0</td><td>95</td><td>1</td></tr><tr><td>4</td><td>&quot;98ce0dabf6964640b63079fbafd42câ€¦</td><td>0</td><td>2087645</td><td>2024-05-17 03:03:08</td><td>0.0</td><td>0.0</td><td>95</td><td>1</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>18146427</td><td>&quot;88f8c53a28bf4f438941fd67338009â€¦</td><td>0</td><td>3046852</td><td>2024-10-29 12:46:20</td><td>0.0</td><td>0.0</td><td>371</td><td>1</td></tr><tr><td>18146428</td><td>&quot;88f8c53a28bf4f438941fd67338009â€¦</td><td>0</td><td>3046852</td><td>2024-10-29 12:46:20</td><td>0.0</td><td>0.0</td><td>371</td><td>1</td></tr><tr><td>18146429</td><td>&quot;88f8c53a28bf4f438941fd67338009â€¦</td><td>0</td><td>3046852</td><td>2024-10-29 12:46:20</td><td>0.0</td><td>0.0</td><td>371</td><td>1</td></tr><tr><td>18146430</td><td>&quot;88f8c53a28bf4f438941fd67338009â€¦</td><td>0</td><td>3046852</td><td>2024-10-29 12:46:20</td><td>0.0</td><td>0.0</td><td>371</td><td>1</td></tr><tr><td>18146431</td><td>&quot;88f8c53a28bf4f438941fd67338009â€¦</td><td>0</td><td>3046852</td><td>2024-10-29 12:46:20</td><td>0.0</td><td>0.0</td><td>371</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (18_145_372, 9)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ Id       â”† ranker_id  â”† selected â”† profileId â”† â€¦ â”† miniRules â”† miniRules â”† frequentF â”† pricingIn â”‚\n",
       "â”‚ ---      â”† ---        â”† ---      â”† ---       â”†   â”† 0_percent â”† 1_percent â”† lyer      â”† fo_passen â”‚\n",
       "â”‚ i64      â”† str        â”† i64      â”† i64       â”†   â”† age       â”† age       â”† ---       â”† gerCount  â”‚\n",
       "â”‚          â”†            â”†          â”†           â”†   â”† ---       â”† ---       â”† i32       â”† ---       â”‚\n",
       "â”‚          â”†            â”†          â”†           â”†   â”† f64       â”† f64       â”†           â”† i64       â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 0        â”† 98ce0dabf6 â”† 1        â”† 2087645   â”† â€¦ â”† 0.0       â”† 0.0       â”† 95        â”† 1         â”‚\n",
       "â”‚          â”† 964640b630 â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† 79fbafd42c â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† â€¦          â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ 1        â”† 98ce0dabf6 â”† 0        â”† 2087645   â”† â€¦ â”† 0.0       â”† 0.0       â”† 95        â”† 1         â”‚\n",
       "â”‚          â”† 964640b630 â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† 79fbafd42c â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† â€¦          â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ 2        â”† 98ce0dabf6 â”† 0        â”† 2087645   â”† â€¦ â”† 0.0       â”† 0.0       â”† 95        â”† 1         â”‚\n",
       "â”‚          â”† 964640b630 â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† 79fbafd42c â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† â€¦          â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ 3        â”† 98ce0dabf6 â”† 0        â”† 2087645   â”† â€¦ â”† 0.0       â”† 0.0       â”† 95        â”† 1         â”‚\n",
       "â”‚          â”† 964640b630 â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† 79fbafd42c â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† â€¦          â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ 4        â”† 98ce0dabf6 â”† 0        â”† 2087645   â”† â€¦ â”† 0.0       â”† 0.0       â”† 95        â”† 1         â”‚\n",
       "â”‚          â”† 964640b630 â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† 79fbafd42c â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† â€¦          â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ â€¦        â”† â€¦          â”† â€¦        â”† â€¦         â”† â€¦ â”† â€¦         â”† â€¦         â”† â€¦         â”† â€¦         â”‚\n",
       "â”‚ 18146427 â”† 88f8c53a28 â”† 0        â”† 3046852   â”† â€¦ â”† 0.0       â”† 0.0       â”† 371       â”† 1         â”‚\n",
       "â”‚          â”† bf4f438941 â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† fd67338009 â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† â€¦          â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ 18146428 â”† 88f8c53a28 â”† 0        â”† 3046852   â”† â€¦ â”† 0.0       â”† 0.0       â”† 371       â”† 1         â”‚\n",
       "â”‚          â”† bf4f438941 â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† fd67338009 â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† â€¦          â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ 18146429 â”† 88f8c53a28 â”† 0        â”† 3046852   â”† â€¦ â”† 0.0       â”† 0.0       â”† 371       â”† 1         â”‚\n",
       "â”‚          â”† bf4f438941 â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† fd67338009 â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† â€¦          â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ 18146430 â”† 88f8c53a28 â”† 0        â”† 3046852   â”† â€¦ â”† 0.0       â”† 0.0       â”† 371       â”† 1         â”‚\n",
       "â”‚          â”† bf4f438941 â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† fd67338009 â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† â€¦          â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ 18146431 â”† 88f8c53a28 â”† 0        â”† 3046852   â”† â€¦ â”† 0.0       â”† 0.0       â”† 371       â”† 1         â”‚\n",
       "â”‚          â”† bf4f438941 â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† fd67338009 â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚          â”† â€¦          â”†          â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_xgb = X.with_columns([(pl.col(c).rank(\"dense\") - 1).fill_null(-1).cast(pl.Int32) for c in cat_features_final])\n",
    "\n",
    "n1 = 16487352 # split train to train and val (10%) in time\n",
    "n2 = train.height\n",
    "data_xgb_tr, data_xgb_va, data_xgb_te = data_xgb[:n1], data_xgb[n1:n2], data_xgb[n2:]\n",
    "y_tr, y_va, y_te = y[:n1], y[n1:n2], y[n2:]\n",
    "groups_tr, groups_va, groups_te = groups[:n1], groups[n1:n2], groups[n2:]\n",
    "\n",
    "group_sizes_tr = groups_tr.group_by('ranker_id').agg(pl.len()).sort('ranker_id')['len'].to_numpy()\n",
    "group_sizes_va = groups_va.group_by('ranker_id').agg(pl.len()).sort('ranker_id')['len'].to_numpy()\n",
    "group_sizes_te = groups_te.group_by('ranker_id').agg(pl.len()).sort('ranker_id')['len'].to_numpy()\n",
    "dtrain = xgb.DMatrix(data_xgb_tr, label=y_tr, group=group_sizes_tr, feature_names=data_xgb.columns)\n",
    "dval   = xgb.DMatrix(data_xgb_va, label=y_va, group=group_sizes_va, feature_names=data_xgb.columns)\n",
    "dtest  = xgb.DMatrix(data_xgb_te, label=y_te, group=group_sizes_te, feature_names=data_xgb.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9587985",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ab393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"colsample_bytree\": 0.5242042724303907,\n",
    "    \"learning_rate\": 0.014470794293130388,\n",
    "    \"min_child_samples\": 25,\n",
    "    \"min_child_weight\": 0.1936457311991661,\n",
    "    \"n_estimators\": 441,\n",
    "    \"n_jobs\": -1,\n",
    "    \"num_leaves\": 15,\n",
    "    \"max_depth\": 12,\n",
    "    \"random_state\": 42,\n",
    "    \"reg_alpha\": 76.69015407123774,\n",
    "    \"reg_lambda\": 78.57981723239948,\n",
    "    \"subsample\": 0.35497610282716086,\n",
    "    \"verbose\": -1\n",
    "}\n",
    "\n",
    "lgbm_goss_params = {\n",
    "    \"boosting_type\": \"goss\",\n",
    "    \"colsample_bytree\": 0.32266516869045214,\n",
    "    \"learning_rate\": 0.013684657681610528,\n",
    "    \"min_child_samples\": 16,\n",
    "    \"min_child_weight\": 0.652800548618323,\n",
    "    \"n_estimators\": 268,\n",
    "    \"n_jobs\": -1,\n",
    "    \"num_leaves\": 20,\n",
    "    \"max_depth\": 6,\n",
    "    \"random_state\": 42,\n",
    "    \"reg_alpha\": 24.43093150663448,\n",
    "    \"reg_lambda\": 39.81794248056326,\n",
    "    \"subsample\": 0.21026644887863555,\n",
    "    \"verbose\": -1\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"device\": \"gpu\",\n",
    "    \"colsample_bylevel\": 0.4778,\n",
    "    \"colsample_bynode\": 0.3628,\n",
    "    \"colsample_bytree\": 0.7107,\n",
    "    \"gamma\": 1.7095,\n",
    "    \"learning_rate\": 0.02213,\n",
    "    \"max_depth\": 20,\n",
    "    \"max_leaves\": 12,\n",
    "    \"min_child_weight\": 16,\n",
    "    \"n_estimators\": 1667,\n",
    "    \"subsample\": 0.06567,\n",
    "    \"reg_alpha\": 39.3524,\n",
    "    \"reg_lambda\": 75.4484,\n",
    "    \"verbosity\": 0,\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    \"iterations\": 1000,\n",
    "    \"depth\": 6,  # æ§åˆ¶æ¨¡å‹è¤‡é›œåº¦\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"l2_leaf_reg\": 75.0,  # å°æ‡‰ LGBM çš„ reg_lambda é«˜å€¼\n",
    "    \"random_strength\": 42.0,  # å°æ‡‰ XGB çš„ gamma\n",
    "    \"bagging_temperature\": 1.0,  # å¼·åŒ–æ¨£æœ¬å¤šæ¨£æ€§\n",
    "    \"subsample\": 0.35,  # å°é½Š LGBM çš„ subsample\n",
    "    \"rsm\": 0.52,  # å°é½Š colsample_bytree\n",
    "    \"min_data_in_leaf\": 25,  # å°æ‡‰ min_child_samples\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8b05959",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores = {}\n",
    "oof_preds = {}\n",
    "test_preds = {}\n",
    "train_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7878141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBRegressor\n",
      "\n",
      "\n",
      "ğŸ§© Fold 0\n",
      "  â³ Training slice: full_data, samples: 350591\n",
      "  ğŸ“¦ Saved full_data model to output/UMAP/k-fold/no_test//xgb/fold0_full_data/XGBRegressor.pkl\n",
      "  âœ… full_data - val_pearson: 0.146689 - train_pearson: 0.658291\n",
      "  â³ Training slice: last_75pct, samples: 350591\n",
      "  ğŸ“¦ Saved last_75pct model to output/UMAP/k-fold/no_test//xgb/fold0_last_75pct/XGBRegressor.pkl\n",
      "  âœ… last_75pct - val_pearson: 0.147851 - train_pearson: 0.657555\n",
      "  â³ Training slice: last_50pct, samples: 262944\n",
      "  ğŸ“¦ Saved last_50pct model to output/UMAP/k-fold/no_test//xgb/fold0_last_50pct/XGBRegressor.pkl\n",
      "  âœ… last_50pct - val_pearson: 0.105701 - train_pearson: 0.684429\n",
      "\n",
      "ğŸ§© Fold 1\n",
      "  â³ Training slice: full_data, samples: 350591\n",
      "  ğŸ“¦ Saved full_data model to output/UMAP/k-fold/no_test//xgb/fold1_full_data/XGBRegressor.pkl\n",
      "  âœ… full_data - val_pearson: 0.125696 - train_pearson: 0.642596\n",
      "  â³ Training slice: last_75pct, samples: 219120\n",
      "  ğŸ“¦ Saved last_75pct model to output/UMAP/k-fold/no_test//xgb/fold1_last_75pct/XGBRegressor.pkl\n",
      "  âœ… last_75pct - val_pearson: 0.078872 - train_pearson: 0.707050\n",
      "  â³ Training slice: last_50pct, samples: 175295\n",
      "  ğŸ“¦ Saved last_50pct model to output/UMAP/k-fold/no_test//xgb/fold1_last_50pct/XGBRegressor.pkl\n",
      "  âœ… last_50pct - val_pearson: 0.062678 - train_pearson: 0.728677\n",
      "\n",
      "ğŸ§© Fold 2\n",
      "  â³ Training slice: full_data, samples: 350592\n",
      "  ğŸ“¦ Saved full_data model to output/UMAP/k-fold/no_test//xgb/fold2_full_data/XGBRegressor.pkl\n",
      "  âœ… full_data - val_pearson: 0.102786 - train_pearson: 0.665163\n",
      "  â³ Training slice: last_75pct, samples: 219121\n",
      "  ğŸ“¦ Saved last_75pct model to output/UMAP/k-fold/no_test//xgb/fold2_last_75pct/XGBRegressor.pkl\n",
      "  âœ… last_75pct - val_pearson: 0.085581 - train_pearson: 0.724322\n",
      "  â³ Training slice: last_50pct, samples: 87649\n",
      "  ğŸ“¦ Saved last_50pct model to output/UMAP/k-fold/no_test//xgb/fold2_last_50pct/XGBRegressor.pkl\n",
      "  âœ… last_50pct - val_pearson: 0.073863 - train_pearson: 0.783252\n",
      "\n",
      "Simple Ensemble Pearson:   0.114200\n",
      "Weighted Ensemble Pearson: 0.114756\n",
      "\n",
      "------ Overall Score: 0.114200 - Mean val_pearson: 0.103302 Â± 0.029444 - Mean train_pearson: 0.694593 Â± 0.042719 ------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_trainer = Trainer(XGBRegressor(**xgb_params), models_folder= f\"{CFG.sample_sub_path}/xgb\")\n",
    "\n",
    "oof_preds[\"XGBoost\"], test_preds[\"XGBoost\"], val_scores[\"XGBoost\"], train_scores[\"XGBoost\"] = xgb_trainer.fit_predict(X, y, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2611648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBRegressor\n",
      "\n",
      "\n",
      "ğŸ§© Fold 0\n",
      "  â³ Training slice: full_data, samples: 350591\n",
      "  ğŸ“¦ Saved full_data model to output/UMAP/k-fold/no_test//xgb/fold0_full_data/XGBRegressor.pkl\n",
      "  âœ… full_data - val_pearson: 0.151434 - train_pearson: 0.622458\n",
      "  â³ Training slice: last_75pct, samples: 350591\n",
      "  ğŸ“¦ Saved last_75pct model to output/UMAP/k-fold/no_test//xgb/fold0_last_75pct/XGBRegressor.pkl\n",
      "  âœ… last_75pct - val_pearson: 0.138200 - train_pearson: 0.621360\n",
      "  â³ Training slice: last_50pct, samples: 262944\n",
      "  ğŸ“¦ Saved last_50pct model to output/UMAP/k-fold/no_test//xgb/fold0_last_50pct/XGBRegressor.pkl\n",
      "  âœ… last_50pct - val_pearson: 0.133944 - train_pearson: 0.654433\n",
      "\n",
      "ğŸ§© Fold 1\n",
      "  â³ Training slice: full_data, samples: 350591\n",
      "  ğŸ“¦ Saved full_data model to output/UMAP/k-fold/no_test//xgb/fold1_full_data/XGBRegressor.pkl\n",
      "  âœ… full_data - val_pearson: 0.112764 - train_pearson: 0.601977\n",
      "  â³ Training slice: last_75pct, samples: 219120\n",
      "  ğŸ“¦ Saved last_75pct model to output/UMAP/k-fold/no_test//xgb/fold1_last_75pct/XGBRegressor.pkl\n",
      "  âœ… last_75pct - val_pearson: 0.072593 - train_pearson: 0.668101\n",
      "  â³ Training slice: last_50pct, samples: 175295\n",
      "  ğŸ“¦ Saved last_50pct model to output/UMAP/k-fold/no_test//xgb/fold1_last_50pct/XGBRegressor.pkl\n",
      "  âœ… last_50pct - val_pearson: 0.068919 - train_pearson: 0.693415\n",
      "\n",
      "ğŸ§© Fold 2\n",
      "  â³ Training slice: full_data, samples: 350592\n",
      "  ğŸ“¦ Saved full_data model to output/UMAP/k-fold/no_test//xgb/fold2_full_data/XGBRegressor.pkl\n",
      "  âœ… full_data - val_pearson: 0.124382 - train_pearson: 0.616930\n",
      "  â³ Training slice: last_75pct, samples: 219121\n",
      "  ğŸ“¦ Saved last_75pct model to output/UMAP/k-fold/no_test//xgb/fold2_last_75pct/XGBRegressor.pkl\n",
      "  âœ… last_75pct - val_pearson: 0.105446 - train_pearson: 0.670247\n",
      "  â³ Training slice: last_50pct, samples: 87649\n",
      "  ğŸ“¦ Saved last_50pct model to output/UMAP/k-fold/no_test//xgb/fold2_last_50pct/XGBRegressor.pkl\n",
      "  âœ… last_50pct - val_pearson: 0.071684 - train_pearson: 0.724487\n",
      "\n",
      "Simple Ensemble Pearson:   0.117918\n",
      "Weighted Ensemble Pearson: 0.118544\n",
      "\n",
      "------ Overall Score: 0.117918 - Mean val_pearson: 0.108818 Â± 0.029558 - Mean train_pearson: 0.652601 Â± 0.038164 ------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_trainer = Trainer(XGBRegressor(**xgb_params), models_folder= f\"{CFG.sample_sub_path}/xgb\")\n",
    "\n",
    "oof_preds[\"XGBoost\"], test_preds[\"XGBoost\"], val_scores[\"XGBoost\"], train_scores[\"XGBoost\"] = xgb_trainer.fit_predict(X, y, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdffed69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Saved ensemble submission to submission_try_new_selected_no_test_UMAP.csv\n"
     ]
    }
   ],
   "source": [
    "sub_ens = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'prediction': test_preds[\"XGBoost\"]\n",
    "})\n",
    "ens_name = 'submission_try_new_selected_no_test_UMAP.csv'\n",
    "sub_ens.to_csv(os.path.join(CFG.sample_sub_path, ens_name), index=False)\n",
    "print(f\"ğŸ“¦ Saved ensemble submission to {ens_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3739226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBMRegressor\n",
      "\n",
      "\n",
      "ğŸ§© Fold 0\n",
      "  â³ Training slice: full_data, samples: 350591\n",
      "  ğŸ“¦ Saved full_data model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold0_full_data/LGBMRegressor.pkl\n",
      "  âœ… full_data - val_pearson: 0.088601 - train_pearson: 0.624664\n",
      "  â³ Training slice: last_75pct, samples: 350591\n",
      "  ğŸ“¦ Saved last_75pct model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold0_last_75pct/LGBMRegressor.pkl\n",
      "  âœ… last_75pct - val_pearson: 0.096252 - train_pearson: 0.625570\n",
      "  â³ Training slice: last_50pct, samples: 262944\n",
      "  ğŸ“¦ Saved last_50pct model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold0_last_50pct/LGBMRegressor.pkl\n",
      "  âœ… last_50pct - val_pearson: 0.092069 - train_pearson: 0.654027\n",
      "\n",
      "ğŸ§© Fold 1\n",
      "  â³ Training slice: full_data, samples: 350591\n",
      "  ğŸ“¦ Saved full_data model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold1_full_data/LGBMRegressor.pkl\n",
      "  âœ… full_data - val_pearson: 0.084856 - train_pearson: 0.618298\n",
      "  â³ Training slice: last_75pct, samples: 219120\n",
      "  ğŸ“¦ Saved last_75pct model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold1_last_75pct/LGBMRegressor.pkl\n",
      "  âœ… last_75pct - val_pearson: 0.062559 - train_pearson: 0.673706\n",
      "  â³ Training slice: last_50pct, samples: 175295\n",
      "  ğŸ“¦ Saved last_50pct model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold1_last_50pct/LGBMRegressor.pkl\n",
      "  âœ… last_50pct - val_pearson: 0.044353 - train_pearson: 0.702625\n",
      "\n",
      "ğŸ§© Fold 2\n",
      "  â³ Training slice: full_data, samples: 350592\n",
      "  ğŸ“¦ Saved full_data model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold2_full_data/LGBMRegressor.pkl\n",
      "  âœ… full_data - val_pearson: 0.092442 - train_pearson: 0.623153\n",
      "  â³ Training slice: last_75pct, samples: 219121\n",
      "  ğŸ“¦ Saved last_75pct model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold2_last_75pct/LGBMRegressor.pkl\n",
      "  âœ… last_75pct - val_pearson: 0.081782 - train_pearson: 0.693528\n",
      "  â³ Training slice: last_50pct, samples: 87649\n",
      "  ğŸ“¦ Saved last_50pct model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold2_last_50pct/LGBMRegressor.pkl\n",
      "  âœ… last_50pct - val_pearson: 0.066824 - train_pearson: 0.787775\n",
      "\n",
      "Simple Ensemble Pearson:   0.079919\n",
      "Weighted Ensemble Pearson: 0.080119\n",
      "\n",
      "------ Overall Score: 0.079919 - Mean val_pearson: 0.078860 Â± 0.016337 - Mean train_pearson: 0.667038 Â± 0.052256 ------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm_trainer = Trainer(\n",
    "    LGBMRegressor(**lgbm_params),\n",
    "    models_folder= f\"{CFG.sample_sub_path}/lgbm_gbdt\"\n",
    ")\n",
    "\n",
    "oof_preds[\"LightGBM (gbdt)\"], test_preds[\"LightGBM (gbdt)\"], val_scores[\"LightGBM (gbdt)\"], train_scores[\"LightGBM (gbdt)\"] = lgbm_trainer.fit_predict(X, y, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12324e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBMRegressor\n",
      "\n",
      "\n",
      "ğŸ§© Fold 0\n",
      "  â³ Training slice: full_data, samples: 350591\n",
      "  ğŸ“¦ Saved full_data model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold0_full_data/LGBMRegressor.pkl\n",
      "  âœ… full_data - val_pearson: 0.099182 - train_pearson: 0.625195\n",
      "  â³ Training slice: last_75pct, samples: 350591\n",
      "  ğŸ“¦ Saved last_75pct model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold0_last_75pct/LGBMRegressor.pkl\n",
      "  âœ… last_75pct - val_pearson: 0.099182 - train_pearson: 0.625195\n",
      "  â³ Training slice: last_50pct, samples: 262944\n",
      "  ğŸ“¦ Saved last_50pct model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold0_last_50pct/LGBMRegressor.pkl\n",
      "  âœ… last_50pct - val_pearson: 0.092069 - train_pearson: 0.654027\n",
      "\n",
      "ğŸ§© Fold 1\n",
      "  â³ Training slice: full_data, samples: 350591\n",
      "  ğŸ“¦ Saved full_data model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold1_full_data/LGBMRegressor.pkl\n",
      "  âœ… full_data - val_pearson: 0.080179 - train_pearson: 0.621754\n",
      "  â³ Training slice: last_75pct, samples: 219120\n",
      "  ğŸ“¦ Saved last_75pct model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold1_last_75pct/LGBMRegressor.pkl\n",
      "  âœ… last_75pct - val_pearson: 0.068955 - train_pearson: 0.674912\n",
      "  â³ Training slice: last_50pct, samples: 175295\n",
      "  ğŸ“¦ Saved last_50pct model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold1_last_50pct/LGBMRegressor.pkl\n",
      "  âœ… last_50pct - val_pearson: 0.048026 - train_pearson: 0.706151\n",
      "\n",
      "ğŸ§© Fold 2\n",
      "  â³ Training slice: full_data, samples: 350592\n",
      "  ğŸ“¦ Saved full_data model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold2_full_data/LGBMRegressor.pkl\n",
      "  âœ… full_data - val_pearson: 0.094560 - train_pearson: 0.625477\n",
      "  â³ Training slice: last_75pct, samples: 219121\n",
      "  ğŸ“¦ Saved last_75pct model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold2_last_75pct/LGBMRegressor.pkl\n",
      "  âœ… last_75pct - val_pearson: 0.077062 - train_pearson: 0.694888\n",
      "  â³ Training slice: last_50pct, samples: 87649\n",
      "  ğŸ“¦ Saved last_50pct model to output/UMAP/k-fold/no_test//lgbm_gbdt/fold2_last_50pct/LGBMRegressor.pkl\n",
      "  âœ… last_50pct - val_pearson: 0.066283 - train_pearson: 0.789036\n",
      "\n",
      "Simple Ensemble Pearson:   0.081778\n",
      "Weighted Ensemble Pearson: 0.082304\n",
      "\n",
      "------ Overall Score: 0.081778 - Mean val_pearson: 0.080611 Â± 0.016436 - Mean train_pearson: 0.668515 Â± 0.052367 ------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm_trainer = Trainer(\n",
    "    LGBMRegressor(**lgbm_params),\n",
    "    models_folder= f\"{CFG.sample_sub_path}/lgbm_gbdt\"\n",
    ")\n",
    "\n",
    "oof_preds[\"LightGBM (gbdt)\"], test_preds[\"LightGBM (gbdt)\"], val_scores[\"LightGBM (gbdt)\"], train_scores[\"LightGBM (gbdt)\"] = lgbm_trainer.fit_predict(X, y, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "657c3503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Saved ensemble submission to submission_try_new_selected_no_test_UMAP.csv\n"
     ]
    }
   ],
   "source": [
    "sub_ens = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'prediction': test_preds[\"LightGBM (gbdt)\"]\n",
    "})\n",
    "ens_name = 'submission_try_new_selected_no_test_UMAP.csv'\n",
    "sub_ens.to_csv(os.path.join(CFG.sample_sub_path, ens_name), index=False)\n",
    "print(f\"ğŸ“¦ Saved ensemble submission to {ens_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ef555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBMRegressor\n",
      "\n",
      "ğŸ“¦ Saved fold 0 model to output/k-fold/other_selected_25/ensemble//goss_lgbm/fold0/LGBMRegressor.pkl\n",
      "--- Fold 0 - val_pearson: 0.180462 - train_pearson: 0.207905\n",
      "ğŸ“¦ Saved fold 1 model to output/k-fold/other_selected_25/ensemble//goss_lgbm/fold1/LGBMRegressor.pkl\n",
      "--- Fold 1 - val_pearson: 0.113101 - train_pearson: 0.222472\n",
      "ğŸ“¦ Saved fold 2 model to output/k-fold/other_selected_25/ensemble//goss_lgbm/fold2/LGBMRegressor.pkl\n",
      "--- Fold 2 - val_pearson: 0.068248 - train_pearson: 0.223433\n",
      "ğŸ“¦ Saved fold 3 model to output/k-fold/other_selected_25/ensemble//goss_lgbm/fold3/LGBMRegressor.pkl\n",
      "--- Fold 3 - val_pearson: 0.148354 - train_pearson: 0.210567\n",
      "ğŸ“¦ Saved fold 4 model to output/k-fold/other_selected_25/ensemble//goss_lgbm/fold4/LGBMRegressor.pkl\n",
      "--- Fold 4 - val_pearson: 0.091773 - train_pearson: 0.226333\n",
      "\n",
      "------ Overall Score: 0.100684 - Mean val_pearson: 0.120388 Â± 0.039954 - Mean train_pearson: 0.218142 Â± 0.007430 ------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm_goss_trainer = Trainer(LGBMRegressor(**lgbm_goss_params),     models_folder= f\"{CFG.sample_sub_path}/goss_lgbm\")\n",
    "\n",
    "oof_preds[\"LightGBM (goss)\"], test_preds[\"LightGBM (goss)\"], val_scores[\"LightGBM (goss)\"], train_scores[\"LightGBM (goss)\"] = lgbm_goss_trainer.fit_predict(X, y, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a15430b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBRegressor\n",
      "\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[22:08:45] /Users/runner/miniforge3/conda-bld/xgboost-split_1748292887431/work/src/data/gradient_index.h:100: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000015d67a0f0 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000015d81b618 void xgboost::GHistIndexMatrix::SetIndexData<xgboost::data::ColumnarAdapterBatch, unsigned int, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::ColumnarAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&), xgboost::data::IsValidFunctor&>(xgboost::common::Span<xgboost::data::IsValidFunctor&, 18446744073709551615ul>, unsigned long, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, unsigned long, xgboost::data::ColumnarAdapterBatch const&, xgboost::data::IsValidFunctor&, unsigned long, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::ColumnarAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&)&&) + 328\n  [bt] (2) 3   libxgboost.dylib                    0x000000015d81a1d0 void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::ColumnarAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>) + 404\n  [bt] (3) 4   libxgboost.dylib                    0x000000015d819cc4 void xgboost::GHistIndexMatrix::PushAdapterBatch<xgboost::data::ColumnarAdapterBatch>(xgboost::Context const*, unsigned long, unsigned long, xgboost::data::ColumnarAdapterBatch const&, float, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, double, unsigned long long) + 300\n  [bt] (4) 5   libxgboost.dylib                    0x000000015d821520 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 2404\n  [bt] (5) 6   libxgboost.dylib                    0x000000015d82075c xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int, long long) + 828\n  [bt] (6) 7   libxgboost.dylib                    0x000000015d7c45e0 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int, long long) + 152\n  [bt] (7) 8   libxgboost.dylib                    0x000000015d683f6c XGQuantileDMatrixCreateFromCallback + 508\n  [bt] (8) 9   libffi.8.dylib                      0x0000000101c20050 ffi_call_SYSV + 80\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m xgb_trainer \u001b[38;5;241m=\u001b[39m Trainer(XGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mxgb_params), models_folder\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCFG\u001b[38;5;241m.\u001b[39msample_sub_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/xgb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m oof_preds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m\"\u001b[39m], test_preds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m\"\u001b[39m], val_scores[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m\"\u001b[39m], train_scores[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m, in \u001b[0;36mTrainer.fit_predict\u001b[0;34m(self, X, y, X_test)\u001b[0m\n\u001b[1;32m     26\u001b[0m y_train, y_val \u001b[38;5;241m=\u001b[39m y[train_idx], y[val_idx]\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# ä¿å­˜æœ¬æŠ˜æ¨¡å‹\u001b[39;00m\n\u001b[1;32m     31\u001b[0m fold_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/predCryto/lib/python3.10/site-packages/xgboost/core.py:705\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    704\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/predCryto/lib/python3.10/site-packages/xgboost/sklearn.py:1222\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1217\u001b[0m model, metric, params, feature_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[1;32m   1218\u001b[0m     xgb_model, params, feature_weights\n\u001b[1;32m   1219\u001b[0m )\n\u001b[1;32m   1221\u001b[0m evals_result: TrainingCallback\u001b[38;5;241m.\u001b[39mEvalsLog \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1222\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1242\u001b[0m     obj: Optional[Objective] \u001b[38;5;241m=\u001b[39m _objective_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/predCryto/lib/python3.10/site-packages/xgboost/sklearn.py:628\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrap_evaluation_matrices\u001b[39m(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    609\u001b[0m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[1;32m    625\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m    626\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m    way.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     train_dmatrix \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m     n_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/predCryto/lib/python3.10/site-packages/xgboost/sklearn.py:1137\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbooster \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1137\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_bin\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/predCryto/lib/python3.10/site-packages/xgboost/core.py:705\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    704\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/predCryto/lib/python3.10/site-packages/xgboost/core.py:1590\u001b[0m, in \u001b[0;36mQuantileDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, max_quantile_batches, data_split_mode)\u001b[0m\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1571\u001b[0m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1583\u001b[0m         )\n\u001b[1;32m   1584\u001b[0m     ):\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1586\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf data iterator is used as input, data like label should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1587\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as batch argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1588\u001b[0m         )\n\u001b[0;32m-> 1590\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_quantile_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_quantile_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/predCryto/lib/python3.10/site-packages/xgboost/core.py:1656\u001b[0m, in \u001b[0;36mQuantileDMatrix._init\u001b[0;34m(self, data, ref, enable_categorical, max_quantile_blocks, **meta)\u001b[0m\n\u001b[1;32m   1654\u001b[0m it\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[0;32m-> 1656\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m handle\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ref \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/predCryto/lib/python3.10/site-packages/xgboost/core.py:310\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [22:08:45] /Users/runner/miniforge3/conda-bld/xgboost-split_1748292887431/work/src/data/gradient_index.h:100: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000015d67a0f0 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000015d81b618 void xgboost::GHistIndexMatrix::SetIndexData<xgboost::data::ColumnarAdapterBatch, unsigned int, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::ColumnarAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&), xgboost::data::IsValidFunctor&>(xgboost::common::Span<xgboost::data::IsValidFunctor&, 18446744073709551615ul>, unsigned long, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, unsigned long, xgboost::data::ColumnarAdapterBatch const&, xgboost::data::IsValidFunctor&, unsigned long, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::ColumnarAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&)&&) + 328\n  [bt] (2) 3   libxgboost.dylib                    0x000000015d81a1d0 void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::ColumnarAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>) + 404\n  [bt] (3) 4   libxgboost.dylib                    0x000000015d819cc4 void xgboost::GHistIndexMatrix::PushAdapterBatch<xgboost::data::ColumnarAdapterBatch>(xgboost::Context const*, unsigned long, unsigned long, xgboost::data::ColumnarAdapterBatch const&, float, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, double, unsigned long long) + 300\n  [bt] (4) 5   libxgboost.dylib                    0x000000015d821520 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 2404\n  [bt] (5) 6   libxgboost.dylib                    0x000000015d82075c xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int, long long) + 828\n  [bt] (6) 7   libxgboost.dylib                    0x000000015d7c45e0 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int, long long) + 152\n  [bt] (7) 8   libxgboost.dylib                    0x000000015d683f6c XGQuantileDMatrixCreateFromCallback + 508\n  [bt] (8) 9   libffi.8.dylib                      0x0000000101c20050 ffi_call_SYSV + 80\n\n"
     ]
    }
   ],
   "source": [
    "xgb_trainer = Trainer(XGBRegressor(**xgb_params), models_folder= f\"{CFG.sample_sub_path}/xgb\")\n",
    "\n",
    "oof_preds[\"XGBoost\"], test_preds[\"XGBoost\"], val_scores[\"XGBoost\"], train_scores[\"XGBoost\"] = xgb_trainer.fit_predict(X, y, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d1a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ens = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'prediction': test_preds[\"Ridge\"]\n",
    "})\n",
    "ens_name = 'submission_try_umap.csv'\n",
    "sub_ens.to_csv(os.path.join(CFG.sample_sub_path, ens_name), index=False)\n",
    "print(f\"ğŸ“¦ Saved ensemble submission to {ens_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95e5890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Ridge\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m ridge_model \u001b[38;5;241m=\u001b[39m Ridge(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mCFG\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m      2\u001b[0m ridge_trainer \u001b[38;5;241m=\u001b[39m Trainer(ridge_model, models_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCFG\u001b[38;5;241m.\u001b[39msample_sub_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/ridge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m oof_preds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRidge\u001b[39m\u001b[38;5;124m\"\u001b[39m], test_preds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRidge\u001b[39m\u001b[38;5;124m\"\u001b[39m], val_scores[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRidge\u001b[39m\u001b[38;5;124m\"\u001b[39m], train_scores[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRidge\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mridge_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m, in \u001b[0;36mTrainer.fit_predict\u001b[0;34m(self, X, y, X_test)\u001b[0m\n\u001b[1;32m     26\u001b[0m y_train, y_val \u001b[38;5;241m=\u001b[39m y[train_idx], y[val_idx]\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# ä¿å­˜æœ¬æŠ˜æ¨¡å‹\u001b[39;00m\n\u001b[1;32m     31\u001b[0m fold_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/predCryto/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/predCryto/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:1239\u001b[0m, in \u001b[0;36mRidge.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1237\u001b[0m _accept_sparse \u001b[38;5;241m=\u001b[39m _get_valid_accept_sparse(sparse\u001b[38;5;241m.\u001b[39missparse(X), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver)\n\u001b[1;32m   1238\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X, y, sample_weight)\n\u001b[0;32m-> 1239\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_accept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/predCryto/lib/python3.10/site-packages/sklearn/utils/validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/predCryto/lib/python3.10/site-packages/sklearn/utils/validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[0;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/predCryto/lib/python3.10/site-packages/sklearn/utils/validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/predCryto/lib/python3.10/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/predCryto/lib/python3.10/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "ridge_model = Ridge(alpha=1.0, random_state=CFG.seed)\n",
    "ridge_trainer = Trainer(ridge_model, models_folder=f\"{CFG.sample_sub_path}/ridge\")\n",
    "oof_preds[\"Ridge\"], test_preds[\"Ridge\"], val_scores[\"Ridge\"], train_scores[\"Ridge\"] = ridge_trainer.fit_predict(X, y, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f6a408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Ridge\n",
      "\n",
      "ğŸ“¦ Saved fold 0 model to output/k-fold/other_selected_25/ensemble//ridge/fold0/Ridge.pkl\n",
      "--- Fold 0 - val_pearson: 0.158685 - train_pearson: 0.141299\n",
      "ğŸ“¦ Saved fold 1 model to output/k-fold/other_selected_25/ensemble//ridge/fold1/Ridge.pkl\n",
      "--- Fold 1 - val_pearson: 0.083304 - train_pearson: 0.159373\n",
      "ğŸ“¦ Saved fold 2 model to output/k-fold/other_selected_25/ensemble//ridge/fold2/Ridge.pkl\n",
      "--- Fold 2 - val_pearson: 0.057616 - train_pearson: 0.168464\n",
      "ğŸ“¦ Saved fold 3 model to output/k-fold/other_selected_25/ensemble//ridge/fold3/Ridge.pkl\n",
      "--- Fold 3 - val_pearson: 0.122938 - train_pearson: 0.148199\n",
      "ğŸ“¦ Saved fold 4 model to output/k-fold/other_selected_25/ensemble//ridge/fold4/Ridge.pkl\n",
      "--- Fold 4 - val_pearson: 0.100707 - train_pearson: 0.168487\n",
      "\n",
      "------ Overall Score: 0.093739 - Mean val_pearson: 0.104650 Â± 0.034461 - Mean train_pearson: 0.157165 Â± 0.010889 ------\n",
      "\n",
      "Training CatBoostRegressor\n",
      "\n",
      "ğŸ“¦ Saved fold 0 model to output/k-fold/other_selected_25/ensemble//catboost/fold0/CatBoostRegressor.pkl\n",
      "--- Fold 0 - val_pearson: 0.107524 - train_pearson: 0.897621\n",
      "ğŸ“¦ Saved fold 1 model to output/k-fold/other_selected_25/ensemble//catboost/fold1/CatBoostRegressor.pkl\n",
      "--- Fold 1 - val_pearson: 0.098105 - train_pearson: 0.897984\n",
      "ğŸ“¦ Saved fold 2 model to output/k-fold/other_selected_25/ensemble//catboost/fold2/CatBoostRegressor.pkl\n",
      "--- Fold 2 - val_pearson: 0.009265 - train_pearson: 0.893905\n",
      "ğŸ“¦ Saved fold 3 model to output/k-fold/other_selected_25/ensemble//catboost/fold3/CatBoostRegressor.pkl\n",
      "--- Fold 3 - val_pearson: 0.131843 - train_pearson: 0.897699\n",
      "ğŸ“¦ Saved fold 4 model to output/k-fold/other_selected_25/ensemble//catboost/fold4/CatBoostRegressor.pkl\n",
      "--- Fold 4 - val_pearson: 0.095506 - train_pearson: 0.898291\n",
      "\n",
      "------ Overall Score: 0.085098 - Mean val_pearson: 0.088449 Â± 0.041617 - Mean train_pearson: 0.897100 Â± 0.001615 ------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, BayesianRidge\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0, random_state=CFG.seed)\n",
    "catboost_model = CatBoostRegressor(verbose=0, random_state=CFG.seed)\n",
    "bayes_model = BayesianRidge()\n",
    "\n",
    "ridge_trainer = Trainer(ridge_model, models_folder=f\"{CFG.sample_sub_path}/ridge\")\n",
    "catboost_trainer = Trainer(catboost_model, models_folder=f\"{CFG.sample_sub_path}/catboost\")\n",
    "bayes_trainer = Trainer(bayes_model, models_folder=f\"{CFG.sample_sub_path}/bayesridge\")\n",
    "\n",
    "# åŸ·è¡Œè¨“ç·´èˆ‡é æ¸¬\n",
    "oof_preds[\"Ridge\"], test_preds[\"Ridge\"], val_scores[\"Ridge\"], train_scores[\"Ridge\"] = ridge_trainer.fit_predict(X, y, X_test)\n",
    "oof_preds[\"CatBoost\"], test_preds[\"CatBoost\"], val_scores[\"CatBoost\"], train_scores[\"CatBoost\"] = catboost_trainer.fit_predict(X, y, X_test)\n",
    "oof_preds[\"BayesRidge\"], test_preds[\"BayesRidge\"], scores[\"BayesRidge\"] = bayes_trainer.fit_predict(X, y, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac925a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize predictions to zero-mean before ensembling\n",
    "def normalize_preds(preds_dict):\n",
    "    return {k: v - np.mean(v) for k, v in preds_dict.items()}\n",
    "nor_oof_preds = normalize_preds(oof_preds)\n",
    "nor_test_preds = normalize_preds(test_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from catboost import CatBoostRegressor\n",
    "from scripts.config import CFG\n",
    "import json\n",
    "\n",
    "\n",
    "def save_config_as_json(params_dict, selected_feature_sets, seeds, save_dir):\n",
    "    config = {\n",
    "        \"model_params\": params_dict,\n",
    "        \"selected_feature_sets\": selected_feature_sets,\n",
    "        \"seeds\": seeds\n",
    "    }\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with open(os.path.join(save_dir, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "def run_all_models_with_seeds(train_df, test_df, target_col, selected_feature_sets: dict, seeds: list,\n",
    "                              lgbm_params, lgbm_goss_params, xgb_params, catboost_params):\n",
    "\n",
    "    # ä¿å­˜åƒæ•¸èˆ‡è¨­å®š\n",
    "    save_config_as_json({\n",
    "        \"lgbm_params\": lgbm_params,\n",
    "        \"lgbm_goss_params\": lgbm_goss_params,\n",
    "        \"xgb_params\": xgb_params,\n",
    "        \"catboost_params\": catboost_params\n",
    "    }, selected_feature_sets, seeds, CFG.sample_sub_path)\n",
    "\n",
    "    ens_train_frames = []\n",
    "    ens_test_frames = []\n",
    "\n",
    "    for feat_name, selected_features in selected_feature_sets.items():\n",
    "        for seed in seeds:\n",
    "            folder = os.path.join(CFG.sample_sub_path, f\"{feat_name}/{seed}\")\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "            train = train_df[selected_features + [target_col]].copy()\n",
    "            test = test_df[selected_features].copy()\n",
    "            X = train.drop(target_col, axis=1)\n",
    "            y = train[target_col]\n",
    "            X_test = test\n",
    "\n",
    "            oof_preds, test_preds = {}, {}\n",
    "\n",
    "            # å‹•æ…‹æ›´æ–° seed åˆ°åƒæ•¸\n",
    "            lgbm_params_seeded = lgbm_params.copy()\n",
    "            lgbm_params_seeded[\"random_state\"] = seed\n",
    "\n",
    "            lgbm_goss_params_seeded = lgbm_goss_params.copy()\n",
    "            lgbm_goss_params_seeded[\"random_state\"] = seed\n",
    "\n",
    "            xgb_params_seeded = xgb_params.copy()\n",
    "            xgb_params_seeded[\"random_state\"] = seed\n",
    "            \n",
    "            catboost_params_seeded = catboost_params.copy()\n",
    "            catboost_params_seeded[\"random_state\"] = seed\n",
    "            \n",
    "            models = {\n",
    "                \"lgbm_gbdt\": LGBMRegressor(**lgbm_params_seeded),\n",
    "                \"lgbm_goss\": LGBMRegressor(**lgbm_goss_params_seeded),\n",
    "                \"xgb\": XGBRegressor(**xgb_params_seeded),\n",
    "                \"ridge\": Ridge(random_state=seed),\n",
    "                \"catboost\": CatBoostRegressor(**catboost_params_seeded),\n",
    "            }\n",
    "\n",
    "\n",
    "            for name, model in models.items():\n",
    "                model_folder = os.path.join(folder, name)\n",
    "                trainer = Trainer(model, models_folder=model_folder)\n",
    "                oof, test_pred, _, _ = trainer.fit_predict(X, y, X_test)\n",
    "                oof_preds[name] = oof\n",
    "                test_preds[name] = test_pred\n",
    "\n",
    "            # Normalize predictions\n",
    "            def normalize(preds):\n",
    "                return {k: v - np.mean(v) for k, v in preds.items()}\n",
    "\n",
    "            norm_oof = normalize(oof_preds)\n",
    "            norm_test = normalize(test_preds)\n",
    "\n",
    "            oof_df = pd.DataFrame({\n",
    "                f\"{k}_{feat_name}_seed{seed}\": v for k, v in norm_oof.items()\n",
    "            })\n",
    "\n",
    "            test_df_part = pd.DataFrame({\n",
    "                f\"{k}_{feat_name}_seed{seed}\": v for k, v in norm_test.items()\n",
    "            })\n",
    "\n",
    "            ens_train_frames.append(oof_df)\n",
    "            ens_test_frames.append(test_df_part)\n",
    "\n",
    "    X_ens = pd.concat(ens_train_frames, axis=1)\n",
    "    X_test_ens = pd.concat(ens_test_frames, axis=1)\n",
    "    return X_ens, X_test_ens\n",
    "\n",
    "\n",
    "CFG.sample_sub_path = \"output/k-fold/all_different_data_sand_seed/ensemble/\"\n",
    "\n",
    "other_selected_25 = list(dict.fromkeys([\n",
    "    \"X863\", \"X856\", \"X344\", \"X598\", \"X862\", \"X385\", \"X852\", \"X603\", \"X860\", \"X674\",\n",
    "    \"X415\", \"X345\", \"X137\", \"X855\", \"X174\", \"X302\", \"X178\", \"X532\", \"X168\", \"X612\",\n",
    "    \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\", \"volume\"\n",
    "]))\n",
    "best_21 = list(dict.fromkeys([\n",
    "    'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume',\n",
    "    'X695', 'X21', 'X603', 'X855', 'X285', 'X344', 'X345',\n",
    "    'X444', 'X532', 'X856', 'X862', 'X863', 'X873', 'X887','X40', 'X861',]))\n",
    "best_21_and_others = list(dict.fromkeys([\n",
    "    'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume',\n",
    "    'X695', 'X21', 'X603', 'X855', 'X285', 'X344', 'X345',\n",
    "    'X444', 'X532', 'X856', 'X862', 'X863', 'X873', 'X887','X40', 'X861',\n",
    "    'X598', 'X385', 'X168', 'X174', 'X178', 'X302', 'X415', 'X612', 'X852', 'X860', 'X674', 'X137']))\n",
    "\n",
    "\n",
    "X, X_test = run_all_models_with_seeds(train, test, CFG.target, {\n",
    "    \"other_selected_25\": other_selected_25,\n",
    "    \"selected_21\"   : best_21,\n",
    "    \"other&my_selected_33\": best_21_and_others\n",
    "}, seeds=[42, 1337, 1024], lgbm_params=lgbm_params, lgbm_goss_params=lgbm_goss_params, xgb_params=xgb_params, catboost_params=catboost_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c4110b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 17:37:21,303] A new study created in memory with name: no-name-9c25bbaa-60d9-4d8d-92b0-08655006e825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Tuning LGBM (gbdt)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 17:37:56,193] Trial 0 finished with value: -0.2665519214689309 and parameters: {'learning_rate': 0.001657932720775217, 'num_leaves': 35, 'max_depth': 13, 'min_child_samples': 18, 'min_child_weight': 0.00835840364628969, 'subsample': 0.34277328240621585, 'colsample_bytree': 0.46423490143339535, 'reg_alpha': 6.974970696338066, 'reg_lambda': 34.25138210663246, 'n_estimators': 288}. Best is trial 0 with value: -0.2665519214689309.\n",
      "[I 2025-05-31 17:39:40,966] Trial 1 finished with value: -0.3615868409906824 and parameters: {'learning_rate': 0.003966135442950583, 'num_leaves': 44, 'max_depth': 8, 'min_child_samples': 25, 'min_child_weight': 0.0015157000752770153, 'subsample': 0.4299968122912713, 'colsample_bytree': 0.9793743214547576, 'reg_alpha': 11.334528844804153, 'reg_lambda': 38.00436867219635, 'n_estimators': 887}. Best is trial 0 with value: -0.2665519214689309.\n",
      "[I 2025-05-31 17:44:59,816] Trial 2 finished with value: -0.5988195457125005 and parameters: {'learning_rate': 0.0071883222427213305, 'num_leaves': 63, 'max_depth': 16, 'min_child_samples': 25, 'min_child_weight': 0.4929102416359851, 'subsample': 0.5193246363487476, 'colsample_bytree': 0.5516635683119404, 'reg_alpha': 8.960554020740133, 'reg_lambda': 0.6600488746840276, 'n_estimators': 1745}. Best is trial 0 with value: -0.2665519214689309.\n",
      "[I 2025-05-31 17:45:56,962] Trial 3 finished with value: -0.5359743772642661 and parameters: {'learning_rate': 0.02650552482306855, 'num_leaves': 21, 'max_depth': 16, 'min_child_samples': 12, 'min_child_weight': 0.0032510607380099453, 'subsample': 0.36959443531904157, 'colsample_bytree': 0.92215077154665, 'reg_alpha': 13.464175951939021, 'reg_lambda': 14.356433695510232, 'n_estimators': 793}. Best is trial 0 with value: -0.2665519214689309.\n",
      "[I 2025-05-31 17:49:21,084] Trial 4 finished with value: -0.43947829919023496 and parameters: {'learning_rate': 0.0028352227396895575, 'num_leaves': 35, 'max_depth': 12, 'min_child_samples': 28, 'min_child_weight': 0.0019432437283942907, 'subsample': 0.9003083959803967, 'colsample_bytree': 0.7578460805592877, 'reg_alpha': 2.7265972721176466, 'reg_lambda': 2.1755476223252526, 'n_estimators': 1805}. Best is trial 0 with value: -0.2665519214689309.\n",
      "[I 2025-05-31 17:53:26,184] Trial 5 finished with value: -0.3872909279237664 and parameters: {'learning_rate': 0.0026798535528650214, 'num_leaves': 37, 'max_depth': 15, 'min_child_samples': 23, 'min_child_weight': 0.009354921046662804, 'subsample': 0.5871718451246855, 'colsample_bytree': 0.4697589870658971, 'reg_alpha': 22.895833437623956, 'reg_lambda': 39.57887008954687, 'n_estimators': 1929}. Best is trial 0 with value: -0.2665519214689309.\n",
      "[I 2025-05-31 17:54:45,999] Trial 6 finished with value: -0.27062979542201926 and parameters: {'learning_rate': 0.0015646494009618295, 'num_leaves': 46, 'max_depth': 7, 'min_child_samples': 7, 'min_child_weight': 0.004825237523200932, 'subsample': 0.6782698192307918, 'colsample_bytree': 0.8067879245952171, 'reg_alpha': 65.69578762503583, 'reg_lambda': 0.4135422450442706, 'n_estimators': 531}. Best is trial 0 with value: -0.2665519214689309.\n",
      "[I 2025-05-31 17:57:02,178] Trial 7 finished with value: -0.590744776529582 and parameters: {'learning_rate': 0.024804798547183045, 'num_leaves': 37, 'max_depth': 5, 'min_child_samples': 23, 'min_child_weight': 0.7522982012407217, 'subsample': 0.878055326552351, 'colsample_bytree': 0.9949122142655191, 'reg_alpha': 2.4870205740820372, 'reg_lambda': 45.6249793836967, 'n_estimators': 1570}. Best is trial 0 with value: -0.2665519214689309.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31måœ¨ç›®å‰å„²å­˜æ ¼æˆ–ä¸Šä¸€å€‹å„²å­˜æ ¼ä¸­åŸ·è¡Œç¨‹å¼ç¢¼æ™‚ï¼ŒKernel å·²ææ¯€ã€‚\n",
      "\u001b[1;31mè«‹æª¢é–±å„²å­˜æ ¼ä¸­çš„ç¨‹å¼ç¢¼ï¼Œæ‰¾å‡ºå¤±æ•—çš„å¯èƒ½åŸå› ã€‚\n",
      "\u001b[1;31må¦‚éœ€è©³ç´°è³‡è¨Šï¼Œè«‹æŒ‰ä¸€ä¸‹<a href='https://aka.ms/vscodeJupyterKernelCrash'>é€™è£¡</a>ã€‚\n",
      "\u001b[1;31må¦‚éœ€è©³ç´°è³‡æ–™ï¼Œè«‹æª¢è¦– Jupyter <a href='command:jupyter.viewOutput'>è¨˜éŒ„</a>ã€‚"
     ]
    }
   ],
   "source": [
    "# Step 1: ç‚ºæ¯çµ„ç‰¹å¾µèª¿åƒ\n",
    "\n",
    "import optuna\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def tune_all_models(train_df, target_col, selected_features, n_trials=30):\n",
    "    X = train_df[selected_features]\n",
    "    y = train_df[target_col]\n",
    "\n",
    "    def evaluate_model(model, X, y, n_splits=5, alpha=1):\n",
    "        kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "        val_scores, train_scores = [], []\n",
    "        for train_idx, val_idx in kf.split(X):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            val_pred = model.predict(X_val)\n",
    "            train_pred = model.predict(X_train)\n",
    "            val_scores.append(pearsonr(val_pred, y_val)[0])\n",
    "            train_scores.append(pearsonr(train_pred, y_train)[0])\n",
    "        val_mean = np.mean(val_scores)\n",
    "        train_mean = np.mean(train_scores)\n",
    "        score = val_mean - alpha * (train_mean - val_mean)\n",
    "        return score\n",
    "\n",
    "\n",
    "    def objective_lgbm(trial):\n",
    "        params = {\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.05, log=True),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 15, 64),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 16),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 30),\n",
    "            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-3, 1.0, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.3, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 1.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.1, 100.0, log=True),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.1, 100.0, log=True),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 2000),\n",
    "            \"random_state\": 42,\n",
    "            \"n_jobs\": -1,\n",
    "            \"verbose\": -1\n",
    "        }\n",
    "        model = LGBMRegressor(**params)\n",
    "        return evaluate_model(model, X, y)\n",
    "\n",
    "    def objective_lgbm_goss(trial):\n",
    "        params = {\n",
    "            \"boosting_type\": \"goss\",\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.05, log=True),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 30),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 30),\n",
    "            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-3, 1.0, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.2, 0.8),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 1.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.1, 50.0, log=True),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.1, 50.0, log=True),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 2000),\n",
    "            \"random_state\": 42,\n",
    "            \"n_jobs\": -1,\n",
    "            \"verbose\": -1\n",
    "        }\n",
    "        model = LGBMRegressor(**params)\n",
    "        return evaluate_model(model, X, y)\n",
    "\n",
    "    def objective_xgb(trial):\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.05, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 16),\n",
    "            \"max_leaves\": trial.suggest_int(\"max_leaves\", 4, 32),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 30),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.3, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 1.0),\n",
    "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.3, 1.0),\n",
    "            \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.3, 1.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.1, 100.0, log=True),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.1, 100.0, log=True),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 2000),\n",
    "            \"n_jobs\": -1,\n",
    "            \"random_state\": 42,\n",
    "            \"verbosity\": 0,\n",
    "        }\n",
    "        model = XGBRegressor(**params)\n",
    "        return evaluate_model(model, X, y)\n",
    "\n",
    "    def objective_catboost(trial):\n",
    "        params = {\n",
    "            \"iterations\": trial.suggest_int(\"iterations\", 100, 2000),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 100.0, log=True),\n",
    "            \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.3, 1.0),\n",
    "            \"rsm\": trial.suggest_float(\"rsm\", 0.3, 1.0),\n",
    "            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 30),\n",
    "            \"random_strength\": trial.suggest_float(\"random_strength\", 1.0, 50.0),\n",
    "            \"early_stopping_rounds\": 30,\n",
    "            \"verbose\": 0,\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "        model = CatBoostRegressor(**params)\n",
    "        return evaluate_model(model, X, y)\n",
    "\n",
    "    print(\"ğŸ” Tuning LGBM (gbdt)...\")\n",
    "    lgbm_study = optuna.create_study(direction=\"maximize\")\n",
    "    lgbm_study.optimize(objective_lgbm, n_trials=n_trials)\n",
    "\n",
    "    print(\"ğŸ” Tuning LGBM (goss)...\")\n",
    "    goss_study = optuna.create_study(direction=\"maximize\")\n",
    "    goss_study.optimize(objective_lgbm_goss, n_trials=n_trials)\n",
    "\n",
    "    print(\"ğŸ” Tuning XGBoost...\")\n",
    "    xgb_study = optuna.create_study(direction=\"maximize\")\n",
    "    xgb_study.optimize(objective_xgb, n_trials=n_trials)\n",
    "\n",
    "    print(\"ğŸ” Tuning CatBoost...\")\n",
    "    cat_study = optuna.create_study(direction=\"maximize\")\n",
    "    cat_study.optimize(objective_catboost, n_trials=n_trials)\n",
    "\n",
    "    return {\n",
    "        \"lgbm_params\": lgbm_study.best_params,\n",
    "        \"lgbm_goss_params\": goss_study.best_params,\n",
    "        \"xgb_params\": xgb_study.best_params,\n",
    "        \"catboost_params\": cat_study.best_params\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "all_best_params = {\n",
    "    name: tune_all_models(train_df=train, target_col=CFG.target, selected_features=feat_list)\n",
    "    for name, feat_list in {\n",
    "    \"selected_21\"   : best_21,\n",
    "    \"other&my_selected_33\": best_21_and_others\n",
    "    }.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31019486",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_best_params[\"other_selected_25\"] = {\n",
    "    \"lgbm_params\": {\n",
    "        \"learning_rate\": 0.001637813850082008,\n",
    "        \"num_leaves\": 10,\n",
    "        \"max_depth\": 11,\n",
    "        \"min_child_samples\": 5,\n",
    "        \"min_child_weight\": 0.016271558733787436,\n",
    "        \"subsample\": 0.6593613083987416,\n",
    "        \"colsample_bytree\": 0.4575721609314488,\n",
    "        \"reg_alpha\": 1.699018742860479,\n",
    "        \"reg_lambda\": 0.25754595543778913,\n",
    "        \"n_estimators\": 323,\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"verbose\": -1,\n",
    "    },\n",
    "    \"lgbm_goss_params\": {\n",
    "        \"learning_rate\": 0.0010158433360328654,\n",
    "        \"num_leaves\": 15,\n",
    "        \"max_depth\": 4,\n",
    "        \"min_child_samples\": 22,\n",
    "        \"min_child_weight\": 0.0011960681689680813,\n",
    "        \"subsample\": 0.9788004638386685,\n",
    "        \"colsample_bytree\": 0.4336760144773911,\n",
    "        \"reg_alpha\": 0.4229242901001258,\n",
    "        \"reg_lambda\": 94.77537152236522,\n",
    "        \"n_estimators\": 112,\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "        \"boosting_type\": \"goss\",\n",
    "        \"verbose\": -1,\n",
    "    },\n",
    "    \"xgb_params\": {\n",
    "        \"learning_rate\": 0.001889595449145299,\n",
    "        \"max_depth\": 5,\n",
    "        \"max_leaves\": 4,\n",
    "        \"min_child_weight\": 19,\n",
    "        \"gamma\": 0.4730991659940904,\n",
    "        \"subsample\": 0.9997261759454115,\n",
    "        \"colsample_bytree\": 0.3180029675660578,\n",
    "        \"colsample_bylevel\": 0.8413518907705161,\n",
    "        \"colsample_bynode\": 0.33111438641088414,\n",
    "        \"reg_alpha\": 39.861565568274074,\n",
    "        \"reg_lambda\": 0.10988448724798101,\n",
    "        \"n_estimators\": 711,\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "        \"verbosity\": 0\n",
    "    },\n",
    "    \"catboost_params\": {\n",
    "        \"iterations\": 144,\n",
    "        \"depth\": 4,\n",
    "        \"learning_rate\": 0.011809472839755976,\n",
    "        \"l2_leaf_reg\": 28.723858155851214,\n",
    "        \"bagging_temperature\": 0.5266938712453599,\n",
    "        \"subsample\": 0.46783583928704287,\n",
    "        \"rsm\": 0.8149725374297446,\n",
    "        \"min_data_in_leaf\": 25,\n",
    "        \"random_strength\": 23.183405644995034,\n",
    "        \"early_stopping_rounds\": 30,\n",
    "        \"verbose\": 0,\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea9ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_models_with_seeds_from_best_params(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    target_col,\n",
    "    selected_feature_sets: dict,\n",
    "    seeds: list,\n",
    "    all_best_params: dict  # <-- å‚³å…¥æ¯å€‹ feature_set çš„æœ€ä½³åƒæ•¸\n",
    "):\n",
    "    ens_train_frames = []\n",
    "    ens_test_frames = []\n",
    "\n",
    "    for feat_name, selected_features in selected_feature_sets.items():\n",
    "        best_params = all_best_params[feat_name]\n",
    "\n",
    "        # ä¿å­˜è©² feature_set çš„è¨­å®š\n",
    "        save_config_as_json(\n",
    "            best_params,\n",
    "            {feat_name: selected_features},\n",
    "            seeds,\n",
    "            os.path.join(CFG.sample_sub_path, feat_name)\n",
    "        )\n",
    "\n",
    "        for seed in seeds:\n",
    "            folder = os.path.join(CFG.sample_sub_path, f\"{feat_name}/{seed}\")\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "            train = train_df[selected_features + [target_col]].copy()\n",
    "            test = test_df[selected_features].copy()\n",
    "            X = train.drop(target_col, axis=1)\n",
    "            y = train[target_col]\n",
    "            X_test = test\n",
    "\n",
    "            oof_preds, test_preds = {}, {}\n",
    "\n",
    "            # æ³¨å…¥å°æ‡‰ seed\n",
    "            lgbm_params_seeded = best_params[\"lgbm_params\"].copy()\n",
    "            lgbm_params_seeded[\"random_state\"] = seed\n",
    "\n",
    "            lgbm_goss_params_seeded = best_params[\"lgbm_goss_params\"].copy()\n",
    "            lgbm_goss_params_seeded[\"random_state\"] = seed\n",
    "\n",
    "            xgb_params_seeded = best_params[\"xgb_params\"].copy()\n",
    "            xgb_params_seeded[\"random_state\"] = seed\n",
    "\n",
    "            catboost_params_seeded = best_params[\"catboost_params\"].copy()\n",
    "            catboost_params_seeded[\"random_state\"] = seed\n",
    "\n",
    "            models = {\n",
    "                \"lgbm_gbdt\": LGBMRegressor(**lgbm_params_seeded),\n",
    "                \"lgbm_goss\": LGBMRegressor(**lgbm_goss_params_seeded),\n",
    "                \"xgb\": XGBRegressor(**xgb_params_seeded),\n",
    "                \"ridge\": Ridge(random_state=seed),\n",
    "                \"catboost\": CatBoostRegressor(**catboost_params_seeded),\n",
    "            }\n",
    "\n",
    "            for name, model in models.items():\n",
    "                model_folder = os.path.join(folder, name)\n",
    "                trainer = Trainer(model, models_folder=model_folder)\n",
    "                oof, test_pred, _, _ = trainer.fit_predict(X, y, X_test)\n",
    "                oof_preds[name] = oof\n",
    "                test_preds[name] = test_pred\n",
    "\n",
    "            # Normalize predictions\n",
    "            def normalize(preds):\n",
    "                return {k: v - np.mean(v) for k, v in preds.items()}\n",
    "\n",
    "            norm_oof = normalize(oof_preds)\n",
    "            norm_test = normalize(test_preds)\n",
    "\n",
    "            oof_df = pd.DataFrame({\n",
    "                f\"{k}_{feat_name}_seed{seed}\": v for k, v in norm_oof.items()\n",
    "            })\n",
    "\n",
    "            test_df_part = pd.DataFrame({\n",
    "                f\"{k}_{feat_name}_seed{seed}\": v for k, v in norm_test.items()\n",
    "            })\n",
    "\n",
    "            ens_train_frames.append(oof_df)\n",
    "            ens_test_frames.append(test_df_part)\n",
    "\n",
    "    X_ens = pd.concat(ens_train_frames, axis=1)\n",
    "    X_test_ens = pd.concat(ens_test_frames, axis=1)\n",
    "    return X_ens, X_test_ens\n",
    "\n",
    "\n",
    "\n",
    "CFG.sample_sub_path = \"output/k-fold/all_different_data_sand_seed_v2/ensemble/\"\n",
    "X, X_test = run_all_models_with_seeds_from_best_params(\n",
    "    train, test, CFG.target,\n",
    "    {\n",
    "        \"other_selected_25\": other_selected_25,\n",
    "        \"selected_21\": best_21,\n",
    "        \"other&my_selected_33\": best_21_and_others\n",
    "    },\n",
    "    seeds=[42, 1337, 1024],\n",
    "    all_best_params=all_best_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(nor_oof_preds)\n",
    "X_test = pd.DataFrame(nor_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da78e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å„²å­˜è³‡æ–™\n",
    "X.to_parquet(os.path.join(CFG.sample_sub_path, \"stacked_train_X.parquet\"), index=False)\n",
    "X_test.to_parquet(os.path.join(CFG.sample_sub_path, \"stacked_test_X.parquet\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987098e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:25:19,408] A new study created in memory with name: no-name-88d3063c-ba61-42db-b318-3690a0429721\n",
      "[I 2025-05-31 04:25:26,535] Trial 1 finished with value: 0.13064979477501532 and parameters: {'alpha': 68.30976363852781, 'tol': 0.0007066106203514526, 'fit_intercept': False, 'positive': False}. Best is trial 1 with value: 0.13064979477501532.\n",
      "[I 2025-05-31 04:25:26,547] Trial 5 finished with value: 0.1427723543510045 and parameters: {'alpha': 672.8733603617318, 'tol': 4.148494110618029e-05, 'fit_intercept': True, 'positive': False}. Best is trial 5 with value: 0.1427723543510045.\n",
      "[I 2025-05-31 04:25:26,560] Trial 3 finished with value: 0.14270257063480707 and parameters: {'alpha': 667.0760734062089, 'tol': 0.007594589914168868, 'fit_intercept': True, 'positive': False}. Best is trial 5 with value: 0.1427723543510045.\n",
      "[I 2025-05-31 04:25:26,572] Trial 0 finished with value: 0.13860206879727455 and parameters: {'alpha': 385.5633850632768, 'tol': 0.006964927334096522, 'fit_intercept': True, 'positive': False}. Best is trial 5 with value: 0.1427723543510045.\n",
      "[I 2025-05-31 04:25:26,594] Trial 7 finished with value: 0.13272376903568586 and parameters: {'alpha': 138.04292598750368, 'tol': 0.003843708971375976, 'fit_intercept': False, 'positive': False}. Best is trial 5 with value: 0.1427723543510045.\n",
      "[I 2025-05-31 04:25:26,595] Trial 6 finished with value: 0.14046194490383299 and parameters: {'alpha': 532.3642783133832, 'tol': 0.006524367188551186, 'fit_intercept': False, 'positive': False}. Best is trial 5 with value: 0.1427723543510045.\n",
      "[I 2025-05-31 04:25:26,621] Trial 4 finished with value: 0.17080725506393254 and parameters: {'alpha': 942.4157244712944, 'tol': 0.007904897707516473, 'fit_intercept': True, 'positive': True}. Best is trial 4 with value: 0.17080725506393254.\n",
      "[I 2025-05-31 04:25:26,640] Trial 2 finished with value: 0.170599833893823 and parameters: {'alpha': 699.2649724468885, 'tol': 0.003812318450936095, 'fit_intercept': False, 'positive': True}. Best is trial 4 with value: 0.17080725506393254.\n",
      "[I 2025-05-31 04:25:33,492] Trial 8 finished with value: 0.14492443759777487 and parameters: {'alpha': 924.1218386958585, 'tol': 0.0025614901224287278, 'fit_intercept': False, 'positive': False}. Best is trial 4 with value: 0.17080725506393254.\n",
      "[I 2025-05-31 04:25:33,605] Trial 9 finished with value: 0.1409144418012728 and parameters: {'alpha': 531.0616351508285, 'tol': 0.006504298177304344, 'fit_intercept': True, 'positive': False}. Best is trial 4 with value: 0.17080725506393254.\n",
      "[I 2025-05-31 04:25:33,642] Trial 15 finished with value: 0.17059980189600296 and parameters: {'alpha': 554.2748701071819, 'tol': 0.0037196482202504137, 'fit_intercept': False, 'positive': True}. Best is trial 4 with value: 0.17080725506393254.\n",
      "[I 2025-05-31 04:25:33,663] Trial 14 finished with value: 0.17059973251414112 and parameters: {'alpha': 240.1674894378679, 'tol': 0.008676844611715329, 'fit_intercept': False, 'positive': True}. Best is trial 4 with value: 0.17080725506393254.\n",
      "[I 2025-05-31 04:25:33,663] Trial 13 finished with value: 0.1705998867038962 and parameters: {'alpha': 938.738647686095, 'tol': 0.0041618659581310995, 'fit_intercept': False, 'positive': True}. Best is trial 4 with value: 0.17080725506393254.\n",
      "[I 2025-05-31 04:25:33,677] Trial 11 finished with value: 0.17059971993915699 and parameters: {'alpha': 183.2785361274103, 'tol': 0.0047095269465263195, 'fit_intercept': False, 'positive': True}. Best is trial 4 with value: 0.17080725506393254.\n",
      "[I 2025-05-31 04:25:33,694] Trial 10 finished with value: 0.17059987846301133 and parameters: {'alpha': 901.3547410325284, 'tol': 0.004859278730297825, 'fit_intercept': False, 'positive': True}. Best is trial 4 with value: 0.17080725506393254.\n",
      "[I 2025-05-31 04:25:33,719] Trial 12 finished with value: 0.1708071749567826 and parameters: {'alpha': 319.0629518679493, 'tol': 0.009498812457676333, 'fit_intercept': True, 'positive': True}. Best is trial 4 with value: 0.17080725506393254.\n",
      "[I 2025-05-31 04:25:35,820] Trial 16 finished with value: 0.1705997999239767 and parameters: {'alpha': 545.3417877668527, 'tol': 0.00829512320389967, 'fit_intercept': False, 'positive': True}. Best is trial 4 with value: 0.17080725506393254.\n",
      "[I 2025-05-31 04:25:36,216] Trial 19 finished with value: 0.17080725721015358 and parameters: {'alpha': 959.1462517071269, 'tol': 0.009998298114292382, 'fit_intercept': True, 'positive': True}. Best is trial 19 with value: 0.17080725721015358.\n",
      "[I 2025-05-31 04:25:36,525] Trial 18 finished with value: 0.17080725500890354 and parameters: {'alpha': 941.986774869544, 'tol': 0.00861206913431812, 'fit_intercept': True, 'positive': True}. Best is trial 19 with value: 0.17080725721015358.\n",
      "[I 2025-05-31 04:25:36,563] Trial 23 finished with value: 0.17080725827813997 and parameters: {'alpha': 967.4721506426324, 'tol': 0.008354399455957392, 'fit_intercept': True, 'positive': True}. Best is trial 23 with value: 0.17080725827813997.\n",
      "[I 2025-05-31 04:25:36,601] Trial 22 finished with value: 0.17080725746149275 and parameters: {'alpha': 961.1056281678824, 'tol': 0.009621101310695764, 'fit_intercept': True, 'positive': True}. Best is trial 23 with value: 0.17080725827813997.\n",
      "[I 2025-05-31 04:25:36,601] Trial 17 finished with value: 0.17080726187737633 and parameters: {'alpha': 995.5342209691347, 'tol': 0.009500342246849272, 'fit_intercept': True, 'positive': True}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:36,612] Trial 20 finished with value: 0.17080722785092256 and parameters: {'alpha': 730.4158065983368, 'tol': 0.009577888402133548, 'fit_intercept': True, 'positive': True}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:36,643] Trial 21 finished with value: 0.17080724507464223 and parameters: {'alpha': 864.5662539820947, 'tol': 0.008447477414024478, 'fit_intercept': True, 'positive': True}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:40,689] Trial 24 finished with value: 0.17080724975771827 and parameters: {'alpha': 901.0586455438338, 'tol': 0.009401938398612314, 'fit_intercept': True, 'positive': True}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:40,714] Trial 25 finished with value: 0.17080726159741935 and parameters: {'alpha': 993.3513311371556, 'tol': 0.008990006894377983, 'fit_intercept': True, 'positive': True}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:42,019] Trial 31 finished with value: 0.14533276748651197 and parameters: {'alpha': 916.2122510767383, 'tol': 0.009922001944113143, 'fit_intercept': True, 'positive': False}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:42,034] Trial 27 finished with value: 0.14521716222696712 and parameters: {'alpha': 956.5259155341195, 'tol': 0.009573670785937566, 'fit_intercept': False, 'positive': False}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:42,069] Trial 30 finished with value: 0.17059986261593832 and parameters: {'alpha': 829.4813578981634, 'tol': 0.009563296305422542, 'fit_intercept': False, 'positive': True}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:42,088] Trial 28 finished with value: 0.17080723577601195 and parameters: {'alpha': 792.129557522339, 'tol': 0.004992126341036757, 'fit_intercept': True, 'positive': True}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:42,101] Trial 26 finished with value: 0.17080723430465003 and parameters: {'alpha': 780.6702619707067, 'tol': 0.006094034444596561, 'fit_intercept': True, 'positive': True}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:42,126] Trial 29 finished with value: 0.1705998927305008 and parameters: {'alpha': 966.0811295119004, 'tol': 0.009341709917115306, 'fit_intercept': False, 'positive': True}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:45,356] Trial 33 finished with value: 0.17059989417412744 and parameters: {'alpha': 972.6312394323495, 'tol': 0.008081578179829352, 'fit_intercept': False, 'positive': True}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:45,367] Trial 32 finished with value: 0.17059986605421237 and parameters: {'alpha': 845.0737313548475, 'tol': 0.007919756179212282, 'fit_intercept': False, 'positive': True}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:46,105] Trial 34 finished with value: 0.1457276750193554 and parameters: {'alpha': 959.8014971390687, 'tol': 0.006088106117887194, 'fit_intercept': True, 'positive': False}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:46,133] Trial 36 finished with value: 0.17080725437337616 and parameters: {'alpha': 937.0329318796275, 'tol': 0.004317357620160617, 'fit_intercept': True, 'positive': True}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:46,146] Trial 37 finished with value: 0.1708072604995873 and parameters: {'alpha': 984.7915329939359, 'tol': 0.007639987031786562, 'fit_intercept': True, 'positive': True}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:46,155] Trial 39 finished with value: 0.17080726017710465 and parameters: {'alpha': 982.2772126917166, 'tol': 0.007202031727984279, 'fit_intercept': True, 'positive': True}. Best is trial 17 with value: 0.17080726187737633.\n",
      "[I 2025-05-31 04:25:46,186] Trial 38 finished with value: 0.17080726231420734 and parameters: {'alpha': 998.9403477953053, 'tol': 0.009678239296649372, 'fit_intercept': True, 'positive': True}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:46,186] Trial 35 finished with value: 0.1708072177722316 and parameters: {'alpha': 651.9622252664008, 'tol': 0.00631690848701983, 'fit_intercept': True, 'positive': True}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:50,181] Trial 41 finished with value: 0.14570380975759312 and parameters: {'alpha': 957.1151101521118, 'tol': 0.006071201991157694, 'fit_intercept': True, 'positive': False}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:50,219] Trial 40 finished with value: 0.1708071663689482 and parameters: {'alpha': 252.36475305326363, 'tol': 0.001995121776355888, 'fit_intercept': True, 'positive': True}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:51,199] Trial 43 finished with value: 0.13118427570290897 and parameters: {'alpha': 72.58995593581852, 'tol': 0.0068412619578308445, 'fit_intercept': True, 'positive': False}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:51,375] Trial 44 finished with value: 0.17080725913978498 and parameters: {'alpha': 974.1897168576339, 'tol': 0.007427399370623408, 'fit_intercept': True, 'positive': True}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:51,396] Trial 45 finished with value: 0.1708071784174937 and parameters: {'alpha': 345.9478660616294, 'tol': 0.002826381558762293, 'fit_intercept': True, 'positive': True}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:51,407] Trial 46 finished with value: 0.17080725984728656 and parameters: {'alpha': 979.7057359434551, 'tol': 0.0096576995711833, 'fit_intercept': True, 'positive': True}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:51,408] Trial 47 finished with value: 0.17080713821876767 and parameters: {'alpha': 33.90684584312595, 'tol': 0.0024468831705008022, 'fit_intercept': True, 'positive': True}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:51,428] Trial 42 finished with value: 0.17080718156501112 and parameters: {'alpha': 370.4031867262672, 'tol': 0.0015041809940335425, 'fit_intercept': True, 'positive': True}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:53,918] Trial 48 finished with value: 0.17080726163753435 and parameters: {'alpha': 993.6641154905881, 'tol': 0.007141419903931688, 'fit_intercept': True, 'positive': True}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:53,930] Trial 49 finished with value: 0.17080725949982803 and parameters: {'alpha': 976.9967629045526, 'tol': 0.007003845275507179, 'fit_intercept': True, 'positive': True}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:55,673] Trial 51 finished with value: 0.17080725791567525 and parameters: {'alpha': 964.6463743553214, 'tol': 0.007455455601074621, 'fit_intercept': True, 'positive': True}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:55,699] Trial 55 finished with value: 0.17080725747766212 and parameters: {'alpha': 961.231681226405, 'tol': 0.006803657536581002, 'fit_intercept': True, 'positive': True}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:55,711] Trial 52 finished with value: 0.17080725326638618 and parameters: {'alpha': 928.4044323459063, 'tol': 0.006063812890078827, 'fit_intercept': True, 'positive': True}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:55,743] Trial 53 finished with value: 0.1708072618359629 and parameters: {'alpha': 995.2113093194431, 'tol': 0.008170644262588664, 'fit_intercept': True, 'positive': True}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:55,757] Trial 50 finished with value: 0.17080726073544283 and parameters: {'alpha': 986.6304646218808, 'tol': 0.006578013852010645, 'fit_intercept': True, 'positive': True}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:55,777] Trial 54 finished with value: 0.17080725521968923 and parameters: {'alpha': 943.6298484991017, 'tol': 0.007754465998349828, 'fit_intercept': True, 'positive': True}. Best is trial 38 with value: 0.17080726231420734.\n",
      "[I 2025-05-31 04:25:58,401] Trial 57 finished with value: 0.17080726241106634 and parameters: {'alpha': 999.695600649308, 'tol': 0.009977931259283345, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:25:58,442] Trial 56 finished with value: 0.17080725786370515 and parameters: {'alpha': 964.2412184025657, 'tol': 0.009860518873519162, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:25:58,541] Trial 60 finished with value: 0.17080726193881193 and parameters: {'alpha': 996.0132524656296, 'tol': 0.007856205356307117, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:25:58,968] Trial 59 finished with value: 0.1708072613632121 and parameters: {'alpha': 991.5251824446187, 'tol': 0.008540304662539723, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:25:59,058] Trial 63 finished with value: 0.16829207495993934 and parameters: {'alpha': 892.6557562620571, 'tol': 0.0006742176929760701, 'fit_intercept': False, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:25:59,085] Trial 58 finished with value: 0.17080726063609458 and parameters: {'alpha': 985.8558581513452, 'tol': 0.006795431206593502, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:25:59,096] Trial 62 finished with value: 0.17080724175623485 and parameters: {'alpha': 838.7123831949982, 'tol': 0.009999080316086971, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:25:59,117] Trial 61 finished with value: 0.17080725988247328 and parameters: {'alpha': 979.9800728382411, 'tol': 0.009754395279044608, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:03,882] Trial 66 finished with value: 0.14210598333540145 and parameters: {'alpha': 657.1901130953795, 'tol': 0.001507461974072782, 'fit_intercept': False, 'positive': False}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:04,017] Trial 65 finished with value: 0.17080726120737713 and parameters: {'alpha': 990.3101246599585, 'tol': 0.008878742025167431, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:04,042] Trial 64 finished with value: 0.17080726162133036 and parameters: {'alpha': 993.5377698939861, 'tol': 0.009657071906334032, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:05,140] Trial 67 finished with value: 0.1459392579173521 and parameters: {'alpha': 983.9200436818975, 'tol': 0.009134323873520638, 'fit_intercept': True, 'positive': False}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:05,234] Trial 68 finished with value: 0.17080725848042727 and parameters: {'alpha': 969.0492026685924, 'tol': 0.009909566344072463, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:05,275] Trial 70 finished with value: 0.17080724086786564 and parameters: {'alpha': 831.7916861537799, 'tol': 0.009895181549047378, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:05,286] Trial 69 finished with value: 0.17080726011202013 and parameters: {'alpha': 981.7697684002364, 'tol': 0.009417204811415872, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:05,321] Trial 71 finished with value: 0.164663123420968 and parameters: {'alpha': 972.7639055383538, 'tol': 0.00015148817047698146, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:09,099] Trial 74 finished with value: 0.13878314252686638 and parameters: {'alpha': 423.1296302347498, 'tol': 0.00888123012310217, 'fit_intercept': False, 'positive': False}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:09,193] Trial 72 finished with value: 0.1708072568265716 and parameters: {'alpha': 956.1559846958119, 'tol': 0.009995980513470578, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:09,214] Trial 73 finished with value: 0.17080715579628675 and parameters: {'alpha': 170.28512775362606, 'tol': 0.006042375083568046, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:10,450] Trial 79 finished with value: 0.13283330666844428 and parameters: {'alpha': 142.0437771869656, 'tol': 0.009688252516637318, 'fit_intercept': False, 'positive': False}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:10,528] Trial 76 finished with value: 0.13603568938292052 and parameters: {'alpha': 275.9346799525774, 'tol': 0.006943098708945888, 'fit_intercept': False, 'positive': False}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:10,528] Trial 77 finished with value: 0.13643913145529873 and parameters: {'alpha': 295.4384761593294, 'tol': 0.0007480227078487979, 'fit_intercept': False, 'positive': False}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:10,561] Trial 75 finished with value: 0.17080714444460512 and parameters: {'alpha': 82.19929447915075, 'tol': 0.007421067687073126, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:10,575] Trial 78 finished with value: 0.17080722234912785 and parameters: {'alpha': 687.5850238969747, 'tol': 0.008689891045483603, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:14,397] Trial 82 finished with value: 0.17080722266467105 and parameters: {'alpha': 690.0412129504025, 'tol': 0.007782924464270312, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:14,398] Trial 80 finished with value: 0.17080726075142616 and parameters: {'alpha': 986.7550850350895, 'tol': 0.009896365466866456, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:14,422] Trial 81 finished with value: 0.17080722911427465 and parameters: {'alpha': 740.2522833933126, 'tol': 0.0083988294253012, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:15,293] Trial 87 finished with value: 0.14200479317466008 and parameters: {'alpha': 611.2186406052288, 'tol': 0.004182689425196077, 'fit_intercept': True, 'positive': False}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:15,332] Trial 84 finished with value: 0.1421587342582427 and parameters: {'alpha': 623.2200732275838, 'tol': 0.008771351329044628, 'fit_intercept': True, 'positive': False}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:15,352] Trial 86 finished with value: 0.17080726203765 and parameters: {'alpha': 996.783924440218, 'tol': 0.008801402332643513, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:15,368] Trial 83 finished with value: 0.17080726127456858 and parameters: {'alpha': 990.8340206619085, 'tol': 0.00910747192473852, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:15,380] Trial 85 finished with value: 0.17080723101191625 and parameters: {'alpha': 755.0283579009949, 'tol': 0.008696717417476412, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:17,612] Trial 90 finished with value: 0.17080726190993836 and parameters: {'alpha': 995.7881165412932, 'tol': 0.008360522127146549, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:17,761] Trial 94 finished with value: 0.1708072596757468 and parameters: {'alpha': 978.3683150367333, 'tol': 0.008323471093848933, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:17,783] Trial 88 finished with value: 0.17080725845229688 and parameters: {'alpha': 968.829894590024, 'tol': 0.008258859640192785, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:17,813] Trial 89 finished with value: 0.17080725995567542 and parameters: {'alpha': 980.5508019480276, 'tol': 0.008405751792125353, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:18,452] Trial 92 finished with value: 0.170807258214203 and parameters: {'alpha': 966.9736944204998, 'tol': 0.00800522934276063, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:18,483] Trial 93 finished with value: 0.17080726109103228 and parameters: {'alpha': 989.4029797602398, 'tol': 0.00896396634577045, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:18,504] Trial 91 finished with value: 0.17080726237010438 and parameters: {'alpha': 999.3762014842423, 'tol': 0.008463227463878627, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:18,521] Trial 95 finished with value: 0.17080725939385916 and parameters: {'alpha': 976.1705804465392, 'tol': 0.00831958356589696, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:23,099] Trial 97 finished with value: 0.14606588469471246 and parameters: {'alpha': 998.618139341804, 'tol': 0.007597710899123065, 'fit_intercept': True, 'positive': False}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:23,135] Trial 96 finished with value: 0.1705998990141796 and parameters: {'alpha': 994.5930282025038, 'tol': 0.006379518671560631, 'fit_intercept': False, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:23,135] Trial 99 finished with value: 0.17059988914738844 and parameters: {'alpha': 949.8243328627784, 'tol': 0.006983158794862042, 'fit_intercept': False, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:23,156] Trial 98 finished with value: 0.17080725899337004 and parameters: {'alpha': 973.0482174684483, 'tol': 0.007854871871479503, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:24,498] Trial 100 finished with value: 0.1453841573757955 and parameters: {'alpha': 975.4661862250629, 'tol': 0.005757493810491316, 'fit_intercept': False, 'positive': False}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:24,515] Trial 101 finished with value: 0.13576518799303108 and parameters: {'alpha': 243.07518198105947, 'tol': 0.0019558770819484137, 'fit_intercept': True, 'positive': False}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:24,581] Trial 103 finished with value: 0.14493433826801627 and parameters: {'alpha': 925.201591505959, 'tol': 0.005946863363065389, 'fit_intercept': False, 'positive': False}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:24,613] Trial 102 finished with value: 0.17059989738865827 and parameters: {'alpha': 987.2169995043066, 'tol': 0.008299344474494964, 'fit_intercept': False, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:26,954] Trial 104 finished with value: 0.17059989609804166 and parameters: {'alpha': 981.3607972966081, 'tol': 0.008747407377030038, 'fit_intercept': False, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:27,088] Trial 105 finished with value: 0.17059988848305419 and parameters: {'alpha': 946.8103207456177, 'tol': 0.009620348947388129, 'fit_intercept': False, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:27,106] Trial 109 finished with value: 0.17059989809253862 and parameters: {'alpha': 990.410928380457, 'tol': 0.009080366711728113, 'fit_intercept': False, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:27,119] Trial 106 finished with value: 0.17080726058911713 and parameters: {'alpha': 985.4895819851101, 'tol': 0.00819857370134134, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:27,128] Trial 107 finished with value: 0.17080726070867208 and parameters: {'alpha': 986.4217359607812, 'tol': 0.009997141000753591, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:27,664] Trial 110 finished with value: 0.170807261107974 and parameters: {'alpha': 989.5350747632035, 'tol': 0.009881308777279336, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:27,695] Trial 108 finished with value: 0.17080726205690808 and parameters: {'alpha': 996.9340863230611, 'tol': 0.008951719400721963, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:27,708] Trial 111 finished with value: 0.17080725313590145 and parameters: {'alpha': 927.3873886817942, 'tol': 0.00969098402120929, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:29,458] Trial 112 finished with value: 0.17080725945650108 and parameters: {'alpha': 976.6589654722429, 'tol': 0.00809676501846657, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:30,032] Trial 115 finished with value: 0.17080726207398178 and parameters: {'alpha': 997.0672158856022, 'tol': 0.009691332325623614, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:30,067] Trial 114 finished with value: 0.17080723986239696 and parameters: {'alpha': 823.9590663173537, 'tol': 0.008016907761776876, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:30,078] Trial 116 finished with value: 0.17080726116943057 and parameters: {'alpha': 990.0142535560441, 'tol': 0.0082130884211656, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:30,086] Trial 113 finished with value: 0.17080726004593932 and parameters: {'alpha': 981.2545579759895, 'tol': 0.00991469711616688, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:30,701] Trial 117 finished with value: 0.17080724149600648 and parameters: {'alpha': 836.6850885910558, 'tol': 0.007310455812225115, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:30,720] Trial 119 finished with value: 0.17080726215875303 and parameters: {'alpha': 997.728207966675, 'tol': 0.007306882682225995, 'fit_intercept': True, 'positive': True}. Best is trial 57 with value: 0.17080726241106634.\n",
      "[I 2025-05-31 04:26:30,734] Trial 118 finished with value: 0.17080726244036032 and parameters: {'alpha': 999.9240196772589, 'tol': 0.008816224747449947, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:32,555] Trial 120 finished with value: 0.17080726103828592 and parameters: {'alpha': 988.9917159583607, 'tol': 0.006732508278149444, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:32,580] Trial 122 finished with value: 0.1708072609305262 and parameters: {'alpha': 988.1515157235038, 'tol': 0.009861026809595778, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:32,812] Trial 123 finished with value: 0.1708072599311966 and parameters: {'alpha': 980.3599500309282, 'tol': 0.008996636769610577, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:33,176] Trial 121 finished with value: 0.17080724200731384 and parameters: {'alpha': 840.6684216704732, 'tol': 0.00795748065636079, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:33,211] Trial 124 finished with value: 0.170807261013395 and parameters: {'alpha': 988.7976415158379, 'tol': 0.007438664429542277, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:33,557] Trial 127 finished with value: 0.17080726222479203 and parameters: {'alpha': 998.2431396406482, 'tol': 0.0077950251328404736, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:33,805] Trial 125 finished with value: 0.17080726137648414 and parameters: {'alpha': 991.628666410744, 'tol': 0.006463939272808059, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:33,840] Trial 126 finished with value: 0.1708072592976946 and parameters: {'alpha': 975.4208407147248, 'tol': 0.008409793847439475, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:37,551] Trial 129 finished with value: 0.170807259834877 and parameters: {'alpha': 979.608983655224, 'tol': 0.007325757481674548, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:37,568] Trial 128 finished with value: 0.17080726209361197 and parameters: {'alpha': 997.220279309645, 'tol': 0.006757207159267009, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:37,582] Trial 130 finished with value: 0.1708072612340974 and parameters: {'alpha': 990.518464291229, 'tol': 0.007650974092886568, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:38,421] Trial 132 finished with value: 0.14492885712726555 and parameters: {'alpha': 873.4652845210387, 'tol': 0.0020005229533280227, 'fit_intercept': True, 'positive': False}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:38,538] Trial 131 finished with value: 0.17080726111264816 and parameters: {'alpha': 989.5715193285453, 'tol': 0.007000801163902124, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:38,539] Trial 133 finished with value: 0.170807262334931 and parameters: {'alpha': 999.101939227086, 'tol': 0.007599656729140833, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:39,271] Trial 135 finished with value: 0.1682907650292538 and parameters: {'alpha': 325.5106686673726, 'tol': 0.000787168913321647, 'fit_intercept': False, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:39,309] Trial 134 finished with value: 0.17080726145143954 and parameters: {'alpha': 992.2131027816266, 'tol': 0.007802307629188201, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:43,609] Trial 136 finished with value: 0.13875935156971742 and parameters: {'alpha': 421.69759158532617, 'tol': 0.004202246569464786, 'fit_intercept': False, 'positive': False}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:43,763] Trial 137 finished with value: 0.17080726158345014 and parameters: {'alpha': 993.2424105990419, 'tol': 0.007793024170118268, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:43,775] Trial 138 finished with value: 0.1708072595858951 and parameters: {'alpha': 977.6677842540332, 'tol': 0.005256771168838407, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:44,315] Trial 139 finished with value: 0.1708072397010229 and parameters: {'alpha': 822.7019912057552, 'tol': 0.008607023353006756, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:44,367] Trial 141 finished with value: 0.17080725937526797 and parameters: {'alpha': 976.0256357511466, 'tol': 0.005337477249073726, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:44,424] Trial 140 finished with value: 0.16479550511759863 and parameters: {'alpha': 237.7057603764864, 'tol': 7.752869714990583e-05, 'fit_intercept': False, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:46,164] Trial 143 finished with value: 0.14518883884438177 and parameters: {'alpha': 900.7721001337749, 'tol': 0.009772476154188896, 'fit_intercept': True, 'positive': False}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:46,242] Trial 142 finished with value: 0.17080721563825868 and parameters: {'alpha': 635.3555438392797, 'tol': 0.0022838190772834642, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:50,236] Trial 144 finished with value: 0.17080723892036473 and parameters: {'alpha': 816.6209267145332, 'tol': 0.009571600863119064, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:50,257] Trial 146 finished with value: 0.17080723772487877 and parameters: {'alpha': 807.3088919101183, 'tol': 0.008936301762702556, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:50,269] Trial 145 finished with value: 0.17080725631421442 and parameters: {'alpha': 952.1619103893607, 'tol': 0.005300883315661815, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:51,683] Trial 149 finished with value: 0.1459076407129972 and parameters: {'alpha': 980.2811752539394, 'tol': 0.009725118026877278, 'fit_intercept': True, 'positive': False}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:51,739] Trial 147 finished with value: 0.14387883875419671 and parameters: {'alpha': 770.3429683623281, 'tol': 0.008925773819635212, 'fit_intercept': True, 'positive': False}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:51,807] Trial 148 finished with value: 0.17080726065003668 and parameters: {'alpha': 985.964562813001, 'tol': 0.007838362944564118, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:52,376] Trial 150 finished with value: 0.17080723517477753 and parameters: {'alpha': 787.4469212698755, 'tol': 0.009465492812484006, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:52,427] Trial 151 finished with value: 0.17080726054869758 and parameters: {'alpha': 985.1744372030618, 'tol': 0.008849298105917587, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:53,650] Trial 152 finished with value: 0.170807260487914 and parameters: {'alpha': 984.7005182976079, 'tol': 0.008902335685573408, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:53,995] Trial 154 finished with value: 0.17080726145961206 and parameters: {'alpha': 992.2768251597552, 'tol': 0.00886029578957798, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:54,023] Trial 153 finished with value: 0.17080725762824825 and parameters: {'alpha': 962.4056235057803, 'tol': 0.008809463767643846, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:54,159] Trial 155 finished with value: 0.17080726230037346 and parameters: {'alpha': 998.8324789296635, 'tol': 0.008505105014504537, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:54,390] Trial 157 finished with value: 0.17080725900847962 and parameters: {'alpha': 973.1660164885052, 'tol': 0.008869346701840147, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:54,571] Trial 156 finished with value: 0.17080726042580444 and parameters: {'alpha': 984.216262480713, 'tol': 0.00822499053782731, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:55,010] Trial 158 finished with value: 0.17080726119665784 and parameters: {'alpha': 990.2265459588684, 'tol': 0.007795861298932335, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:55,119] Trial 159 finished with value: 0.17080723867182013 and parameters: {'alpha': 814.6848908485943, 'tol': 0.006427319502085215, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:56,294] Trial 160 finished with value: 0.17080724164630684 and parameters: {'alpha': 837.8559925259893, 'tol': 0.0063005916041319315, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:56,719] Trial 162 finished with value: 0.17080726232639715 and parameters: {'alpha': 999.0353969423471, 'tol': 0.007298954660761018, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:56,864] Trial 163 finished with value: 0.1708072620870846 and parameters: {'alpha': 997.1693832132536, 'tol': 0.008809666384459071, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:56,884] Trial 161 finished with value: 0.1708072604009143 and parameters: {'alpha': 984.022199325978, 'tol': 0.007866595552844984, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:57,193] Trial 164 finished with value: 0.17080723144855278 and parameters: {'alpha': 758.4284202411563, 'tol': 0.006335384828556114, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:57,427] Trial 165 finished with value: 0.17080723371154108 and parameters: {'alpha': 776.0512028582664, 'tol': 0.006522775048321104, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:57,959] Trial 167 finished with value: 0.17080725855120515 and parameters: {'alpha': 969.6009972443115, 'tol': 0.009734775158846002, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:58,004] Trial 166 finished with value: 0.1708072402284016 and parameters: {'alpha': 826.8102099635666, 'tol': 0.00621406542526392, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:59,134] Trial 168 finished with value: 0.17080726118539213 and parameters: {'alpha': 990.1387063199347, 'tol': 0.00775228207825813, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:59,331] Trial 170 finished with value: 0.17059968286699545 and parameters: {'alpha': 15.637725678458025, 'tol': 0.007374354530377849, 'fit_intercept': False, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:59,430] Trial 169 finished with value: 0.17080726136823562 and parameters: {'alpha': 991.5643517007096, 'tol': 0.007067039291508336, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:26:59,852] Trial 171 finished with value: 0.1708072622887476 and parameters: {'alpha': 998.7418274704038, 'tol': 0.009852488881217349, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:00,216] Trial 172 finished with value: 0.17080725708887862 and parameters: {'alpha': 958.2008304753205, 'tol': 0.009615751877541419, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:00,290] Trial 173 finished with value: 0.1708072611681043 and parameters: {'alpha': 990.0039125692829, 'tol': 0.00979642799064832, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:00,434] Trial 174 finished with value: 0.1705997613456411 and parameters: {'alpha': 370.6478541269977, 'tol': 0.006408280366435952, 'fit_intercept': False, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:01,463] Trial 175 finished with value: 0.17080726186973855 and parameters: {'alpha': 995.474667444355, 'tol': 0.00689412987035127, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:03,481] Trial 178 finished with value: 0.13682506950920528 and parameters: {'alpha': 292.3330659022795, 'tol': 0.005331419287986309, 'fit_intercept': True, 'positive': False}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:03,554] Trial 176 finished with value: 0.17080726013061207 and parameters: {'alpha': 981.914724084412, 'tol': 0.009900365857436838, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:03,577] Trial 177 finished with value: 0.17080726199297733 and parameters: {'alpha': 996.4355969637357, 'tol': 0.00870497882598381, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:04,139] Trial 180 finished with value: 0.17080726171450272 and parameters: {'alpha': 994.2642544878186, 'tol': 0.009146114429366628, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:04,156] Trial 179 finished with value: 0.17080725991028442 and parameters: {'alpha': 980.1969056472225, 'tol': 0.009778604008799412, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:04,658] Trial 181 finished with value: 0.17080726135229757 and parameters: {'alpha': 991.4400810746055, 'tol': 0.009946016649226555, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:04,680] Trial 182 finished with value: 0.1708072619949646 and parameters: {'alpha': 996.4510922175483, 'tol': 0.008959643438031647, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:05,265] Trial 183 finished with value: 0.17080724215413773 and parameters: {'alpha': 841.8122676392068, 'tol': 0.008889915343128042, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:06,418] Trial 184 finished with value: 0.1708072622539116 and parameters: {'alpha': 998.470196731871, 'tol': 0.009593229197288257, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:06,442] Trial 186 finished with value: 0.17080726222135598 and parameters: {'alpha': 998.2163473511847, 'tol': 0.00854308348157981, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:06,498] Trial 185 finished with value: 0.1708072597358783 and parameters: {'alpha': 978.8371329941233, 'tol': 0.009116367715035174, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:06,741] Trial 187 finished with value: 0.17080724228761446 and parameters: {'alpha': 842.8521373658687, 'tol': 0.00873175141229252, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:07,079] Trial 188 finished with value: 0.1708072616504538 and parameters: {'alpha': 993.7648509677313, 'tol': 0.007974802074416157, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:07,645] Trial 190 finished with value: 0.17080723676884213 and parameters: {'alpha': 799.8623534627027, 'tol': 0.00764589163312761, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:07,677] Trial 189 finished with value: 0.17080726118600345 and parameters: {'alpha': 990.1434729369729, 'tol': 0.008674066299799603, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:08,174] Trial 191 finished with value: 0.17080726131329946 and parameters: {'alpha': 991.1360087172421, 'tol': 0.008476894803926755, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:10,604] Trial 192 finished with value: 0.17080726242529287 and parameters: {'alpha': 999.8065314744646, 'tol': 0.008130574881164384, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:10,645] Trial 193 finished with value: 0.1708072611504592 and parameters: {'alpha': 989.8663329122918, 'tol': 0.00809314980799841, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:10,673] Trial 194 finished with value: 0.1708072623192924 and parameters: {'alpha': 998.9799985294233, 'tol': 0.008621153790201647, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:10,674] Trial 195 finished with value: 0.17080726056703244 and parameters: {'alpha': 985.3173910183971, 'tol': 0.009292416441625616, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:11,311] Trial 196 finished with value: 0.17080726190058568 and parameters: {'alpha': 995.7151913507325, 'tol': 0.009883614807298408, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:13,184] Trial 198 finished with value: 0.1708072615583203 and parameters: {'alpha': 993.0464686808994, 'tol': 0.009062957436379013, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:13,218] Trial 197 finished with value: 0.17080726214193362 and parameters: {'alpha': 997.5970606851816, 'tol': 0.009415639908875378, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:14,271] Trial 199 finished with value: 0.17080726225756623 and parameters: {'alpha': 998.4986936324087, 'tol': 0.007896346153063333, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:16,284] Trial 202 finished with value: 0.1460228711300205 and parameters: {'alpha': 993.6029219722378, 'tol': 0.00696985668733204, 'fit_intercept': True, 'positive': False}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:16,401] Trial 203 finished with value: 0.17080723352498142 and parameters: {'alpha': 774.5983238345779, 'tol': 0.009500445630901388, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:16,428] Trial 200 finished with value: 0.17080726168507882 and parameters: {'alpha': 994.0348293914791, 'tol': 0.008912964174549632, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:16,439] Trial 201 finished with value: 0.1708072623274397 and parameters: {'alpha': 999.0435262951555, 'tol': 0.009994003000755017, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:17,020] Trial 204 finished with value: 0.17080724190433638 and parameters: {'alpha': 839.8661701761373, 'tol': 0.007823875651749372, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:17,610] Trial 206 finished with value: 0.17080723612422627 and parameters: {'alpha': 794.8416356074039, 'tol': 0.009967579791888776, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:17,621] Trial 205 finished with value: 0.17080723724189423 and parameters: {'alpha': 803.5469022741543, 'tol': 0.008054513891833183, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:18,186] Trial 207 finished with value: 0.17080724191013455 and parameters: {'alpha': 839.9113408236399, 'tol': 0.00752047250156163, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:18,999] Trial 209 finished with value: 0.17080724728175345 and parameters: {'alpha': 881.7640206129848, 'tol': 0.007103292864174821, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:19,116] Trial 208 finished with value: 0.1708072376232497 and parameters: {'alpha': 806.5172914471099, 'tol': 0.009837060152457125, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:19,235] Trial 211 finished with value: 0.17080725779500466 and parameters: {'alpha': 963.7056355675523, 'tol': 0.006302351113001233, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:19,250] Trial 210 finished with value: 0.17080724012707227 and parameters: {'alpha': 826.0208589485848, 'tol': 0.008014136686127537, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:19,838] Trial 212 finished with value: 0.17080724396016517 and parameters: {'alpha': 855.882895649601, 'tol': 0.009968365920710336, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:20,427] Trial 214 finished with value: 0.17080726225243675 and parameters: {'alpha': 998.4586967068134, 'tol': 0.006885867283664971, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:20,438] Trial 213 finished with value: 0.17080726194047394 and parameters: {'alpha': 996.0262117352605, 'tol': 0.006073524926267745, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:20,947] Trial 215 finished with value: 0.17080725907422478 and parameters: {'alpha': 973.6785870664046, 'tol': 0.009999804538396155, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:21,662] Trial 217 finished with value: 0.17080726202398158 and parameters: {'alpha': 996.6773470942305, 'tol': 0.00992023525409678, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:22,290] Trial 218 finished with value: 0.1708072623622818 and parameters: {'alpha': 999.3152057069603, 'tol': 0.008923130691099073, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:22,303] Trial 216 finished with value: 0.1708072604822212 and parameters: {'alpha': 984.6561326294706, 'tol': 0.009752859143602747, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:22,314] Trial 219 finished with value: 0.17080726030219673 and parameters: {'alpha': 983.2525214689489, 'tol': 0.009992818342601106, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:22,879] Trial 220 finished with value: 0.17059968797214817 and parameters: {'alpha': 38.716839720280404, 'tol': 0.003824025421386478, 'fit_intercept': False, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:23,551] Trial 222 finished with value: 0.17080726238253705 and parameters: {'alpha': 999.4731445330782, 'tol': 0.007767834352520979, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:23,571] Trial 221 finished with value: 0.1708072612525953 and parameters: {'alpha': 990.662693388086, 'tol': 0.007528868190980398, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:24,208] Trial 223 finished with value: 0.17080726096230284 and parameters: {'alpha': 988.3992769514385, 'tol': 0.006722269021909904, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:24,981] Trial 224 finished with value: 0.17080726222722836 and parameters: {'alpha': 998.2621364989255, 'tol': 0.007322353233503294, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:26,630] Trial 226 finished with value: 0.14273431642397266 and parameters: {'alpha': 710.353577534381, 'tol': 0.00801224990684878, 'fit_intercept': False, 'positive': False}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:26,670] Trial 225 finished with value: 0.17080726019791373 and parameters: {'alpha': 982.4394549490936, 'tol': 0.006514823178547209, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:26,682] Trial 227 finished with value: 0.1708072612646911 and parameters: {'alpha': 990.7570053662023, 'tol': 0.0066025640953249285, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:27,191] Trial 228 finished with value: 0.1708072600861197 and parameters: {'alpha': 981.5678312871582, 'tol': 0.006796359209383769, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:27,732] Trial 230 finished with value: 0.17080726182819253 and parameters: {'alpha': 995.1507215359899, 'tol': 0.006547695581169801, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:27,860] Trial 229 finished with value: 0.17080725526777865 and parameters: {'alpha': 944.0047075735458, 'tol': 0.007457866791425141, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:28,279] Trial 231 finished with value: 0.1708072587634024 and parameters: {'alpha': 971.2553280673608, 'tol': 0.007017577547861246, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:29,066] Trial 232 finished with value: 0.17080725993042412 and parameters: {'alpha': 980.3539270061126, 'tol': 0.00650218218694119, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:29,751] Trial 235 finished with value: 0.1708072607296104 and parameters: {'alpha': 986.5849895014319, 'tol': 0.006698601122350052, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:29,770] Trial 234 finished with value: 0.1708072623042703 and parameters: {'alpha': 998.8628644849427, 'tol': 0.0075961329229928135, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:29,790] Trial 233 finished with value: 0.17080725365011606 and parameters: {'alpha': 931.3953912517704, 'tol': 0.00673726060987145, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:30,418] Trial 236 finished with value: 0.17080726115864542 and parameters: {'alpha': 989.9301612906688, 'tol': 0.007254755647127032, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:31,806] Trial 238 finished with value: 0.1708072575860979 and parameters: {'alpha': 962.0770264533089, 'tol': 0.008140185215874727, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:31,820] Trial 237 finished with value: 0.17080725764262922 and parameters: {'alpha': 962.5177356394421, 'tol': 0.007482185803415264, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:33,552] Trial 240 finished with value: 0.1708072614721337 and parameters: {'alpha': 992.3744580365402, 'tol': 0.008531554842997226, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:33,577] Trial 239 finished with value: 0.17080726216883935 and parameters: {'alpha': 997.8068546180754, 'tol': 0.008060096640180711, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:35,079] Trial 241 finished with value: 0.12934472404201708 and parameters: {'alpha': 19.049943792867964, 'tol': 0.009624724990682518, 'fit_intercept': True, 'positive': False}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:35,191] Trial 242 finished with value: 0.1705997870293807 and parameters: {'alpha': 486.93816395639016, 'tol': 0.00990220906494749, 'fit_intercept': False, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:35,587] Trial 243 finished with value: 0.1708072614402223 and parameters: {'alpha': 992.1256403987428, 'tol': 0.008307705828939242, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:35,999] Trial 244 finished with value: 0.17080725837238858 and parameters: {'alpha': 968.2069204898252, 'tol': 0.008083263607757069, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:36,041] Trial 245 finished with value: 0.170807262144101 and parameters: {'alpha': 997.613960686319, 'tol': 0.007691087668550329, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:36,054] Trial 246 finished with value: 0.17080726198580737 and parameters: {'alpha': 996.3796905157301, 'tol': 0.008259333549359234, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:36,311] Trial 247 finished with value: 0.17080726158641832 and parameters: {'alpha': 993.2655540714737, 'tol': 0.008174143139534424, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:36,450] Trial 248 finished with value: 0.1708072596618591 and parameters: {'alpha': 978.260038982106, 'tol': 0.007997984603315867, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n",
      "[I 2025-05-31 04:27:36,651] Trial 249 finished with value: 0.17080725582377057 and parameters: {'alpha': 948.3387438501152, 'tol': 0.008177921825087734, 'fit_intercept': True, 'positive': True}. Best is trial 118 with value: 0.17080726244036032.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.base import clone\n",
    "from scipy.stats import pearsonr\n",
    "import gc\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"random_state\": CFG.seed,\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1000),\n",
    "        \"tol\": trial.suggest_float(\"tol\", 1e-6, 1e-2),\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "        \"positive\": trial.suggest_categorical(\"positive\", [True, False])\n",
    "    }\n",
    "\n",
    "    model = Ridge(**params)\n",
    "    scores = []\n",
    "\n",
    "    split = KFold(n_splits=CFG.n_folds, shuffle=False).split(X, y)\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(split):\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "        clf = clone(model)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_val)\n",
    "        score = pearsonr(y_val, y_pred)[0]\n",
    "        scores.append(score)\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    return avg_score\n",
    "\n",
    "if CFG.run_optuna:\n",
    "    sampler = optuna.samplers.TPESampler(seed=CFG.seed, multivariate=True)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=CFG.n_optuna_trials, n_jobs=-1, catch=(ValueError,))\n",
    "    best_params = study.best_params\n",
    "\n",
    "    ridge_params = {\n",
    "        \"random_state\": CFG.seed,\n",
    "        \"alpha\": best_params[\"alpha\"],\n",
    "        \"tol\": best_params[\"tol\"],\n",
    "        \"fit_intercept\": best_params[\"fit_intercept\"],\n",
    "        \"positive\": best_params[\"positive\"]\n",
    "    }\n",
    "else:\n",
    "    ridge_params = {\n",
    "        \"random_state\": CFG.seed\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8990d3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_state': 42,\n",
       " 'alpha': 408.8163672645777,\n",
       " 'tol': 0.00011771640004788044,\n",
       " 'fit_intercept': True,\n",
       " 'positive': True}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "514f7057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_state': 42,\n",
       " 'alpha': 2.349295484010465,\n",
       " 'tol': 0.0005740485460709731,\n",
       " 'fit_intercept': False,\n",
       " 'positive': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e3c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Ridge\n",
      "\n",
      "ğŸ“¦ Saved fold 0 model to output/k-fold/all_different_data_sand_seed/ensemble//metamodel_ridge_optuna/fold0/Ridge.pkl\n",
      "--- Fold 0 - val_pearson: 0.227517 - train_pearson: 0.145023\n",
      "ğŸ“¦ Saved fold 1 model to output/k-fold/all_different_data_sand_seed/ensemble//metamodel_ridge_optuna/fold1/Ridge.pkl\n",
      "--- Fold 1 - val_pearson: 0.154319 - train_pearson: 0.157349\n",
      "ğŸ“¦ Saved fold 2 model to output/k-fold/all_different_data_sand_seed/ensemble//metamodel_ridge_optuna/fold2/Ridge.pkl\n",
      "--- Fold 2 - val_pearson: 0.111058 - train_pearson: 0.164239\n",
      "ğŸ“¦ Saved fold 3 model to output/k-fold/all_different_data_sand_seed/ensemble//metamodel_ridge_optuna/fold3/Ridge.pkl\n",
      "--- Fold 3 - val_pearson: 0.220108 - train_pearson: 0.144857\n",
      "ğŸ“¦ Saved fold 4 model to output/k-fold/all_different_data_sand_seed/ensemble//metamodel_ridge_optuna/fold4/Ridge.pkl\n",
      "--- Fold 4 - val_pearson: 0.141035 - train_pearson: 0.179207\n",
      "\n",
      "------ Overall Score: 0.134633 - Mean val_pearson: 0.170807 Â± 0.045552 - Mean train_pearson: 0.158135 Â± 0.012885 ------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ridge_trainer = Trainer(Ridge(**ridge_params), models_folder=f\"{CFG.sample_sub_path}/metamodel_ridge_optuna/\")\n",
    "# X = pd.DataFrame(oof_preds)\n",
    "# X_test = pd.DataFrame(test_preds)\n",
    "_, ridge_test_preds, val_scores[\"Ridge (ensemble)\"], train_scores[\"Ridge (ensemble)\"] = ridge_trainer.fit_predict(X, y, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e394d08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Saved ensemble submission to submission_ensemble_no_MLP_ridge_op.csv\n"
     ]
    }
   ],
   "source": [
    "sub_ens = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'prediction': ridge_test_preds\n",
    "})\n",
    "ens_name = 'submission_ensemble_no_MLP_ridge_op.csv'\n",
    "sub_ens.to_csv(os.path.join(CFG.sample_sub_path, ens_name), index=False)\n",
    "print(f\"ğŸ“¦ Saved ensemble submission to {ens_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfb178e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 21:01:20,064] A new study created in memory with name: no-name-73a0436e-badc-4639-ac2c-40640694e3f9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 21:02:36,086] Trial 3 finished with value: 0.134202350233761 and parameters: {'hidden_layer_sizes': (6,), 'activation': 'tanh', 'solver': 'lbfgs', 'alpha': 0.003024536856435966, 'learning_rate_init': 0.00031541353228579515, 'batch_size': 346}. Best is trial 3 with value: 0.134202350233761.\n",
      "[I 2025-05-30 21:02:57,921] Trial 1 finished with value: 0.1337708131462209 and parameters: {'hidden_layer_sizes': (8, 4), 'activation': 'relu', 'solver': 'lbfgs', 'alpha': 0.005013483541110714, 'learning_rate_init': 0.002345802415777592, 'batch_size': 1130}. Best is trial 3 with value: 0.134202350233761.\n",
      "[I 2025-05-30 21:03:39,626] Trial 0 finished with value: 0.13574429482257883 and parameters: {'hidden_layer_sizes': (10, 5), 'activation': 'tanh', 'solver': 'lbfgs', 'alpha': 3.7658628818285295e-05, 'learning_rate_init': 0.0001477580844823203, 'batch_size': 623}. Best is trial 0 with value: 0.13574429482257883.\n",
      "[I 2025-05-30 21:03:43,476] Trial 8 finished with value: 0.13744597712409007 and parameters: {'hidden_layer_sizes': (6,), 'activation': 'tanh', 'solver': 'lbfgs', 'alpha': 1.1795379085002838e-05, 'learning_rate_init': 0.0023836364056681074, 'batch_size': 371}. Best is trial 8 with value: 0.13744597712409007.\n",
      "[I 2025-05-30 21:04:51,480] Trial 10 finished with value: 0.1405020486491466 and parameters: {'hidden_layer_sizes': (8, 4), 'activation': 'relu', 'solver': 'lbfgs', 'alpha': 0.0012293030343299878, 'learning_rate_init': 0.0072889643718725946, 'batch_size': 286}. Best is trial 10 with value: 0.1405020486491466.\n",
      "[I 2025-05-30 21:05:46,563] Trial 12 finished with value: 0.1355698303286625 and parameters: {'hidden_layer_sizes': (6,), 'activation': 'tanh', 'solver': 'lbfgs', 'alpha': 0.041947445569884854, 'learning_rate_init': 0.00040589561125499646, 'batch_size': 914}. Best is trial 10 with value: 0.1405020486491466.\n",
      "[I 2025-05-30 21:07:31,059] Trial 11 finished with value: 0.1366558020280527 and parameters: {'hidden_layer_sizes': (8,), 'activation': 'relu', 'solver': 'adam', 'alpha': 0.007811625535820459, 'learning_rate_init': 0.00634967249228685, 'batch_size': 911}. Best is trial 10 with value: 0.1405020486491466.\n",
      "[I 2025-05-30 21:09:22,913] Trial 4 finished with value: 0.1498707819949332 and parameters: {'hidden_layer_sizes': (8,), 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0013155165746584962, 'learning_rate_init': 0.000532877117361393, 'batch_size': 1835}. Best is trial 4 with value: 0.1498707819949332.\n",
      "[I 2025-05-30 21:11:27,088] Trial 14 finished with value: 0.13645430724548868 and parameters: {'hidden_layer_sizes': (8,), 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0030911004141208807, 'learning_rate_init': 0.0032744786401882986, 'batch_size': 1366}. Best is trial 4 with value: 0.1498707819949332.\n",
      "[I 2025-05-30 21:19:02,686] Trial 9 finished with value: 0.14107217186498175 and parameters: {'hidden_layer_sizes': (8, 4), 'activation': 'relu', 'solver': 'adam', 'alpha': 0.019580287765680125, 'learning_rate_init': 0.0005249416043909642, 'batch_size': 558}. Best is trial 4 with value: 0.1498707819949332.\n",
      "[I 2025-05-30 21:19:27,590] Trial 2 finished with value: 0.14388128164803354 and parameters: {'hidden_layer_sizes': (10, 5), 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00022565794768661715, 'learning_rate_init': 0.0005634638238656081, 'batch_size': 267}. Best is trial 4 with value: 0.1498707819949332.\n",
      "[I 2025-05-30 21:26:36,652] Trial 13 finished with value: 0.1512635673328433 and parameters: {'hidden_layer_sizes': (8, 4), 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.007490844863905224, 'learning_rate_init': 0.00015847803549408194, 'batch_size': 659}. Best is trial 13 with value: 0.1512635673328433.\n",
      "[I 2025-05-30 21:26:42,470] Trial 5 finished with value: 0.1509859863810493 and parameters: {'hidden_layer_sizes': (8, 4), 'activation': 'tanh', 'solver': 'adam', 'alpha': 3.8515037768521214e-05, 'learning_rate_init': 0.00019457098152450499, 'batch_size': 616}. Best is trial 13 with value: 0.1512635673328433.\n",
      "[I 2025-05-30 21:26:44,775] Trial 17 finished with value: 0.1396591263155448 and parameters: {'hidden_layer_sizes': (8,), 'activation': 'tanh', 'solver': 'adam', 'alpha': 5.86112845668972e-05, 'learning_rate_init': 0.0022945590196923687, 'batch_size': 1907}. Best is trial 13 with value: 0.1512635673328433.\n",
      "[I 2025-05-30 21:27:44,505] Trial 16 finished with value: 0.11358205483456925 and parameters: {'hidden_layer_sizes': (12, 6, 3), 'activation': 'relu', 'solver': 'adam', 'alpha': 3.561577713665294e-05, 'learning_rate_init': 0.0030525719127471166, 'batch_size': 158}. Best is trial 13 with value: 0.1512635673328433.\n",
      "[I 2025-05-30 21:34:39,090] Trial 6 finished with value: 0.13184929091197894 and parameters: {'hidden_layer_sizes': (10, 5), 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.001587069397235479, 'learning_rate_init': 0.0019992484651690286, 'batch_size': 250}. Best is trial 13 with value: 0.1512635673328433.\n",
      "[I 2025-05-30 21:36:33,841] Trial 18 finished with value: 0.1417571967768049 and parameters: {'hidden_layer_sizes': (10, 5), 'activation': 'relu', 'solver': 'adam', 'alpha': 2.0886074557478802e-05, 'learning_rate_init': 0.0003925902352235664, 'batch_size': 247}. Best is trial 13 with value: 0.1512635673328433.\n",
      "[I 2025-05-30 21:38:18,288] Trial 19 finished with value: 0.1514433149476575 and parameters: {'hidden_layer_sizes': (8, 4), 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0015410000888280186, 'learning_rate_init': 0.0002136003904156808, 'batch_size': 1521}. Best is trial 19 with value: 0.1514433149476575.\n",
      "[I 2025-05-30 21:40:58,257] Trial 15 finished with value: 0.1394973677524169 and parameters: {'hidden_layer_sizes': (12, 6, 3), 'activation': 'tanh', 'solver': 'adam', 'alpha': 2.915115761882358e-05, 'learning_rate_init': 0.00030734161606358495, 'batch_size': 968}. Best is trial 19 with value: 0.1514433149476575.\n",
      "[I 2025-05-30 21:44:41,842] Trial 22 finished with value: 0.15306728359806115 and parameters: {'hidden_layer_sizes': (8, 4), 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0010081545482437762, 'learning_rate_init': 0.00010372750675449598, 'batch_size': 780}. Best is trial 22 with value: 0.15306728359806115.\n",
      "[I 2025-05-30 21:50:05,401] Trial 27 finished with value: 0.14634152916412776 and parameters: {'hidden_layer_sizes': (6,), 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0008517881719564289, 'learning_rate_init': 0.00013519390676356374, 'batch_size': 987}. Best is trial 22 with value: 0.15306728359806115.\n",
      "[I 2025-05-30 21:50:54,611] Trial 20 finished with value: 0.14944325471231562 and parameters: {'hidden_layer_sizes': (8, 4), 'activation': 'tanh', 'solver': 'adam', 'alpha': 6.055205921492076e-05, 'learning_rate_init': 0.00023794982081950253, 'batch_size': 345}. Best is trial 22 with value: 0.15306728359806115.\n",
      "[I 2025-05-30 21:52:56,058] Trial 21 finished with value: 0.15037771464981312 and parameters: {'hidden_layer_sizes': (8, 4), 'activation': 'tanh', 'solver': 'adam', 'alpha': 7.674485152902651e-05, 'learning_rate_init': 0.0002696514345984511, 'batch_size': 339}. Best is trial 22 with value: 0.15306728359806115.\n",
      "[I 2025-05-30 21:53:49,394] Trial 24 finished with value: 0.15161968971338563 and parameters: {'hidden_layer_sizes': (8, 4), 'activation': 'tanh', 'solver': 'adam', 'alpha': 8.206703343225276e-05, 'learning_rate_init': 0.0002197800071984334, 'batch_size': 904}. Best is trial 22 with value: 0.15306728359806115.\n",
      "[I 2025-05-30 21:54:41,321] Trial 30 finished with value: 0.13679094385195467 and parameters: {'hidden_layer_sizes': (8, 4), 'activation': 'tanh', 'solver': 'lbfgs', 'alpha': 0.0016918489209680504, 'learning_rate_init': 0.0002088813185805563, 'batch_size': 1744}. Best is trial 22 with value: 0.15306728359806115.\n",
      "[I 2025-05-30 21:55:37,161] Trial 31 finished with value: 0.13651028707731921 and parameters: {'hidden_layer_sizes': (8, 4), 'activation': 'tanh', 'solver': 'lbfgs', 'alpha': 0.00098540740954681, 'learning_rate_init': 0.0001520242812863973, 'batch_size': 1246}. Best is trial 22 with value: 0.15306728359806115.\n",
      "[I 2025-05-30 21:56:17,696] Trial 23 finished with value: 0.15248126948642785 and parameters: {'hidden_layer_sizes': (8, 4), 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0028265534565901345, 'learning_rate_init': 0.0001010541132051779, 'batch_size': 557}. Best is trial 22 with value: 0.15306728359806115.\n",
      "[I 2025-05-30 21:57:52,230] Trial 26 finished with value: 0.15287590381245594 and parameters: {'hidden_layer_sizes': (8, 4), 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.023212131921438773, 'learning_rate_init': 0.00011921786659454696, 'batch_size': 922}. Best is trial 22 with value: 0.15306728359806115.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_units = trial.suggest_categorical(\"hidden_layer_sizes\", [(6,), (8,), (8, 4), (10, 5), (12, 6, 3)])\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"])\n",
    "    solver = trial.suggest_categorical(\"solver\", [\"adam\", \"lbfgs\"])\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-5, 1e-1, log=True)\n",
    "    learning_rate_init = trial.suggest_float(\"learning_rate_init\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 64, 2048, log=True)\n",
    "\n",
    "    params = {\n",
    "        \"hidden_layer_sizes\": hidden_units,\n",
    "        \"activation\": activation,\n",
    "        \"solver\": solver,\n",
    "        \"alpha\": alpha,\n",
    "        \"learning_rate_init\": learning_rate_init,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_iter\": 200,\n",
    "        \"early_stopping\": True,\n",
    "        \"random_state\": CFG.seed\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(MLPRegressor(**params), models_folder=f\"{CFG.sample_sub_path}/metamodel_6_mlp_optuna\")\n",
    "    return trainer.tune(X, y)\n",
    "\n",
    "\n",
    "if CFG.run_optuna:\n",
    "    sampler = optuna.samplers.TPESampler(seed=CFG.seed, multivariate=True)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=CFG.n_optuna_trials, n_jobs=-1, catch=(ValueError,))\n",
    "    best_params = study.best_params\n",
    "\n",
    "    mlp_params = {\n",
    "        \"hidden_layer_sizes\": best_params[\"hidden_layer_sizes\"],\n",
    "        \"activation\": best_params[\"activation\"],\n",
    "        \"solver\": best_params[\"solver\"],\n",
    "        \"alpha\": best_params[\"alpha\"],\n",
    "        \"learning_rate_init\": best_params[\"learning_rate_init\"],\n",
    "        \"batch_size\": 64,\n",
    "        \"max_iter\": 200,\n",
    "        \"early_stopping\": True,\n",
    "        \"random_state\": CFG.seed\n",
    "    }\n",
    "else:\n",
    "    mlp_params = {\n",
    "        \"hidden_layer_sizes\": (8, 4),\n",
    "        \"activation\": \"relu\",\n",
    "        \"solver\": \"adam\",\n",
    "        \"alpha\": 1e-3,\n",
    "        \"learning_rate_init\": 1e-3,\n",
    "        \"batch_size\": 64,\n",
    "        \"max_iter\": 200,\n",
    "        \"early_stopping\": True,\n",
    "        \"random_state\": CFG.seed\n",
    "    }\n",
    "\n",
    "# ç”¢ç”Ÿ MLP Trainer ä¸¦åŸ·è¡Œ\n",
    "mlp_trainer = Trainer(MLPRegressor(**mlp_params), models_folder=f\"{CFG.sample_sub_path}/metamodel_6_mlp_optuna\")\n",
    "oof_preds[\"MLP\"], test_preds[\"MLP\"], scores[\"MLP\"] = mlp_trainer.fit_predict(X, y, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dd9b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_layer_sizes': (6,),\n",
       " 'activation': 'tanh',\n",
       " 'solver': 'adam',\n",
       " 'alpha': 0.07769191538624844,\n",
       " 'learning_rate_init': 0.003347750729180181,\n",
       " 'batch_size': 64,\n",
       " 'max_iter': 200,\n",
       " 'early_stopping': True,\n",
       " 'random_state': 42}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb657f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLPRegressor\n",
      "\n",
      "ğŸ“¦ Saved fold 0 model to output/k-fold/ensemble//metamodel_mlp/fold0/MLPRegressor.pkl\n",
      "--- Fold 0 - Score: 0.070023\n",
      "ğŸ“¦ Saved fold 1 model to output/k-fold/ensemble//metamodel_mlp/fold1/MLPRegressor.pkl\n",
      "--- Fold 1 - Score: 0.109087\n",
      "ğŸ“¦ Saved fold 2 model to output/k-fold/ensemble//metamodel_mlp/fold2/MLPRegressor.pkl\n",
      "--- Fold 2 - Score: 0.037309\n",
      "ğŸ“¦ Saved fold 3 model to output/k-fold/ensemble//metamodel_mlp/fold3/MLPRegressor.pkl\n",
      "--- Fold 3 - Score: 0.133597\n",
      "ğŸ“¦ Saved fold 4 model to output/k-fold/ensemble//metamodel_mlp/fold4/MLPRegressor.pkl\n",
      "--- Fold 4 - Score: 0.109197\n",
      "\n",
      "------ Overall Score: 0.077463 - Mean Score: 0.091843 Â± 0.034035\n"
     ]
    }
   ],
   "source": [
    "sub_ens = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'prediction': test_preds[\"MLP\"]\n",
    "})\n",
    "ens_name = 'submission_ensemble_6models_mlp_optuna.csv'\n",
    "sub_ens.to_csv(os.path.join(CFG.sample_sub_path, ens_name), index=False)\n",
    "print(f\"ğŸ“¦ Saved ensemble submission to {ens_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86b6f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å„²å­˜è³‡æ–™\n",
    "X.to_parquet(os.path.join(CFG.sample_sub_path, \"stacked_train_X.parquet\"), index=False)\n",
    "X_test.to_parquet(os.path.join(CFG.sample_sub_path, \"stacked_test_X.parquet\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc78b531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Saved ensemble submission to submission_ensemble_6models_mlp_op.csv\n"
     ]
    }
   ],
   "source": [
    "sub_ens = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'prediction': ridge_test_preds\n",
    "})\n",
    "ens_name = 'submission_ensemble_6models_mlp_op.csv'\n",
    "sub_ens.to_csv(os.path.join(CFG.sample_sub_path, ens_name), index=False)\n",
    "print(f\"ğŸ“¦ Saved ensemble submission to {ens_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FlightRank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
